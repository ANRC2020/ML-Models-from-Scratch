{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import random as rand\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "x_points = []\n",
    "y_points = []\n",
    "\n",
    "for i in range(100):\n",
    "    \n",
    "    # We need to normalize X values to ensure that our obj function doesn't blow up in value \n",
    "    x_points.append(float(i)/100)\n",
    "\n",
    "    if i < 40:\n",
    "        y_points.append(0)\n",
    "    elif i >= 60:\n",
    "        y_points.append(1)\n",
    "    else:\n",
    "        y_points.append((1/20)*(i - 60) + 1)\n",
    "\n",
    "plt.plot(x_points, y_points, \"o\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data into training and validation sets\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(x_points, y_points, test_size=0.3, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now that we've generated some points at random, its time to use torch \n",
    "# to execute linear regression\n",
    "\n",
    "X_train = torch.tensor(X_train)\n",
    "y_train = torch.tensor(y_train)\n",
    "\n",
    "X_test = torch.tensor(X_test)\n",
    "y_test = torch.tensor(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([3.0344, 4.3492, 6.3598, 3.5966, 6.8895, 2.8577, 5.2593, 5.2070, 3.8574,\n",
       "         3.1899, 4.4371, 3.8190, 2.9154, 7.0287, 3.5609, 3.2871, 6.1104, 3.4903,\n",
       "         5.0531, 3.0957, 3.4556, 2.8011, 3.2220, 3.9749, 2.9447, 5.9299, 2.8864,\n",
       "         5.1552, 3.8962, 6.6194, 4.7588, 7.3155, 4.6646, 4.1787, 4.4817, 5.3122,\n",
       "         4.3060, 5.3656, 5.0028, 7.1707, 5.9895, 4.0960, 4.8550, 4.3929, 7.2427,\n",
       "         4.8066, 5.7546, 3.7434, 6.9588, 4.9037, 5.1039, 6.2965, 3.9354, 3.6328,\n",
       "         2.7456, 4.5722, 3.3535, 2.7732, 3.4212, 6.4883, 6.7531, 5.6973, 6.4237,\n",
       "         6.1719, 3.3201, 4.9530, 5.5290, 3.1268, 6.8210, 4.5267]),\n",
       " tensor([1.1100, 1.4700, 1.8500, 1.2800, 1.9300, 1.0500, 1.6600, 1.6500, 1.3500,\n",
       "         1.1600, 1.4900, 1.3400, 1.0700, 1.9500, 1.2700, 1.1900, 1.8100, 1.2500,\n",
       "         1.6200, 1.1300, 1.2400, 1.0300, 1.1700, 1.3800, 1.0800, 1.7800, 1.0600,\n",
       "         1.6400, 1.3600, 1.8900, 1.5600, 1.9900, 1.5400, 1.4300, 1.5000, 1.6700,\n",
       "         1.4600, 1.6800, 1.6100, 1.9700, 1.7900, 1.4100, 1.5800, 1.4800, 1.9800,\n",
       "         1.5700, 1.7500, 1.3200, 1.9400, 1.5900, 1.6300, 1.8400, 1.3700, 1.2900,\n",
       "         1.0100, 1.5200, 1.2100, 1.0200, 1.2300, 1.8700, 1.9100, 1.7400, 1.8600,\n",
       "         1.8200, 1.2000, 1.6000, 1.7100, 1.1400, 1.9200, 1.5100]))"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.exp(1.0*X_train + 1.0), 1.0*X_train + 1.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss = 1.1156245470046997\n",
      "Iteration 1: b = tensor([0.6632, 0.6944], grad_fn=<SubBackward0>), Loss = 1.1156245470046997\n",
      "tensor([0.3368, 0.3056])\n",
      "loss = 0.9282042384147644\n",
      "Iteration 2: b = tensor([0.4088, 0.4317], grad_fn=<SubBackward0>), Loss = 0.9282042384147644\n",
      "tensor([0.2543, 0.2627])\n",
      "loss = 0.8101170063018799\n",
      "Iteration 3: b = tensor([0.2353, 0.2131], grad_fn=<SubBackward0>), Loss = 0.8101170063018799\n",
      "tensor([0.1736, 0.2186])\n",
      "loss = 0.7419558763504028\n",
      "Iteration 4: b = tensor([0.1282, 0.0321], grad_fn=<SubBackward0>), Loss = 0.7419558763504028\n",
      "tensor([0.1071, 0.1810])\n",
      "loss = 0.7028899192810059\n",
      "Iteration 5: b = tensor([ 0.0700, -0.1205], grad_fn=<SubBackward0>), Loss = 0.7028899192810059\n",
      "tensor([0.0582, 0.1526])\n",
      "loss = 0.6787511110305786\n",
      "Iteration 6: b = tensor([ 0.0457, -0.2528], grad_fn=<SubBackward0>), Loss = 0.6787511110305786\n",
      "tensor([0.0244, 0.1323])\n",
      "loss = 0.6618901491165161\n",
      "Iteration 7: b = tensor([ 0.0441, -0.3707], grad_fn=<SubBackward0>), Loss = 0.6618901491165161\n",
      "tensor([0.0016, 0.1179])\n",
      "loss = 0.6485884189605713\n",
      "Iteration 8: b = tensor([ 0.0577, -0.4785], grad_fn=<SubBackward0>), Loss = 0.6485884189605713\n",
      "tensor([-0.0136,  0.1078])\n",
      "loss = 0.6371062994003296\n",
      "Iteration 9: b = tensor([ 0.0815, -0.5790], grad_fn=<SubBackward0>), Loss = 0.6371062994003296\n",
      "tensor([-0.0237,  0.1005])\n",
      "loss = 0.6266355514526367\n",
      "Iteration 10: b = tensor([ 0.1118, -0.6742], grad_fn=<SubBackward0>), Loss = 0.6266355514526367\n",
      "tensor([-0.0303,  0.0951])\n",
      "loss = 0.6167974472045898\n",
      "Iteration 11: b = tensor([ 0.1463, -0.7652], grad_fn=<SubBackward0>), Loss = 0.6167974472045898\n",
      "tensor([-0.0346,  0.0911])\n",
      "loss = 0.6074107885360718\n",
      "Iteration 12: b = tensor([ 0.1836, -0.8531], grad_fn=<SubBackward0>), Loss = 0.6074107885360718\n",
      "tensor([-0.0372,  0.0879])\n",
      "loss = 0.5983858108520508\n",
      "Iteration 13: b = tensor([ 0.2224, -0.9384], grad_fn=<SubBackward0>), Loss = 0.5983858108520508\n",
      "tensor([-0.0388,  0.0853])\n",
      "loss = 0.5896751284599304\n",
      "Iteration 14: b = tensor([ 0.2621, -1.0215], grad_fn=<SubBackward0>), Loss = 0.5896751284599304\n",
      "tensor([-0.0397,  0.0832])\n",
      "loss = 0.5812514424324036\n",
      "Iteration 15: b = tensor([ 0.3022, -1.1029], grad_fn=<SubBackward0>), Loss = 0.5812514424324036\n",
      "tensor([-0.0401,  0.0813])\n",
      "loss = 0.5730965733528137\n",
      "Iteration 16: b = tensor([ 0.3424, -1.1825], grad_fn=<SubBackward0>), Loss = 0.5730965733528137\n",
      "tensor([-0.0402,  0.0797])\n",
      "loss = 0.5651975870132446\n",
      "Iteration 17: b = tensor([ 0.3824, -1.2607], grad_fn=<SubBackward0>), Loss = 0.5651975870132446\n",
      "tensor([-0.0400,  0.0782])\n",
      "loss = 0.5575432777404785\n",
      "Iteration 18: b = tensor([ 0.4221, -1.3376], grad_fn=<SubBackward0>), Loss = 0.5575432777404785\n",
      "tensor([-0.0397,  0.0768])\n",
      "loss = 0.5501241683959961\n",
      "Iteration 19: b = tensor([ 0.4613, -1.4131], grad_fn=<SubBackward0>), Loss = 0.5501241683959961\n",
      "tensor([-0.0393,  0.0755])\n",
      "loss = 0.5429309010505676\n",
      "Iteration 20: b = tensor([ 0.5001, -1.4874], grad_fn=<SubBackward0>), Loss = 0.5429309010505676\n",
      "tensor([-0.0388,  0.0743])\n",
      "loss = 0.5359553098678589\n",
      "Iteration 21: b = tensor([ 0.5384, -1.5606], grad_fn=<SubBackward0>), Loss = 0.5359553098678589\n",
      "tensor([-0.0383,  0.0732])\n",
      "loss = 0.5291891098022461\n",
      "Iteration 22: b = tensor([ 0.5762, -1.6326], grad_fn=<SubBackward0>), Loss = 0.5291891098022461\n",
      "tensor([-0.0377,  0.0720])\n",
      "loss = 0.5226246118545532\n",
      "Iteration 23: b = tensor([ 0.6134, -1.7036], grad_fn=<SubBackward0>), Loss = 0.5226246118545532\n",
      "tensor([-0.0372,  0.0709])\n",
      "loss = 0.516254186630249\n",
      "Iteration 24: b = tensor([ 0.6500, -1.7735], grad_fn=<SubBackward0>), Loss = 0.516254186630249\n",
      "tensor([-0.0367,  0.0699])\n",
      "loss = 0.5100708603858948\n",
      "Iteration 25: b = tensor([ 0.6861, -1.8423], grad_fn=<SubBackward0>), Loss = 0.5100708603858948\n",
      "tensor([-0.0361,  0.0689])\n",
      "loss = 0.504067599773407\n",
      "Iteration 26: b = tensor([ 0.7217, -1.9102], grad_fn=<SubBackward0>), Loss = 0.504067599773407\n",
      "tensor([-0.0356,  0.0679])\n",
      "loss = 0.4982377290725708\n",
      "Iteration 27: b = tensor([ 0.7568, -1.9771], grad_fn=<SubBackward0>), Loss = 0.4982377290725708\n",
      "tensor([-0.0350,  0.0669])\n",
      "loss = 0.4925748109817505\n",
      "Iteration 28: b = tensor([ 0.7913, -2.0431], grad_fn=<SubBackward0>), Loss = 0.4925748109817505\n",
      "tensor([-0.0345,  0.0660])\n",
      "loss = 0.48707279562950134\n",
      "Iteration 29: b = tensor([ 0.8253, -2.1081], grad_fn=<SubBackward0>), Loss = 0.48707279562950134\n",
      "tensor([-0.0340,  0.0650])\n",
      "loss = 0.4817257523536682\n",
      "Iteration 30: b = tensor([ 0.8588, -2.1722], grad_fn=<SubBackward0>), Loss = 0.4817257523536682\n",
      "tensor([-0.0335,  0.0641])\n",
      "loss = 0.47652795910835266\n",
      "Iteration 31: b = tensor([ 0.8918, -2.2355], grad_fn=<SubBackward0>), Loss = 0.47652795910835266\n",
      "tensor([-0.0330,  0.0632])\n",
      "loss = 0.47147396206855774\n",
      "Iteration 32: b = tensor([ 0.9243, -2.2978], grad_fn=<SubBackward0>), Loss = 0.47147396206855774\n",
      "tensor([-0.0325,  0.0624])\n",
      "loss = 0.4665585160255432\n",
      "Iteration 33: b = tensor([ 0.9564, -2.3594], grad_fn=<SubBackward0>), Loss = 0.4665585160255432\n",
      "tensor([-0.0321,  0.0615])\n",
      "loss = 0.46177658438682556\n",
      "Iteration 34: b = tensor([ 0.9880, -2.4201], grad_fn=<SubBackward0>), Loss = 0.46177658438682556\n",
      "tensor([-0.0316,  0.0607])\n",
      "loss = 0.45712339878082275\n",
      "Iteration 35: b = tensor([ 1.0192, -2.4800], grad_fn=<SubBackward0>), Loss = 0.45712339878082275\n",
      "tensor([-0.0312,  0.0599])\n",
      "loss = 0.4525943100452423\n",
      "Iteration 36: b = tensor([ 1.0499, -2.5391], grad_fn=<SubBackward0>), Loss = 0.4525943100452423\n",
      "tensor([-0.0307,  0.0591])\n",
      "loss = 0.44818487763404846\n",
      "Iteration 37: b = tensor([ 1.0802, -2.5974], grad_fn=<SubBackward0>), Loss = 0.44818487763404846\n",
      "tensor([-0.0303,  0.0583])\n",
      "loss = 0.44389083981513977\n",
      "Iteration 38: b = tensor([ 1.1101, -2.6550], grad_fn=<SubBackward0>), Loss = 0.44389083981513977\n",
      "tensor([-0.0299,  0.0576])\n",
      "loss = 0.4397081136703491\n",
      "Iteration 39: b = tensor([ 1.1396, -2.7119], grad_fn=<SubBackward0>), Loss = 0.4397081136703491\n",
      "tensor([-0.0295,  0.0569])\n",
      "loss = 0.43563273549079895\n",
      "Iteration 40: b = tensor([ 1.1687, -2.7680], grad_fn=<SubBackward0>), Loss = 0.43563273549079895\n",
      "tensor([-0.0291,  0.0561])\n",
      "loss = 0.431660920381546\n",
      "Iteration 41: b = tensor([ 1.1974, -2.8234], grad_fn=<SubBackward0>), Loss = 0.431660920381546\n",
      "tensor([-0.0287,  0.0554])\n",
      "loss = 0.4277891516685486\n",
      "Iteration 42: b = tensor([ 1.2257, -2.8782], grad_fn=<SubBackward0>), Loss = 0.4277891516685486\n",
      "tensor([-0.0283,  0.0547])\n",
      "loss = 0.42401400208473206\n",
      "Iteration 43: b = tensor([ 1.2536, -2.9323], grad_fn=<SubBackward0>), Loss = 0.42401400208473206\n",
      "tensor([-0.0280,  0.0541])\n",
      "loss = 0.42033201456069946\n",
      "Iteration 44: b = tensor([ 1.2812, -2.9857], grad_fn=<SubBackward0>), Loss = 0.42033201456069946\n",
      "tensor([-0.0276,  0.0534])\n",
      "loss = 0.4167400300502777\n",
      "Iteration 45: b = tensor([ 1.3085, -3.0384], grad_fn=<SubBackward0>), Loss = 0.4167400300502777\n",
      "tensor([-0.0272,  0.0528])\n",
      "loss = 0.4132350981235504\n",
      "Iteration 46: b = tensor([ 1.3354, -3.0906], grad_fn=<SubBackward0>), Loss = 0.4132350981235504\n",
      "tensor([-0.0269,  0.0521])\n",
      "loss = 0.4098142087459564\n",
      "Iteration 47: b = tensor([ 1.3620, -3.1421], grad_fn=<SubBackward0>), Loss = 0.4098142087459564\n",
      "tensor([-0.0266,  0.0515])\n",
      "loss = 0.4064745306968689\n",
      "Iteration 48: b = tensor([ 1.3882, -3.1930], grad_fn=<SubBackward0>), Loss = 0.4064745306968689\n",
      "tensor([-0.0262,  0.0509])\n",
      "loss = 0.40321341156959534\n",
      "Iteration 49: b = tensor([ 1.4141, -3.2433], grad_fn=<SubBackward0>), Loss = 0.40321341156959534\n",
      "tensor([-0.0259,  0.0503])\n",
      "loss = 0.40002819895744324\n",
      "Iteration 50: b = tensor([ 1.4398, -3.2930], grad_fn=<SubBackward0>), Loss = 0.40002819895744324\n",
      "tensor([-0.0256,  0.0497])\n",
      "loss = 0.3969164192676544\n",
      "Iteration 51: b = tensor([ 1.4651, -3.3422], grad_fn=<SubBackward0>), Loss = 0.3969164192676544\n",
      "tensor([-0.0253,  0.0492])\n",
      "loss = 0.39387571811676025\n",
      "Iteration 52: b = tensor([ 1.4901, -3.3908], grad_fn=<SubBackward0>), Loss = 0.39387571811676025\n",
      "tensor([-0.0250,  0.0486])\n",
      "loss = 0.3909038007259369\n",
      "Iteration 53: b = tensor([ 1.5148, -3.4389], grad_fn=<SubBackward0>), Loss = 0.3909038007259369\n",
      "tensor([-0.0247,  0.0481])\n",
      "loss = 0.38799840211868286\n",
      "Iteration 54: b = tensor([ 1.5392, -3.4864], grad_fn=<SubBackward0>), Loss = 0.38799840211868286\n",
      "tensor([-0.0244,  0.0475])\n",
      "loss = 0.38515743613243103\n",
      "Iteration 55: b = tensor([ 1.5634, -3.5334], grad_fn=<SubBackward0>), Loss = 0.38515743613243103\n",
      "tensor([-0.0242,  0.0470])\n",
      "loss = 0.38237878680229187\n",
      "Iteration 56: b = tensor([ 1.5872, -3.5799], grad_fn=<SubBackward0>), Loss = 0.38237878680229187\n",
      "tensor([-0.0239,  0.0465])\n",
      "loss = 0.37966063618659973\n",
      "Iteration 57: b = tensor([ 1.6109, -3.6260], grad_fn=<SubBackward0>), Loss = 0.37966063618659973\n",
      "tensor([-0.0236,  0.0460])\n",
      "loss = 0.377001017332077\n",
      "Iteration 58: b = tensor([ 1.6342, -3.6715], grad_fn=<SubBackward0>), Loss = 0.377001017332077\n",
      "tensor([-0.0233,  0.0455])\n",
      "loss = 0.3743981122970581\n",
      "Iteration 59: b = tensor([ 1.6573, -3.7165], grad_fn=<SubBackward0>), Loss = 0.3743981122970581\n",
      "tensor([-0.0231,  0.0450])\n",
      "loss = 0.37185022234916687\n",
      "Iteration 60: b = tensor([ 1.6801, -3.7611], grad_fn=<SubBackward0>), Loss = 0.37185022234916687\n",
      "tensor([-0.0228,  0.0446])\n",
      "loss = 0.3693556487560272\n",
      "Iteration 61: b = tensor([ 1.7027, -3.8052], grad_fn=<SubBackward0>), Loss = 0.3693556487560272\n",
      "tensor([-0.0226,  0.0441])\n",
      "loss = 0.36691275238990784\n",
      "Iteration 62: b = tensor([ 1.7251, -3.8488], grad_fn=<SubBackward0>), Loss = 0.36691275238990784\n",
      "tensor([-0.0224,  0.0437])\n",
      "loss = 0.36452001333236694\n",
      "Iteration 63: b = tensor([ 1.7472, -3.8920], grad_fn=<SubBackward0>), Loss = 0.36452001333236694\n",
      "tensor([-0.0221,  0.0432])\n",
      "loss = 0.36217591166496277\n",
      "Iteration 64: b = tensor([ 1.7691, -3.9348], grad_fn=<SubBackward0>), Loss = 0.36217591166496277\n",
      "tensor([-0.0219,  0.0428])\n",
      "loss = 0.3598790764808655\n",
      "Iteration 65: b = tensor([ 1.7908, -3.9771], grad_fn=<SubBackward0>), Loss = 0.3598790764808655\n",
      "tensor([-0.0217,  0.0423])\n",
      "loss = 0.3576280176639557\n",
      "Iteration 66: b = tensor([ 1.8122, -4.0191], grad_fn=<SubBackward0>), Loss = 0.3576280176639557\n",
      "tensor([-0.0214,  0.0419])\n",
      "loss = 0.3554215133190155\n",
      "Iteration 67: b = tensor([ 1.8334, -4.0606], grad_fn=<SubBackward0>), Loss = 0.3554215133190155\n",
      "tensor([-0.0212,  0.0415])\n",
      "loss = 0.3532581329345703\n",
      "Iteration 68: b = tensor([ 1.8544, -4.1017], grad_fn=<SubBackward0>), Loss = 0.3532581329345703\n",
      "tensor([-0.0210,  0.0411])\n",
      "loss = 0.35113680362701416\n",
      "Iteration 69: b = tensor([ 1.8752, -4.1424], grad_fn=<SubBackward0>), Loss = 0.35113680362701416\n",
      "tensor([-0.0208,  0.0407])\n",
      "loss = 0.34905627369880676\n",
      "Iteration 70: b = tensor([ 1.8958, -4.1827], grad_fn=<SubBackward0>), Loss = 0.34905627369880676\n",
      "tensor([-0.0206,  0.0403])\n",
      "loss = 0.3470153212547302\n",
      "Iteration 71: b = tensor([ 1.9162, -4.2227], grad_fn=<SubBackward0>), Loss = 0.3470153212547302\n",
      "tensor([-0.0204,  0.0399])\n",
      "loss = 0.345012903213501\n",
      "Iteration 72: b = tensor([ 1.9364, -4.2623], grad_fn=<SubBackward0>), Loss = 0.345012903213501\n",
      "tensor([-0.0202,  0.0396])\n",
      "loss = 0.3430479168891907\n",
      "Iteration 73: b = tensor([ 1.9564, -4.3015], grad_fn=<SubBackward0>), Loss = 0.3430479168891907\n",
      "tensor([-0.0200,  0.0392])\n",
      "loss = 0.34111934900283813\n",
      "Iteration 74: b = tensor([ 1.9762, -4.3403], grad_fn=<SubBackward0>), Loss = 0.34111934900283813\n",
      "tensor([-0.0198,  0.0388])\n",
      "loss = 0.33922621607780457\n",
      "Iteration 75: b = tensor([ 1.9959, -4.3788], grad_fn=<SubBackward0>), Loss = 0.33922621607780457\n",
      "tensor([-0.0196,  0.0385])\n",
      "loss = 0.33736753463745117\n",
      "Iteration 76: b = tensor([ 2.0153, -4.4170], grad_fn=<SubBackward0>), Loss = 0.33736753463745117\n",
      "tensor([-0.0194,  0.0381])\n",
      "loss = 0.33554238080978394\n",
      "Iteration 77: b = tensor([ 2.0346, -4.4548], grad_fn=<SubBackward0>), Loss = 0.33554238080978394\n",
      "tensor([-0.0193,  0.0378])\n",
      "loss = 0.3337498903274536\n",
      "Iteration 78: b = tensor([ 2.0537, -4.4922], grad_fn=<SubBackward0>), Loss = 0.3337498903274536\n",
      "tensor([-0.0191,  0.0375])\n",
      "loss = 0.3319891095161438\n",
      "Iteration 79: b = tensor([ 2.0726, -4.5294], grad_fn=<SubBackward0>), Loss = 0.3319891095161438\n",
      "tensor([-0.0189,  0.0371])\n",
      "loss = 0.3302592635154724\n",
      "Iteration 80: b = tensor([ 2.0914, -4.5662], grad_fn=<SubBackward0>), Loss = 0.3302592635154724\n",
      "tensor([-0.0188,  0.0368])\n",
      "loss = 0.32855960726737976\n",
      "Iteration 81: b = tensor([ 2.1099, -4.6027], grad_fn=<SubBackward0>), Loss = 0.32855960726737976\n",
      "tensor([-0.0186,  0.0365])\n",
      "loss = 0.32688918709754944\n",
      "Iteration 82: b = tensor([ 2.1284, -4.6389], grad_fn=<SubBackward0>), Loss = 0.32688918709754944\n",
      "tensor([-0.0184,  0.0362])\n",
      "loss = 0.3252473771572113\n",
      "Iteration 83: b = tensor([ 2.1466, -4.6748], grad_fn=<SubBackward0>), Loss = 0.3252473771572113\n",
      "tensor([-0.0183,  0.0359])\n",
      "loss = 0.32363346219062805\n",
      "Iteration 84: b = tensor([ 2.1647, -4.7103], grad_fn=<SubBackward0>), Loss = 0.32363346219062805\n",
      "tensor([-0.0181,  0.0356])\n",
      "loss = 0.3220466673374176\n",
      "Iteration 85: b = tensor([ 2.1827, -4.7456], grad_fn=<SubBackward0>), Loss = 0.3220466673374176\n",
      "tensor([-0.0179,  0.0353])\n",
      "loss = 0.3204863369464874\n",
      "Iteration 86: b = tensor([ 2.2005, -4.7806], grad_fn=<SubBackward0>), Loss = 0.3204863369464874\n",
      "tensor([-0.0178,  0.0350])\n",
      "loss = 0.3189517855644226\n",
      "Iteration 87: b = tensor([ 2.2181, -4.8153], grad_fn=<SubBackward0>), Loss = 0.3189517855644226\n",
      "tensor([-0.0176,  0.0347])\n",
      "loss = 0.3174424171447754\n",
      "Iteration 88: b = tensor([ 2.2356, -4.8497], grad_fn=<SubBackward0>), Loss = 0.3174424171447754\n",
      "tensor([-0.0175,  0.0344])\n",
      "loss = 0.31595757603645325\n",
      "Iteration 89: b = tensor([ 2.2530, -4.8839], grad_fn=<SubBackward0>), Loss = 0.31595757603645325\n",
      "tensor([-0.0174,  0.0341])\n",
      "loss = 0.31449663639068604\n",
      "Iteration 90: b = tensor([ 2.2702, -4.9178], grad_fn=<SubBackward0>), Loss = 0.31449663639068604\n",
      "tensor([-0.0172,  0.0339])\n",
      "loss = 0.31305912137031555\n",
      "Iteration 91: b = tensor([ 2.2872, -4.9514], grad_fn=<SubBackward0>), Loss = 0.31305912137031555\n",
      "tensor([-0.0171,  0.0336])\n",
      "loss = 0.3116443455219269\n",
      "Iteration 92: b = tensor([ 2.3042, -4.9847], grad_fn=<SubBackward0>), Loss = 0.3116443455219269\n",
      "tensor([-0.0169,  0.0333])\n",
      "loss = 0.3102518618106842\n",
      "Iteration 93: b = tensor([ 2.3210, -5.0178], grad_fn=<SubBackward0>), Loss = 0.3102518618106842\n",
      "tensor([-0.0168,  0.0331])\n",
      "loss = 0.30888107419013977\n",
      "Iteration 94: b = tensor([ 2.3376, -5.0506], grad_fn=<SubBackward0>), Loss = 0.30888107419013977\n",
      "tensor([-0.0167,  0.0328])\n",
      "loss = 0.3075315058231354\n",
      "Iteration 95: b = tensor([ 2.3542, -5.0832], grad_fn=<SubBackward0>), Loss = 0.3075315058231354\n",
      "tensor([-0.0165,  0.0326])\n",
      "loss = 0.3062026798725128\n",
      "Iteration 96: b = tensor([ 2.3706, -5.1155], grad_fn=<SubBackward0>), Loss = 0.3062026798725128\n",
      "tensor([-0.0164,  0.0323])\n",
      "loss = 0.30489403009414673\n",
      "Iteration 97: b = tensor([ 2.3869, -5.1475], grad_fn=<SubBackward0>), Loss = 0.30489403009414673\n",
      "tensor([-0.0163,  0.0321])\n",
      "loss = 0.30360519886016846\n",
      "Iteration 98: b = tensor([ 2.4030, -5.1794], grad_fn=<SubBackward0>), Loss = 0.30360519886016846\n",
      "tensor([-0.0162,  0.0318])\n",
      "loss = 0.30233561992645264\n",
      "Iteration 99: b = tensor([ 2.4190, -5.2110], grad_fn=<SubBackward0>), Loss = 0.30233561992645264\n",
      "tensor([-0.0160,  0.0316])\n",
      "loss = 0.3010849952697754\n",
      "Iteration 100: b = tensor([ 2.4349, -5.2423], grad_fn=<SubBackward0>), Loss = 0.3010849952697754\n",
      "tensor([-0.0159,  0.0314])\n",
      "loss = 0.29985278844833374\n",
      "Iteration 101: b = tensor([ 2.4507, -5.2735], grad_fn=<SubBackward0>), Loss = 0.29985278844833374\n",
      "tensor([-0.0158,  0.0311])\n",
      "loss = 0.29863858222961426\n",
      "Iteration 102: b = tensor([ 2.4664, -5.3044], grad_fn=<SubBackward0>), Loss = 0.29863858222961426\n",
      "tensor([-0.0157,  0.0309])\n",
      "loss = 0.2974419891834259\n",
      "Iteration 103: b = tensor([ 2.4820, -5.3351], grad_fn=<SubBackward0>), Loss = 0.2974419891834259\n",
      "tensor([-0.0156,  0.0307])\n",
      "loss = 0.2962626814842224\n",
      "Iteration 104: b = tensor([ 2.4974, -5.3655], grad_fn=<SubBackward0>), Loss = 0.2962626814842224\n",
      "tensor([-0.0154,  0.0305])\n",
      "loss = 0.29510024189949036\n",
      "Iteration 105: b = tensor([ 2.5127, -5.3958], grad_fn=<SubBackward0>), Loss = 0.29510024189949036\n",
      "tensor([-0.0153,  0.0302])\n",
      "loss = 0.2939542531967163\n",
      "Iteration 106: b = tensor([ 2.5280, -5.4258], grad_fn=<SubBackward0>), Loss = 0.2939542531967163\n",
      "tensor([-0.0152,  0.0300])\n",
      "loss = 0.2928244471549988\n",
      "Iteration 107: b = tensor([ 2.5431, -5.4556], grad_fn=<SubBackward0>), Loss = 0.2928244471549988\n",
      "tensor([-0.0151,  0.0298])\n",
      "loss = 0.2917104661464691\n",
      "Iteration 108: b = tensor([ 2.5581, -5.4853], grad_fn=<SubBackward0>), Loss = 0.2917104661464691\n",
      "tensor([-0.0150,  0.0296])\n",
      "loss = 0.2906118631362915\n",
      "Iteration 109: b = tensor([ 2.5730, -5.5147], grad_fn=<SubBackward0>), Loss = 0.2906118631362915\n",
      "tensor([-0.0149,  0.0294])\n",
      "loss = 0.28952842950820923\n",
      "Iteration 110: b = tensor([ 2.5878, -5.5439], grad_fn=<SubBackward0>), Loss = 0.28952842950820923\n",
      "tensor([-0.0148,  0.0292])\n",
      "loss = 0.28845980763435364\n",
      "Iteration 111: b = tensor([ 2.6025, -5.5729], grad_fn=<SubBackward0>), Loss = 0.28845980763435364\n",
      "tensor([-0.0147,  0.0290])\n",
      "loss = 0.28740566968917847\n",
      "Iteration 112: b = tensor([ 2.6171, -5.6017], grad_fn=<SubBackward0>), Loss = 0.28740566968917847\n",
      "tensor([-0.0146,  0.0288])\n",
      "loss = 0.28636571764945984\n",
      "Iteration 113: b = tensor([ 2.6316, -5.6303], grad_fn=<SubBackward0>), Loss = 0.28636571764945984\n",
      "tensor([-0.0145,  0.0286])\n",
      "loss = 0.28533968329429626\n",
      "Iteration 114: b = tensor([ 2.6459, -5.6588], grad_fn=<SubBackward0>), Loss = 0.28533968329429626\n",
      "tensor([-0.0144,  0.0284])\n",
      "loss = 0.28432726860046387\n",
      "Iteration 115: b = tensor([ 2.6602, -5.6870], grad_fn=<SubBackward0>), Loss = 0.28432726860046387\n",
      "tensor([-0.0143,  0.0282])\n",
      "loss = 0.28332820534706116\n",
      "Iteration 116: b = tensor([ 2.6745, -5.7151], grad_fn=<SubBackward0>), Loss = 0.28332820534706116\n",
      "tensor([-0.0142,  0.0281])\n",
      "loss = 0.28234216570854187\n",
      "Iteration 117: b = tensor([ 2.6886, -5.7430], grad_fn=<SubBackward0>), Loss = 0.28234216570854187\n",
      "tensor([-0.0141,  0.0279])\n",
      "loss = 0.28136903047561646\n",
      "Iteration 118: b = tensor([ 2.7026, -5.7707], grad_fn=<SubBackward0>), Loss = 0.28136903047561646\n",
      "tensor([-0.0140,  0.0277])\n",
      "loss = 0.2804083526134491\n",
      "Iteration 119: b = tensor([ 2.7165, -5.7982], grad_fn=<SubBackward0>), Loss = 0.2804083526134491\n",
      "tensor([-0.0139,  0.0275])\n",
      "loss = 0.27946004271507263\n",
      "Iteration 120: b = tensor([ 2.7303, -5.8255], grad_fn=<SubBackward0>), Loss = 0.27946004271507263\n",
      "tensor([-0.0138,  0.0273])\n",
      "loss = 0.2785237431526184\n",
      "Iteration 121: b = tensor([ 2.7441, -5.8527], grad_fn=<SubBackward0>), Loss = 0.2785237431526184\n",
      "tensor([-0.0137,  0.0272])\n",
      "loss = 0.2775992751121521\n",
      "Iteration 122: b = tensor([ 2.7577, -5.8797], grad_fn=<SubBackward0>), Loss = 0.2775992751121521\n",
      "tensor([-0.0137,  0.0270])\n",
      "loss = 0.2766864001750946\n",
      "Iteration 123: b = tensor([ 2.7713, -5.9065], grad_fn=<SubBackward0>), Loss = 0.2766864001750946\n",
      "tensor([-0.0136,  0.0268])\n",
      "loss = 0.2757848799228668\n",
      "Iteration 124: b = tensor([ 2.7848, -5.9332], grad_fn=<SubBackward0>), Loss = 0.2757848799228668\n",
      "tensor([-0.0135,  0.0267])\n",
      "loss = 0.2748945653438568\n",
      "Iteration 125: b = tensor([ 2.7982, -5.9597], grad_fn=<SubBackward0>), Loss = 0.2748945653438568\n",
      "tensor([-0.0134,  0.0265])\n",
      "loss = 0.2740151286125183\n",
      "Iteration 126: b = tensor([ 2.8115, -5.9861], grad_fn=<SubBackward0>), Loss = 0.2740151286125183\n",
      "tensor([-0.0133,  0.0263])\n",
      "loss = 0.27314648032188416\n",
      "Iteration 127: b = tensor([ 2.8248, -6.0122], grad_fn=<SubBackward0>), Loss = 0.27314648032188416\n",
      "tensor([-0.0132,  0.0262])\n",
      "loss = 0.27228832244873047\n",
      "Iteration 128: b = tensor([ 2.8379, -6.0383], grad_fn=<SubBackward0>), Loss = 0.27228832244873047\n",
      "tensor([-0.0132,  0.0260])\n",
      "loss = 0.2714405059814453\n",
      "Iteration 129: b = tensor([ 2.8510, -6.0641], grad_fn=<SubBackward0>), Loss = 0.2714405059814453\n",
      "tensor([-0.0131,  0.0259])\n",
      "loss = 0.270602822303772\n",
      "Iteration 130: b = tensor([ 2.8640, -6.0899], grad_fn=<SubBackward0>), Loss = 0.270602822303772\n",
      "tensor([-0.0130,  0.0257])\n",
      "loss = 0.2697750926017761\n",
      "Iteration 131: b = tensor([ 2.8769, -6.1154], grad_fn=<SubBackward0>), Loss = 0.2697750926017761\n",
      "tensor([-0.0129,  0.0256])\n",
      "loss = 0.26895707845687866\n",
      "Iteration 132: b = tensor([ 2.8898, -6.1408], grad_fn=<SubBackward0>), Loss = 0.26895707845687866\n",
      "tensor([-0.0128,  0.0254])\n",
      "loss = 0.2681487202644348\n",
      "Iteration 133: b = tensor([ 2.9025, -6.1661], grad_fn=<SubBackward0>), Loss = 0.2681487202644348\n",
      "tensor([-0.0128,  0.0253])\n",
      "loss = 0.2673497200012207\n",
      "Iteration 134: b = tensor([ 2.9152, -6.1912], grad_fn=<SubBackward0>), Loss = 0.2673497200012207\n",
      "tensor([-0.0127,  0.0251])\n",
      "loss = 0.26656001806259155\n",
      "Iteration 135: b = tensor([ 2.9278, -6.2162], grad_fn=<SubBackward0>), Loss = 0.26656001806259155\n",
      "tensor([-0.0126,  0.0250])\n",
      "loss = 0.26577940583229065\n",
      "Iteration 136: b = tensor([ 2.9404, -6.2410], grad_fn=<SubBackward0>), Loss = 0.26577940583229065\n",
      "tensor([-0.0125,  0.0248])\n",
      "loss = 0.2650076150894165\n",
      "Iteration 137: b = tensor([ 2.9528, -6.2657], grad_fn=<SubBackward0>), Loss = 0.2650076150894165\n",
      "tensor([-0.0125,  0.0247])\n",
      "loss = 0.2642446756362915\n",
      "Iteration 138: b = tensor([ 2.9652, -6.2903], grad_fn=<SubBackward0>), Loss = 0.2642446756362915\n",
      "tensor([-0.0124,  0.0246])\n",
      "loss = 0.26349034905433655\n",
      "Iteration 139: b = tensor([ 2.9776, -6.3147], grad_fn=<SubBackward0>), Loss = 0.26349034905433655\n",
      "tensor([-0.0123,  0.0244])\n",
      "loss = 0.2627444267272949\n",
      "Iteration 140: b = tensor([ 2.9898, -6.3390], grad_fn=<SubBackward0>), Loss = 0.2627444267272949\n",
      "tensor([-0.0123,  0.0243])\n",
      "loss = 0.2620067894458771\n",
      "Iteration 141: b = tensor([ 3.0020, -6.3631], grad_fn=<SubBackward0>), Loss = 0.2620067894458771\n",
      "tensor([-0.0122,  0.0241])\n",
      "loss = 0.26127734780311584\n",
      "Iteration 142: b = tensor([ 3.0141, -6.3871], grad_fn=<SubBackward0>), Loss = 0.26127734780311584\n",
      "tensor([-0.0121,  0.0240])\n",
      "loss = 0.2605558931827545\n",
      "Iteration 143: b = tensor([ 3.0262, -6.4110], grad_fn=<SubBackward0>), Loss = 0.2605558931827545\n",
      "tensor([-0.0121,  0.0239])\n",
      "loss = 0.2598423957824707\n",
      "Iteration 144: b = tensor([ 3.0382, -6.4347], grad_fn=<SubBackward0>), Loss = 0.2598423957824707\n",
      "tensor([-0.0120,  0.0237])\n",
      "loss = 0.2591365575790405\n",
      "Iteration 145: b = tensor([ 3.0501, -6.4583], grad_fn=<SubBackward0>), Loss = 0.2591365575790405\n",
      "tensor([-0.0119,  0.0236])\n",
      "loss = 0.258438378572464\n",
      "Iteration 146: b = tensor([ 3.0620, -6.4818], grad_fn=<SubBackward0>), Loss = 0.258438378572464\n",
      "tensor([-0.0119,  0.0235])\n",
      "loss = 0.25774767994880676\n",
      "Iteration 147: b = tensor([ 3.0738, -6.5052], grad_fn=<SubBackward0>), Loss = 0.25774767994880676\n",
      "tensor([-0.0118,  0.0234])\n",
      "loss = 0.2570643424987793\n",
      "Iteration 148: b = tensor([ 3.0855, -6.5285], grad_fn=<SubBackward0>), Loss = 0.2570643424987793\n",
      "tensor([-0.0117,  0.0232])\n",
      "loss = 0.25638827681541443\n",
      "Iteration 149: b = tensor([ 3.0972, -6.5516], grad_fn=<SubBackward0>), Loss = 0.25638827681541443\n",
      "tensor([-0.0117,  0.0231])\n",
      "loss = 0.2557193338871002\n",
      "Iteration 150: b = tensor([ 3.1088, -6.5746], grad_fn=<SubBackward0>), Loss = 0.2557193338871002\n",
      "tensor([-0.0116,  0.0230])\n",
      "loss = 0.25505736470222473\n",
      "Iteration 151: b = tensor([ 3.1203, -6.5974], grad_fn=<SubBackward0>), Loss = 0.25505736470222473\n",
      "tensor([-0.0115,  0.0229])\n",
      "loss = 0.2544023096561432\n",
      "Iteration 152: b = tensor([ 3.1318, -6.6202], grad_fn=<SubBackward0>), Loss = 0.2544023096561432\n",
      "tensor([-0.0115,  0.0228])\n",
      "loss = 0.25375398993492126\n",
      "Iteration 153: b = tensor([ 3.1432, -6.6429], grad_fn=<SubBackward0>), Loss = 0.25375398993492126\n",
      "tensor([-0.0114,  0.0226])\n",
      "loss = 0.2531123757362366\n",
      "Iteration 154: b = tensor([ 3.1546, -6.6654], grad_fn=<SubBackward0>), Loss = 0.2531123757362366\n",
      "tensor([-0.0114,  0.0225])\n",
      "loss = 0.2524772882461548\n",
      "Iteration 155: b = tensor([ 3.1659, -6.6878], grad_fn=<SubBackward0>), Loss = 0.2524772882461548\n",
      "tensor([-0.0113,  0.0224])\n",
      "loss = 0.2518486976623535\n",
      "Iteration 156: b = tensor([ 3.1772, -6.7101], grad_fn=<SubBackward0>), Loss = 0.2518486976623535\n",
      "tensor([-0.0113,  0.0223])\n",
      "loss = 0.2512264549732208\n",
      "Iteration 157: b = tensor([ 3.1884, -6.7323], grad_fn=<SubBackward0>), Loss = 0.2512264549732208\n",
      "tensor([-0.0112,  0.0222])\n",
      "loss = 0.25061047077178955\n",
      "Iteration 158: b = tensor([ 3.1995, -6.7544], grad_fn=<SubBackward0>), Loss = 0.25061047077178955\n",
      "tensor([-0.0111,  0.0221])\n",
      "loss = 0.25000059604644775\n",
      "Iteration 159: b = tensor([ 3.2106, -6.7763], grad_fn=<SubBackward0>), Loss = 0.25000059604644775\n",
      "tensor([-0.0111,  0.0220])\n",
      "loss = 0.24939684569835663\n",
      "Iteration 160: b = tensor([ 3.2216, -6.7982], grad_fn=<SubBackward0>), Loss = 0.24939684569835663\n",
      "tensor([-0.0110,  0.0219])\n",
      "loss = 0.24879899621009827\n",
      "Iteration 161: b = tensor([ 3.2326, -6.8199], grad_fn=<SubBackward0>), Loss = 0.24879899621009827\n",
      "tensor([-0.0110,  0.0217])\n",
      "loss = 0.24820706248283386\n",
      "Iteration 162: b = tensor([ 3.2435, -6.8416], grad_fn=<SubBackward0>), Loss = 0.24820706248283386\n",
      "tensor([-0.0109,  0.0216])\n",
      "loss = 0.24762088060379028\n",
      "Iteration 163: b = tensor([ 3.2543, -6.8631], grad_fn=<SubBackward0>), Loss = 0.24762088060379028\n",
      "tensor([-0.0109,  0.0215])\n",
      "loss = 0.24704039096832275\n",
      "Iteration 164: b = tensor([ 3.2652, -6.8845], grad_fn=<SubBackward0>), Loss = 0.24704039096832275\n",
      "tensor([-0.0108,  0.0214])\n",
      "loss = 0.2464655190706253\n",
      "Iteration 165: b = tensor([ 3.2759, -6.9059], grad_fn=<SubBackward0>), Loss = 0.2464655190706253\n",
      "tensor([-0.0108,  0.0213])\n",
      "loss = 0.24589620530605316\n",
      "Iteration 166: b = tensor([ 3.2866, -6.9271], grad_fn=<SubBackward0>), Loss = 0.24589620530605316\n",
      "tensor([-0.0107,  0.0212])\n",
      "loss = 0.2453322857618332\n",
      "Iteration 167: b = tensor([ 3.2973, -6.9482], grad_fn=<SubBackward0>), Loss = 0.2453322857618332\n",
      "tensor([-0.0107,  0.0211])\n",
      "loss = 0.244773730635643\n",
      "Iteration 168: b = tensor([ 3.3079, -6.9693], grad_fn=<SubBackward0>), Loss = 0.244773730635643\n",
      "tensor([-0.0106,  0.0210])\n",
      "loss = 0.24422043561935425\n",
      "Iteration 169: b = tensor([ 3.3184, -6.9902], grad_fn=<SubBackward0>), Loss = 0.24422043561935425\n",
      "tensor([-0.0106,  0.0209])\n",
      "loss = 0.24367234110832214\n",
      "Iteration 170: b = tensor([ 3.3289, -7.0110], grad_fn=<SubBackward0>), Loss = 0.24367234110832214\n",
      "tensor([-0.0105,  0.0208])\n",
      "loss = 0.2431294023990631\n",
      "Iteration 171: b = tensor([ 3.3394, -7.0318], grad_fn=<SubBackward0>), Loss = 0.2431294023990631\n",
      "tensor([-0.0105,  0.0207])\n",
      "loss = 0.2425914704799652\n",
      "Iteration 172: b = tensor([ 3.3498, -7.0524], grad_fn=<SubBackward0>), Loss = 0.2425914704799652\n",
      "tensor([-0.0104,  0.0206])\n",
      "loss = 0.24205851554870605\n",
      "Iteration 173: b = tensor([ 3.3601, -7.0729], grad_fn=<SubBackward0>), Loss = 0.24205851554870605\n",
      "tensor([-0.0104,  0.0205])\n",
      "loss = 0.24153047800064087\n",
      "Iteration 174: b = tensor([ 3.3704, -7.0934], grad_fn=<SubBackward0>), Loss = 0.24153047800064087\n",
      "tensor([-0.0103,  0.0204])\n",
      "loss = 0.2410072535276413\n",
      "Iteration 175: b = tensor([ 3.3807, -7.1137], grad_fn=<SubBackward0>), Loss = 0.2410072535276413\n",
      "tensor([-0.0103,  0.0204])\n",
      "loss = 0.24048878252506256\n",
      "Iteration 176: b = tensor([ 3.3909, -7.1340], grad_fn=<SubBackward0>), Loss = 0.24048878252506256\n",
      "tensor([-0.0102,  0.0203])\n",
      "loss = 0.23997503519058228\n",
      "Iteration 177: b = tensor([ 3.4011, -7.1542], grad_fn=<SubBackward0>), Loss = 0.23997503519058228\n",
      "tensor([-0.0102,  0.0202])\n",
      "loss = 0.2394658774137497\n",
      "Iteration 178: b = tensor([ 3.4112, -7.1743], grad_fn=<SubBackward0>), Loss = 0.2394658774137497\n",
      "tensor([-0.0101,  0.0201])\n",
      "loss = 0.23896129429340363\n",
      "Iteration 179: b = tensor([ 3.4213, -7.1942], grad_fn=<SubBackward0>), Loss = 0.23896129429340363\n",
      "tensor([-0.0101,  0.0200])\n",
      "loss = 0.2384612262248993\n",
      "Iteration 180: b = tensor([ 3.4313, -7.2142], grad_fn=<SubBackward0>), Loss = 0.2384612262248993\n",
      "tensor([-0.0100,  0.0199])\n",
      "loss = 0.23796555399894714\n",
      "Iteration 181: b = tensor([ 3.4413, -7.2340], grad_fn=<SubBackward0>), Loss = 0.23796555399894714\n",
      "tensor([-0.0100,  0.0198])\n",
      "loss = 0.2374742478132248\n",
      "Iteration 182: b = tensor([ 3.4512, -7.2537], grad_fn=<SubBackward0>), Loss = 0.2374742478132248\n",
      "tensor([-0.0099,  0.0197])\n",
      "loss = 0.23698727786540985\n",
      "Iteration 183: b = tensor([ 3.4611, -7.2733], grad_fn=<SubBackward0>), Loss = 0.23698727786540985\n",
      "tensor([-0.0099,  0.0196])\n",
      "loss = 0.23650452494621277\n",
      "Iteration 184: b = tensor([ 3.4710, -7.2929], grad_fn=<SubBackward0>), Loss = 0.23650452494621277\n",
      "tensor([-0.0099,  0.0196])\n",
      "loss = 0.23602603375911713\n",
      "Iteration 185: b = tensor([ 3.4808, -7.3124], grad_fn=<SubBackward0>), Loss = 0.23602603375911713\n",
      "tensor([-0.0098,  0.0195])\n",
      "loss = 0.2355515956878662\n",
      "Iteration 186: b = tensor([ 3.4906, -7.3318], grad_fn=<SubBackward0>), Loss = 0.2355515956878662\n",
      "tensor([-0.0098,  0.0194])\n",
      "loss = 0.235081285238266\n",
      "Iteration 187: b = tensor([ 3.5003, -7.3511], grad_fn=<SubBackward0>), Loss = 0.235081285238266\n",
      "tensor([-0.0097,  0.0193])\n",
      "loss = 0.23461496829986572\n",
      "Iteration 188: b = tensor([ 3.5100, -7.3703], grad_fn=<SubBackward0>), Loss = 0.23461496829986572\n",
      "tensor([-0.0097,  0.0192])\n",
      "loss = 0.23415260016918182\n",
      "Iteration 189: b = tensor([ 3.5196, -7.3894], grad_fn=<SubBackward0>), Loss = 0.23415260016918182\n",
      "tensor([-0.0096,  0.0191])\n",
      "loss = 0.2336941808462143\n",
      "Iteration 190: b = tensor([ 3.5292, -7.4085], grad_fn=<SubBackward0>), Loss = 0.2336941808462143\n",
      "tensor([-0.0096,  0.0191])\n",
      "loss = 0.2332395762205124\n",
      "Iteration 191: b = tensor([ 3.5388, -7.4275], grad_fn=<SubBackward0>), Loss = 0.2332395762205124\n",
      "tensor([-0.0096,  0.0190])\n",
      "loss = 0.23278887569904327\n",
      "Iteration 192: b = tensor([ 3.5483, -7.4464], grad_fn=<SubBackward0>), Loss = 0.23278887569904327\n",
      "tensor([-0.0095,  0.0189])\n",
      "loss = 0.23234182596206665\n",
      "Iteration 193: b = tensor([ 3.5578, -7.4652], grad_fn=<SubBackward0>), Loss = 0.23234182596206665\n",
      "tensor([-0.0095,  0.0188])\n",
      "loss = 0.23189854621887207\n",
      "Iteration 194: b = tensor([ 3.5673, -7.4839], grad_fn=<SubBackward0>), Loss = 0.23189854621887207\n",
      "tensor([-0.0094,  0.0187])\n",
      "loss = 0.2314589023590088\n",
      "Iteration 195: b = tensor([ 3.5767, -7.5026], grad_fn=<SubBackward0>), Loss = 0.2314589023590088\n",
      "tensor([-0.0094,  0.0187])\n",
      "loss = 0.231022909283638\n",
      "Iteration 196: b = tensor([ 3.5860, -7.5212], grad_fn=<SubBackward0>), Loss = 0.231022909283638\n",
      "tensor([-0.0094,  0.0186])\n",
      "loss = 0.23059043288230896\n",
      "Iteration 197: b = tensor([ 3.5954, -7.5397], grad_fn=<SubBackward0>), Loss = 0.23059043288230896\n",
      "tensor([-0.0093,  0.0185])\n",
      "loss = 0.23016150295734406\n",
      "Iteration 198: b = tensor([ 3.6046, -7.5581], grad_fn=<SubBackward0>), Loss = 0.23016150295734406\n",
      "tensor([-0.0093,  0.0184])\n",
      "loss = 0.2297360599040985\n",
      "Iteration 199: b = tensor([ 3.6139, -7.5765], grad_fn=<SubBackward0>), Loss = 0.2297360599040985\n",
      "tensor([-0.0093,  0.0184])\n",
      "loss = 0.22931404411792755\n",
      "Iteration 200: b = tensor([ 3.6231, -7.5948], grad_fn=<SubBackward0>), Loss = 0.22931404411792755\n",
      "tensor([-0.0092,  0.0183])\n",
      "loss = 0.22889535129070282\n",
      "Iteration 201: b = tensor([ 3.6323, -7.6130], grad_fn=<SubBackward0>), Loss = 0.22889535129070282\n",
      "tensor([-0.0092,  0.0182])\n",
      "loss = 0.22848010063171387\n",
      "Iteration 202: b = tensor([ 3.6414, -7.6312], grad_fn=<SubBackward0>), Loss = 0.22848010063171387\n",
      "tensor([-0.0091,  0.0181])\n",
      "loss = 0.22806811332702637\n",
      "Iteration 203: b = tensor([ 3.6505, -7.6492], grad_fn=<SubBackward0>), Loss = 0.22806811332702637\n",
      "tensor([-0.0091,  0.0181])\n",
      "loss = 0.22765937447547913\n",
      "Iteration 204: b = tensor([ 3.6596, -7.6672], grad_fn=<SubBackward0>), Loss = 0.22765937447547913\n",
      "tensor([-0.0091,  0.0180])\n",
      "loss = 0.22725382447242737\n",
      "Iteration 205: b = tensor([ 3.6686, -7.6852], grad_fn=<SubBackward0>), Loss = 0.22725382447242737\n",
      "tensor([-0.0090,  0.0179])\n",
      "loss = 0.22685152292251587\n",
      "Iteration 206: b = tensor([ 3.6776, -7.7030], grad_fn=<SubBackward0>), Loss = 0.22685152292251587\n",
      "tensor([-0.0090,  0.0179])\n",
      "loss = 0.2264522910118103\n",
      "Iteration 207: b = tensor([ 3.6866, -7.7208], grad_fn=<SubBackward0>), Loss = 0.2264522910118103\n",
      "tensor([-0.0090,  0.0178])\n",
      "loss = 0.22605618834495544\n",
      "Iteration 208: b = tensor([ 3.6955, -7.7386], grad_fn=<SubBackward0>), Loss = 0.22605618834495544\n",
      "tensor([-0.0089,  0.0177])\n",
      "loss = 0.22566315531730652\n",
      "Iteration 209: b = tensor([ 3.7044, -7.7562], grad_fn=<SubBackward0>), Loss = 0.22566315531730652\n",
      "tensor([-0.0089,  0.0177])\n",
      "loss = 0.22527316212654114\n",
      "Iteration 210: b = tensor([ 3.7133, -7.7738], grad_fn=<SubBackward0>), Loss = 0.22527316212654114\n",
      "tensor([-0.0089,  0.0176])\n",
      "loss = 0.22488611936569214\n",
      "Iteration 211: b = tensor([ 3.7221, -7.7913], grad_fn=<SubBackward0>), Loss = 0.22488611936569214\n",
      "tensor([-0.0088,  0.0175])\n",
      "loss = 0.22450204193592072\n",
      "Iteration 212: b = tensor([ 3.7309, -7.8088], grad_fn=<SubBackward0>), Loss = 0.22450204193592072\n",
      "tensor([-0.0088,  0.0175])\n",
      "loss = 0.2241208702325821\n",
      "Iteration 213: b = tensor([ 3.7396, -7.8262], grad_fn=<SubBackward0>), Loss = 0.2241208702325821\n",
      "tensor([-0.0088,  0.0174])\n",
      "loss = 0.22374258935451508\n",
      "Iteration 214: b = tensor([ 3.7483, -7.8435], grad_fn=<SubBackward0>), Loss = 0.22374258935451508\n",
      "tensor([-0.0087,  0.0173])\n",
      "loss = 0.22336715459823608\n",
      "Iteration 215: b = tensor([ 3.7570, -7.8607], grad_fn=<SubBackward0>), Loss = 0.22336715459823608\n",
      "tensor([-0.0087,  0.0173])\n",
      "loss = 0.22299453616142273\n",
      "Iteration 216: b = tensor([ 3.7657, -7.8779], grad_fn=<SubBackward0>), Loss = 0.22299453616142273\n",
      "tensor([-0.0087,  0.0172])\n",
      "loss = 0.22262470424175262\n",
      "Iteration 217: b = tensor([ 3.7743, -7.8951], grad_fn=<SubBackward0>), Loss = 0.22262470424175262\n",
      "tensor([-0.0086,  0.0171])\n",
      "loss = 0.2222575694322586\n",
      "Iteration 218: b = tensor([ 3.7829, -7.9121], grad_fn=<SubBackward0>), Loss = 0.2222575694322586\n",
      "tensor([-0.0086,  0.0171])\n",
      "loss = 0.22189322113990784\n",
      "Iteration 219: b = tensor([ 3.7915, -7.9291], grad_fn=<SubBackward0>), Loss = 0.22189322113990784\n",
      "tensor([-0.0086,  0.0170])\n",
      "loss = 0.22153149545192719\n",
      "Iteration 220: b = tensor([ 3.8000, -7.9461], grad_fn=<SubBackward0>), Loss = 0.22153149545192719\n",
      "tensor([-0.0085,  0.0169])\n",
      "loss = 0.22117245197296143\n",
      "Iteration 221: b = tensor([ 3.8085, -7.9629], grad_fn=<SubBackward0>), Loss = 0.22117245197296143\n",
      "tensor([-0.0085,  0.0169])\n",
      "loss = 0.2208160161972046\n",
      "Iteration 222: b = tensor([ 3.8170, -7.9798], grad_fn=<SubBackward0>), Loss = 0.2208160161972046\n",
      "tensor([-0.0085,  0.0168])\n",
      "loss = 0.22046217322349548\n",
      "Iteration 223: b = tensor([ 3.8254, -7.9965], grad_fn=<SubBackward0>), Loss = 0.22046217322349548\n",
      "tensor([-0.0084,  0.0168])\n",
      "loss = 0.22011087834835052\n",
      "Iteration 224: b = tensor([ 3.8338, -8.0132], grad_fn=<SubBackward0>), Loss = 0.22011087834835052\n",
      "tensor([-0.0084,  0.0167])\n",
      "loss = 0.21976210176944733\n",
      "Iteration 225: b = tensor([ 3.8422, -8.0299], grad_fn=<SubBackward0>), Loss = 0.21976210176944733\n",
      "tensor([-0.0084,  0.0166])\n",
      "loss = 0.2194158434867859\n",
      "Iteration 226: b = tensor([ 3.8505, -8.0464], grad_fn=<SubBackward0>), Loss = 0.2194158434867859\n",
      "tensor([-0.0083,  0.0166])\n",
      "loss = 0.21907205879688263\n",
      "Iteration 227: b = tensor([ 3.8588, -8.0629], grad_fn=<SubBackward0>), Loss = 0.21907205879688263\n",
      "tensor([-0.0083,  0.0165])\n",
      "loss = 0.21873070299625397\n",
      "Iteration 228: b = tensor([ 3.8671, -8.0794], grad_fn=<SubBackward0>), Loss = 0.21873070299625397\n",
      "tensor([-0.0083,  0.0165])\n",
      "loss = 0.2183917909860611\n",
      "Iteration 229: b = tensor([ 3.8754, -8.0958], grad_fn=<SubBackward0>), Loss = 0.2183917909860611\n",
      "tensor([-0.0083,  0.0164])\n",
      "loss = 0.21805526316165924\n",
      "Iteration 230: b = tensor([ 3.8836, -8.1121], grad_fn=<SubBackward0>), Loss = 0.21805526316165924\n",
      "tensor([-0.0082,  0.0163])\n",
      "loss = 0.21772105991840363\n",
      "Iteration 231: b = tensor([ 3.8918, -8.1284], grad_fn=<SubBackward0>), Loss = 0.21772105991840363\n",
      "tensor([-0.0082,  0.0163])\n",
      "loss = 0.21738925576210022\n",
      "Iteration 232: b = tensor([ 3.9000, -8.1447], grad_fn=<SubBackward0>), Loss = 0.21738925576210022\n",
      "tensor([-0.0082,  0.0162])\n",
      "loss = 0.21705971658229828\n",
      "Iteration 233: b = tensor([ 3.9081, -8.1608], grad_fn=<SubBackward0>), Loss = 0.21705971658229828\n",
      "tensor([-0.0081,  0.0162])\n",
      "loss = 0.2167324274778366\n",
      "Iteration 234: b = tensor([ 3.9162, -8.1769], grad_fn=<SubBackward0>), Loss = 0.2167324274778366\n",
      "tensor([-0.0081,  0.0161])\n",
      "loss = 0.21640744805335999\n",
      "Iteration 235: b = tensor([ 3.9243, -8.1930], grad_fn=<SubBackward0>), Loss = 0.21640744805335999\n",
      "tensor([-0.0081,  0.0161])\n",
      "loss = 0.21608467400074005\n",
      "Iteration 236: b = tensor([ 3.9324, -8.2090], grad_fn=<SubBackward0>), Loss = 0.21608467400074005\n",
      "tensor([-0.0081,  0.0160])\n",
      "loss = 0.2157641500234604\n",
      "Iteration 237: b = tensor([ 3.9404, -8.2250], grad_fn=<SubBackward0>), Loss = 0.2157641500234604\n",
      "tensor([-0.0080,  0.0160])\n",
      "loss = 0.21544577181339264\n",
      "Iteration 238: b = tensor([ 3.9484, -8.2409], grad_fn=<SubBackward0>), Loss = 0.21544577181339264\n",
      "tensor([-0.0080,  0.0159])\n",
      "loss = 0.215129554271698\n",
      "Iteration 239: b = tensor([ 3.9564, -8.2567], grad_fn=<SubBackward0>), Loss = 0.215129554271698\n",
      "tensor([-0.0080,  0.0158])\n",
      "loss = 0.21481552720069885\n",
      "Iteration 240: b = tensor([ 3.9643, -8.2725], grad_fn=<SubBackward0>), Loss = 0.21481552720069885\n",
      "tensor([-0.0079,  0.0158])\n",
      "loss = 0.21450357139110565\n",
      "Iteration 241: b = tensor([ 3.9723, -8.2882], grad_fn=<SubBackward0>), Loss = 0.21450357139110565\n",
      "tensor([-0.0079,  0.0157])\n",
      "loss = 0.21419373154640198\n",
      "Iteration 242: b = tensor([ 3.9802, -8.3039], grad_fn=<SubBackward0>), Loss = 0.21419373154640198\n",
      "tensor([-0.0079,  0.0157])\n",
      "loss = 0.21388596296310425\n",
      "Iteration 243: b = tensor([ 3.9880, -8.3195], grad_fn=<SubBackward0>), Loss = 0.21388596296310425\n",
      "tensor([-0.0079,  0.0156])\n",
      "loss = 0.2135802060365677\n",
      "Iteration 244: b = tensor([ 3.9959, -8.3351], grad_fn=<SubBackward0>), Loss = 0.2135802060365677\n",
      "tensor([-0.0078,  0.0156])\n",
      "loss = 0.21327652037143707\n",
      "Iteration 245: b = tensor([ 4.0037, -8.3506], grad_fn=<SubBackward0>), Loss = 0.21327652037143707\n",
      "tensor([-0.0078,  0.0155])\n",
      "loss = 0.21297478675842285\n",
      "Iteration 246: b = tensor([ 4.0115, -8.3661], grad_fn=<SubBackward0>), Loss = 0.21297478675842285\n",
      "tensor([-0.0078,  0.0155])\n",
      "loss = 0.2126750648021698\n",
      "Iteration 247: b = tensor([ 4.0192, -8.3816], grad_fn=<SubBackward0>), Loss = 0.2126750648021698\n",
      "tensor([-0.0078,  0.0154])\n",
      "loss = 0.21237733960151672\n",
      "Iteration 248: b = tensor([ 4.0270, -8.3969], grad_fn=<SubBackward0>), Loss = 0.21237733960151672\n",
      "tensor([-0.0077,  0.0154])\n",
      "loss = 0.21208153665065765\n",
      "Iteration 249: b = tensor([ 4.0347, -8.4123], grad_fn=<SubBackward0>), Loss = 0.21208153665065765\n",
      "tensor([-0.0077,  0.0153])\n",
      "loss = 0.2117876261472702\n",
      "Iteration 250: b = tensor([ 4.0424, -8.4275], grad_fn=<SubBackward0>), Loss = 0.2117876261472702\n",
      "tensor([-0.0077,  0.0153])\n",
      "loss = 0.21149563789367676\n",
      "Iteration 251: b = tensor([ 4.0500, -8.4428], grad_fn=<SubBackward0>), Loss = 0.21149563789367676\n",
      "tensor([-0.0077,  0.0152])\n",
      "loss = 0.21120555698871613\n",
      "Iteration 252: b = tensor([ 4.0577, -8.4579], grad_fn=<SubBackward0>), Loss = 0.21120555698871613\n",
      "tensor([-0.0076,  0.0152])\n",
      "loss = 0.21091730892658234\n",
      "Iteration 253: b = tensor([ 4.0653, -8.4731], grad_fn=<SubBackward0>), Loss = 0.21091730892658234\n",
      "tensor([-0.0076,  0.0151])\n",
      "loss = 0.21063092350959778\n",
      "Iteration 254: b = tensor([ 4.0729, -8.4881], grad_fn=<SubBackward0>), Loss = 0.21063092350959778\n",
      "tensor([-0.0076,  0.0151])\n",
      "loss = 0.21034632623195648\n",
      "Iteration 255: b = tensor([ 4.0805, -8.5032], grad_fn=<SubBackward0>), Loss = 0.21034632623195648\n",
      "tensor([-0.0076,  0.0150])\n",
      "loss = 0.21006359159946442\n",
      "Iteration 256: b = tensor([ 4.0880, -8.5182], grad_fn=<SubBackward0>), Loss = 0.21006359159946442\n",
      "tensor([-0.0075,  0.0150])\n",
      "loss = 0.20978260040283203\n",
      "Iteration 257: b = tensor([ 4.0955, -8.5331], grad_fn=<SubBackward0>), Loss = 0.20978260040283203\n",
      "tensor([-0.0075,  0.0149])\n",
      "loss = 0.2095033824443817\n",
      "Iteration 258: b = tensor([ 4.1030, -8.5480], grad_fn=<SubBackward0>), Loss = 0.2095033824443817\n",
      "tensor([-0.0075,  0.0149])\n",
      "loss = 0.20922595262527466\n",
      "Iteration 259: b = tensor([ 4.1105, -8.5628], grad_fn=<SubBackward0>), Loss = 0.20922595262527466\n",
      "tensor([-0.0075,  0.0148])\n",
      "loss = 0.2089501917362213\n",
      "Iteration 260: b = tensor([ 4.1179, -8.5776], grad_fn=<SubBackward0>), Loss = 0.2089501917362213\n",
      "tensor([-0.0074,  0.0148])\n",
      "loss = 0.20867623388767242\n",
      "Iteration 261: b = tensor([ 4.1253, -8.5924], grad_fn=<SubBackward0>), Loss = 0.20867623388767242\n",
      "tensor([-0.0074,  0.0148])\n",
      "loss = 0.20840390026569366\n",
      "Iteration 262: b = tensor([ 4.1328, -8.6071], grad_fn=<SubBackward0>), Loss = 0.20840390026569366\n",
      "tensor([-0.0074,  0.0147])\n",
      "loss = 0.2081332951784134\n",
      "Iteration 263: b = tensor([ 4.1401, -8.6217], grad_fn=<SubBackward0>), Loss = 0.2081332951784134\n",
      "tensor([-0.0074,  0.0147])\n",
      "loss = 0.20786435902118683\n",
      "Iteration 264: b = tensor([ 4.1475, -8.6364], grad_fn=<SubBackward0>), Loss = 0.20786435902118683\n",
      "tensor([-0.0074,  0.0146])\n",
      "loss = 0.2075970470905304\n",
      "Iteration 265: b = tensor([ 4.1548, -8.6509], grad_fn=<SubBackward0>), Loss = 0.2075970470905304\n",
      "tensor([-0.0073,  0.0146])\n",
      "loss = 0.20733137428760529\n",
      "Iteration 266: b = tensor([ 4.1621, -8.6655], grad_fn=<SubBackward0>), Loss = 0.20733137428760529\n",
      "tensor([-0.0073,  0.0145])\n",
      "loss = 0.2070673257112503\n",
      "Iteration 267: b = tensor([ 4.1694, -8.6799], grad_fn=<SubBackward0>), Loss = 0.2070673257112503\n",
      "tensor([-0.0073,  0.0145])\n",
      "loss = 0.20680490136146545\n",
      "Iteration 268: b = tensor([ 4.1767, -8.6944], grad_fn=<SubBackward0>), Loss = 0.20680490136146545\n",
      "tensor([-0.0073,  0.0144])\n",
      "loss = 0.20654401183128357\n",
      "Iteration 269: b = tensor([ 4.1839, -8.7088], grad_fn=<SubBackward0>), Loss = 0.20654401183128357\n",
      "tensor([-0.0072,  0.0144])\n",
      "loss = 0.2062847763299942\n",
      "Iteration 270: b = tensor([ 4.1912, -8.7231], grad_fn=<SubBackward0>), Loss = 0.2062847763299942\n",
      "tensor([-0.0072,  0.0144])\n",
      "loss = 0.20602703094482422\n",
      "Iteration 271: b = tensor([ 4.1984, -8.7374], grad_fn=<SubBackward0>), Loss = 0.20602703094482422\n",
      "tensor([-0.0072,  0.0143])\n",
      "loss = 0.2057708501815796\n",
      "Iteration 272: b = tensor([ 4.2055, -8.7517], grad_fn=<SubBackward0>), Loss = 0.2057708501815796\n",
      "tensor([-0.0072,  0.0143])\n",
      "loss = 0.20551620423793793\n",
      "Iteration 273: b = tensor([ 4.2127, -8.7659], grad_fn=<SubBackward0>), Loss = 0.20551620423793793\n",
      "tensor([-0.0072,  0.0142])\n",
      "loss = 0.20526304841041565\n",
      "Iteration 274: b = tensor([ 4.2198, -8.7801], grad_fn=<SubBackward0>), Loss = 0.20526304841041565\n",
      "tensor([-0.0071,  0.0142])\n",
      "loss = 0.20501139760017395\n",
      "Iteration 275: b = tensor([ 4.2269, -8.7942], grad_fn=<SubBackward0>), Loss = 0.20501139760017395\n",
      "tensor([-0.0071,  0.0141])\n",
      "loss = 0.20476123690605164\n",
      "Iteration 276: b = tensor([ 4.2340, -8.8083], grad_fn=<SubBackward0>), Loss = 0.20476123690605164\n",
      "tensor([-0.0071,  0.0141])\n",
      "loss = 0.20451253652572632\n",
      "Iteration 277: b = tensor([ 4.2411, -8.8224], grad_fn=<SubBackward0>), Loss = 0.20451253652572632\n",
      "tensor([-0.0071,  0.0141])\n",
      "loss = 0.20426532626152039\n",
      "Iteration 278: b = tensor([ 4.2482, -8.8364], grad_fn=<SubBackward0>), Loss = 0.20426532626152039\n",
      "tensor([-0.0071,  0.0140])\n",
      "loss = 0.20401951670646667\n",
      "Iteration 279: b = tensor([ 4.2552, -8.8504], grad_fn=<SubBackward0>), Loss = 0.20401951670646667\n",
      "tensor([-0.0070,  0.0140])\n",
      "loss = 0.20377513766288757\n",
      "Iteration 280: b = tensor([ 4.2622, -8.8643], grad_fn=<SubBackward0>), Loss = 0.20377513766288757\n",
      "tensor([-0.0070,  0.0139])\n",
      "loss = 0.20353220403194427\n",
      "Iteration 281: b = tensor([ 4.2692, -8.8782], grad_fn=<SubBackward0>), Loss = 0.20353220403194427\n",
      "tensor([-0.0070,  0.0139])\n",
      "loss = 0.2032906711101532\n",
      "Iteration 282: b = tensor([ 4.2762, -8.8920], grad_fn=<SubBackward0>), Loss = 0.2032906711101532\n",
      "tensor([-0.0070,  0.0139])\n",
      "loss = 0.20305049419403076\n",
      "Iteration 283: b = tensor([ 4.2831, -8.9059], grad_fn=<SubBackward0>), Loss = 0.20305049419403076\n",
      "tensor([-0.0070,  0.0138])\n",
      "loss = 0.20281170308589935\n",
      "Iteration 284: b = tensor([ 4.2901, -8.9196], grad_fn=<SubBackward0>), Loss = 0.20281170308589935\n",
      "tensor([-0.0069,  0.0138])\n",
      "loss = 0.20257429778575897\n",
      "Iteration 285: b = tensor([ 4.2970, -8.9334], grad_fn=<SubBackward0>), Loss = 0.20257429778575897\n",
      "tensor([-0.0069,  0.0137])\n",
      "loss = 0.20233821868896484\n",
      "Iteration 286: b = tensor([ 4.3039, -8.9471], grad_fn=<SubBackward0>), Loss = 0.20233821868896484\n",
      "tensor([-0.0069,  0.0137])\n",
      "loss = 0.20210349559783936\n",
      "Iteration 287: b = tensor([ 4.3107, -8.9607], grad_fn=<SubBackward0>), Loss = 0.20210349559783936\n",
      "tensor([-0.0069,  0.0137])\n",
      "loss = 0.20187009871006012\n",
      "Iteration 288: b = tensor([ 4.3176, -8.9743], grad_fn=<SubBackward0>), Loss = 0.20187009871006012\n",
      "tensor([-0.0069,  0.0136])\n",
      "loss = 0.20163801312446594\n",
      "Iteration 289: b = tensor([ 4.3244, -8.9879], grad_fn=<SubBackward0>), Loss = 0.20163801312446594\n",
      "tensor([-0.0068,  0.0136])\n",
      "loss = 0.20140723884105682\n",
      "Iteration 290: b = tensor([ 4.3312, -9.0015], grad_fn=<SubBackward0>), Loss = 0.20140723884105682\n",
      "tensor([-0.0068,  0.0135])\n",
      "loss = 0.20117776095867157\n",
      "Iteration 291: b = tensor([ 4.3380, -9.0150], grad_fn=<SubBackward0>), Loss = 0.20117776095867157\n",
      "tensor([-0.0068,  0.0135])\n",
      "loss = 0.200949564576149\n",
      "Iteration 292: b = tensor([ 4.3448, -9.0284], grad_fn=<SubBackward0>), Loss = 0.200949564576149\n",
      "tensor([-0.0068,  0.0135])\n",
      "loss = 0.2007225900888443\n",
      "Iteration 293: b = tensor([ 4.3516, -9.0419], grad_fn=<SubBackward0>), Loss = 0.2007225900888443\n",
      "tensor([-0.0068,  0.0134])\n",
      "loss = 0.20049691200256348\n",
      "Iteration 294: b = tensor([ 4.3583, -9.0552], grad_fn=<SubBackward0>), Loss = 0.20049691200256348\n",
      "tensor([-0.0067,  0.0134])\n",
      "loss = 0.20027248561382294\n",
      "Iteration 295: b = tensor([ 4.3650, -9.0686], grad_fn=<SubBackward0>), Loss = 0.20027248561382294\n",
      "tensor([-0.0067,  0.0134])\n",
      "loss = 0.2000492811203003\n",
      "Iteration 296: b = tensor([ 4.3717, -9.0819], grad_fn=<SubBackward0>), Loss = 0.2000492811203003\n",
      "tensor([-0.0067,  0.0133])\n",
      "loss = 0.19982728362083435\n",
      "Iteration 297: b = tensor([ 4.3784, -9.0952], grad_fn=<SubBackward0>), Loss = 0.19982728362083435\n",
      "tensor([-0.0067,  0.0133])\n",
      "loss = 0.1996065378189087\n",
      "Iteration 298: b = tensor([ 4.3851, -9.1084], grad_fn=<SubBackward0>), Loss = 0.1996065378189087\n",
      "tensor([-0.0067,  0.0132])\n",
      "loss = 0.19938699901103973\n",
      "Iteration 299: b = tensor([ 4.3917, -9.1217], grad_fn=<SubBackward0>), Loss = 0.19938699901103973\n",
      "tensor([-0.0066,  0.0132])\n",
      "loss = 0.1991686373949051\n",
      "Iteration 300: b = tensor([ 4.3984, -9.1348], grad_fn=<SubBackward0>), Loss = 0.1991686373949051\n",
      "tensor([-0.0066,  0.0132])\n",
      "loss = 0.19895143806934357\n",
      "Iteration 301: b = tensor([ 4.4050, -9.1480], grad_fn=<SubBackward0>), Loss = 0.19895143806934357\n",
      "tensor([-0.0066,  0.0131])\n",
      "loss = 0.19873546063899994\n",
      "Iteration 302: b = tensor([ 4.4116, -9.1611], grad_fn=<SubBackward0>), Loss = 0.19873546063899994\n",
      "tensor([-0.0066,  0.0131])\n",
      "loss = 0.19852057099342346\n",
      "Iteration 303: b = tensor([ 4.4181, -9.1741], grad_fn=<SubBackward0>), Loss = 0.19852057099342346\n",
      "tensor([-0.0066,  0.0131])\n",
      "loss = 0.1983068883419037\n",
      "Iteration 304: b = tensor([ 4.4247, -9.1872], grad_fn=<SubBackward0>), Loss = 0.1983068883419037\n",
      "tensor([-0.0066,  0.0130])\n",
      "loss = 0.19809432327747345\n",
      "Iteration 305: b = tensor([ 4.4312, -9.2002], grad_fn=<SubBackward0>), Loss = 0.19809432327747345\n",
      "tensor([-0.0065,  0.0130])\n",
      "loss = 0.19788289070129395\n",
      "Iteration 306: b = tensor([ 4.4378, -9.2131], grad_fn=<SubBackward0>), Loss = 0.19788289070129395\n",
      "tensor([-0.0065,  0.0130])\n",
      "loss = 0.19767259061336517\n",
      "Iteration 307: b = tensor([ 4.4443, -9.2260], grad_fn=<SubBackward0>), Loss = 0.19767259061336517\n",
      "tensor([-0.0065,  0.0129])\n",
      "loss = 0.19746340811252594\n",
      "Iteration 308: b = tensor([ 4.4508, -9.2389], grad_fn=<SubBackward0>), Loss = 0.19746340811252594\n",
      "tensor([-0.0065,  0.0129])\n",
      "loss = 0.19725528359413147\n",
      "Iteration 309: b = tensor([ 4.4572, -9.2518], grad_fn=<SubBackward0>), Loss = 0.19725528359413147\n",
      "tensor([-0.0065,  0.0129])\n",
      "loss = 0.19704829156398773\n",
      "Iteration 310: b = tensor([ 4.4637, -9.2646], grad_fn=<SubBackward0>), Loss = 0.19704829156398773\n",
      "tensor([-0.0065,  0.0128])\n",
      "loss = 0.19684241712093353\n",
      "Iteration 311: b = tensor([ 4.4701, -9.2774], grad_fn=<SubBackward0>), Loss = 0.19684241712093353\n",
      "tensor([-0.0064,  0.0128])\n",
      "loss = 0.19663755595684052\n",
      "Iteration 312: b = tensor([ 4.4766, -9.2902], grad_fn=<SubBackward0>), Loss = 0.19663755595684052\n",
      "tensor([-0.0064,  0.0128])\n",
      "loss = 0.19643378257751465\n",
      "Iteration 313: b = tensor([ 4.4830, -9.3029], grad_fn=<SubBackward0>), Loss = 0.19643378257751465\n",
      "tensor([-0.0064,  0.0127])\n",
      "loss = 0.19623108208179474\n",
      "Iteration 314: b = tensor([ 4.4894, -9.3156], grad_fn=<SubBackward0>), Loss = 0.19623108208179474\n",
      "tensor([-0.0064,  0.0127])\n",
      "loss = 0.1960294246673584\n",
      "Iteration 315: b = tensor([ 4.4957, -9.3283], grad_fn=<SubBackward0>), Loss = 0.1960294246673584\n",
      "tensor([-0.0064,  0.0127])\n",
      "loss = 0.19582881033420563\n",
      "Iteration 316: b = tensor([ 4.5021, -9.3409], grad_fn=<SubBackward0>), Loss = 0.19582881033420563\n",
      "tensor([-0.0064,  0.0126])\n",
      "loss = 0.19562919437885284\n",
      "Iteration 317: b = tensor([ 4.5084, -9.3535], grad_fn=<SubBackward0>), Loss = 0.19562919437885284\n",
      "tensor([-0.0063,  0.0126])\n",
      "loss = 0.19543062150478363\n",
      "Iteration 318: b = tensor([ 4.5147, -9.3660], grad_fn=<SubBackward0>), Loss = 0.19543062150478363\n",
      "tensor([-0.0063,  0.0126])\n",
      "loss = 0.1952330768108368\n",
      "Iteration 319: b = tensor([ 4.5210, -9.3786], grad_fn=<SubBackward0>), Loss = 0.1952330768108368\n",
      "tensor([-0.0063,  0.0125])\n",
      "loss = 0.19503653049468994\n",
      "Iteration 320: b = tensor([ 4.5273, -9.3911], grad_fn=<SubBackward0>), Loss = 0.19503653049468994\n",
      "tensor([-0.0063,  0.0125])\n",
      "loss = 0.19484098255634308\n",
      "Iteration 321: b = tensor([ 4.5336, -9.4035], grad_fn=<SubBackward0>), Loss = 0.19484098255634308\n",
      "tensor([-0.0063,  0.0125])\n",
      "loss = 0.1946464329957962\n",
      "Iteration 322: b = tensor([ 4.5399, -9.4160], grad_fn=<SubBackward0>), Loss = 0.1946464329957962\n",
      "tensor([-0.0063,  0.0124])\n",
      "loss = 0.19445285201072693\n",
      "Iteration 323: b = tensor([ 4.5461, -9.4284], grad_fn=<SubBackward0>), Loss = 0.19445285201072693\n",
      "tensor([-0.0062,  0.0124])\n",
      "loss = 0.19426022469997406\n",
      "Iteration 324: b = tensor([ 4.5523, -9.4408], grad_fn=<SubBackward0>), Loss = 0.19426022469997406\n",
      "tensor([-0.0062,  0.0124])\n",
      "loss = 0.19406859576702118\n",
      "Iteration 325: b = tensor([ 4.5586, -9.4531], grad_fn=<SubBackward0>), Loss = 0.19406859576702118\n",
      "tensor([-0.0062,  0.0123])\n",
      "loss = 0.1938779354095459\n",
      "Iteration 326: b = tensor([ 4.5648, -9.4654], grad_fn=<SubBackward0>), Loss = 0.1938779354095459\n",
      "tensor([-0.0062,  0.0123])\n",
      "loss = 0.19368821382522583\n",
      "Iteration 327: b = tensor([ 4.5709, -9.4777], grad_fn=<SubBackward0>), Loss = 0.19368821382522583\n",
      "tensor([-0.0062,  0.0123])\n",
      "loss = 0.1934994012117386\n",
      "Iteration 328: b = tensor([ 4.5771, -9.4899], grad_fn=<SubBackward0>), Loss = 0.1934994012117386\n",
      "tensor([-0.0062,  0.0122])\n",
      "loss = 0.19331160187721252\n",
      "Iteration 329: b = tensor([ 4.5833, -9.5022], grad_fn=<SubBackward0>), Loss = 0.19331160187721252\n",
      "tensor([-0.0062,  0.0122])\n",
      "loss = 0.19312463700771332\n",
      "Iteration 330: b = tensor([ 4.5894, -9.5143], grad_fn=<SubBackward0>), Loss = 0.19312463700771332\n",
      "tensor([-0.0061,  0.0122])\n",
      "loss = 0.1929386705160141\n",
      "Iteration 331: b = tensor([ 4.5955, -9.5265], grad_fn=<SubBackward0>), Loss = 0.1929386705160141\n",
      "tensor([-0.0061,  0.0122])\n",
      "loss = 0.19275356829166412\n",
      "Iteration 332: b = tensor([ 4.6016, -9.5386], grad_fn=<SubBackward0>), Loss = 0.19275356829166412\n",
      "tensor([-0.0061,  0.0121])\n",
      "loss = 0.19256937503814697\n",
      "Iteration 333: b = tensor([ 4.6077, -9.5507], grad_fn=<SubBackward0>), Loss = 0.19256937503814697\n",
      "tensor([-0.0061,  0.0121])\n",
      "loss = 0.19238610565662384\n",
      "Iteration 334: b = tensor([ 4.6138, -9.5628], grad_fn=<SubBackward0>), Loss = 0.19238610565662384\n",
      "tensor([-0.0061,  0.0121])\n",
      "loss = 0.19220371544361115\n",
      "Iteration 335: b = tensor([ 4.6198, -9.5748], grad_fn=<SubBackward0>), Loss = 0.19220371544361115\n",
      "tensor([-0.0061,  0.0120])\n",
      "loss = 0.19202221930027008\n",
      "Iteration 336: b = tensor([ 4.6259, -9.5869], grad_fn=<SubBackward0>), Loss = 0.19202221930027008\n",
      "tensor([-0.0060,  0.0120])\n",
      "loss = 0.19184158742427826\n",
      "Iteration 337: b = tensor([ 4.6319, -9.5988], grad_fn=<SubBackward0>), Loss = 0.19184158742427826\n",
      "tensor([-0.0060,  0.0120])\n",
      "loss = 0.19166181981563568\n",
      "Iteration 338: b = tensor([ 4.6379, -9.6108], grad_fn=<SubBackward0>), Loss = 0.19166181981563568\n",
      "tensor([-0.0060,  0.0120])\n",
      "loss = 0.19148294627666473\n",
      "Iteration 339: b = tensor([ 4.6439, -9.6227], grad_fn=<SubBackward0>), Loss = 0.19148294627666473\n",
      "tensor([-0.0060,  0.0119])\n",
      "loss = 0.19130489230155945\n",
      "Iteration 340: b = tensor([ 4.6499, -9.6346], grad_fn=<SubBackward0>), Loss = 0.19130489230155945\n",
      "tensor([-0.0060,  0.0119])\n",
      "loss = 0.1911277323961258\n",
      "Iteration 341: b = tensor([ 4.6559, -9.6465], grad_fn=<SubBackward0>), Loss = 0.1911277323961258\n",
      "tensor([-0.0060,  0.0119])\n",
      "loss = 0.1909513771533966\n",
      "Iteration 342: b = tensor([ 4.6619, -9.6583], grad_fn=<SubBackward0>), Loss = 0.1909513771533966\n",
      "tensor([-0.0060,  0.0118])\n",
      "loss = 0.19077587127685547\n",
      "Iteration 343: b = tensor([ 4.6678, -9.6701], grad_fn=<SubBackward0>), Loss = 0.19077587127685547\n",
      "tensor([-0.0059,  0.0118])\n",
      "loss = 0.19060118496418\n",
      "Iteration 344: b = tensor([ 4.6737, -9.6819], grad_fn=<SubBackward0>), Loss = 0.19060118496418\n",
      "tensor([-0.0059,  0.0118])\n",
      "loss = 0.19042737782001495\n",
      "Iteration 345: b = tensor([ 4.6797, -9.6937], grad_fn=<SubBackward0>), Loss = 0.19042737782001495\n",
      "tensor([-0.0059,  0.0118])\n",
      "loss = 0.1902543604373932\n",
      "Iteration 346: b = tensor([ 4.6856, -9.7054], grad_fn=<SubBackward0>), Loss = 0.1902543604373932\n",
      "tensor([-0.0059,  0.0117])\n",
      "loss = 0.1900821477174759\n",
      "Iteration 347: b = tensor([ 4.6915, -9.7171], grad_fn=<SubBackward0>), Loss = 0.1900821477174759\n",
      "tensor([-0.0059,  0.0117])\n",
      "loss = 0.18991075456142426\n",
      "Iteration 348: b = tensor([ 4.6973, -9.7288], grad_fn=<SubBackward0>), Loss = 0.18991075456142426\n",
      "tensor([-0.0059,  0.0117])\n",
      "loss = 0.1897401213645935\n",
      "Iteration 349: b = tensor([ 4.7032, -9.7404], grad_fn=<SubBackward0>), Loss = 0.1897401213645935\n",
      "tensor([-0.0059,  0.0116])\n",
      "loss = 0.189570352435112\n",
      "Iteration 350: b = tensor([ 4.7090, -9.7520], grad_fn=<SubBackward0>), Loss = 0.189570352435112\n",
      "tensor([-0.0058,  0.0116])\n",
      "loss = 0.18940132856369019\n",
      "Iteration 351: b = tensor([ 4.7149, -9.7636], grad_fn=<SubBackward0>), Loss = 0.18940132856369019\n",
      "tensor([-0.0058,  0.0116])\n",
      "loss = 0.18923307955265045\n",
      "Iteration 352: b = tensor([ 4.7207, -9.7752], grad_fn=<SubBackward0>), Loss = 0.18923307955265045\n",
      "tensor([-0.0058,  0.0116])\n",
      "loss = 0.18906565010547638\n",
      "Iteration 353: b = tensor([ 4.7265, -9.7867], grad_fn=<SubBackward0>), Loss = 0.18906565010547638\n",
      "tensor([-0.0058,  0.0115])\n",
      "loss = 0.188898965716362\n",
      "Iteration 354: b = tensor([ 4.7323, -9.7983], grad_fn=<SubBackward0>), Loss = 0.188898965716362\n",
      "tensor([-0.0058,  0.0115])\n",
      "loss = 0.1887330561876297\n",
      "Iteration 355: b = tensor([ 4.7381, -9.8097], grad_fn=<SubBackward0>), Loss = 0.1887330561876297\n",
      "tensor([-0.0058,  0.0115])\n",
      "loss = 0.1885678768157959\n",
      "Iteration 356: b = tensor([ 4.7439, -9.8212], grad_fn=<SubBackward0>), Loss = 0.1885678768157959\n",
      "tensor([-0.0058,  0.0115])\n",
      "loss = 0.18840348720550537\n",
      "Iteration 357: b = tensor([ 4.7496, -9.8326], grad_fn=<SubBackward0>), Loss = 0.18840348720550537\n",
      "tensor([-0.0058,  0.0114])\n",
      "loss = 0.18823984265327454\n",
      "Iteration 358: b = tensor([ 4.7553, -9.8440], grad_fn=<SubBackward0>), Loss = 0.18823984265327454\n",
      "tensor([-0.0057,  0.0114])\n",
      "loss = 0.1880769431591034\n",
      "Iteration 359: b = tensor([ 4.7611, -9.8554], grad_fn=<SubBackward0>), Loss = 0.1880769431591034\n",
      "tensor([-0.0057,  0.0114])\n",
      "loss = 0.18791475892066956\n",
      "Iteration 360: b = tensor([ 4.7668, -9.8668], grad_fn=<SubBackward0>), Loss = 0.18791475892066956\n",
      "tensor([-0.0057,  0.0114])\n",
      "loss = 0.1877533197402954\n",
      "Iteration 361: b = tensor([ 4.7725, -9.8781], grad_fn=<SubBackward0>), Loss = 0.1877533197402954\n",
      "tensor([-0.0057,  0.0113])\n",
      "loss = 0.18759261071681976\n",
      "Iteration 362: b = tensor([ 4.7782, -9.8894], grad_fn=<SubBackward0>), Loss = 0.18759261071681976\n",
      "tensor([-0.0057,  0.0113])\n",
      "loss = 0.18743260204792023\n",
      "Iteration 363: b = tensor([ 4.7839, -9.9007], grad_fn=<SubBackward0>), Loss = 0.18743260204792023\n",
      "tensor([-0.0057,  0.0113])\n",
      "loss = 0.1872733235359192\n",
      "Iteration 364: b = tensor([ 4.7895, -9.9119], grad_fn=<SubBackward0>), Loss = 0.1872733235359192\n",
      "tensor([-0.0057,  0.0113])\n",
      "loss = 0.18711479008197784\n",
      "Iteration 365: b = tensor([ 4.7952, -9.9232], grad_fn=<SubBackward0>), Loss = 0.18711479008197784\n",
      "tensor([-0.0057,  0.0112])\n",
      "loss = 0.18695692718029022\n",
      "Iteration 366: b = tensor([ 4.8008, -9.9344], grad_fn=<SubBackward0>), Loss = 0.18695692718029022\n",
      "tensor([-0.0056,  0.0112])\n",
      "loss = 0.1867997646331787\n",
      "Iteration 367: b = tensor([ 4.8065, -9.9456], grad_fn=<SubBackward0>), Loss = 0.1867997646331787\n",
      "tensor([-0.0056,  0.0112])\n",
      "loss = 0.1866433024406433\n",
      "Iteration 368: b = tensor([ 4.8121, -9.9567], grad_fn=<SubBackward0>), Loss = 0.1866433024406433\n",
      "tensor([-0.0056,  0.0112])\n",
      "loss = 0.18648755550384521\n",
      "Iteration 369: b = tensor([ 4.8177, -9.9678], grad_fn=<SubBackward0>), Loss = 0.18648755550384521\n",
      "tensor([-0.0056,  0.0111])\n",
      "loss = 0.18633246421813965\n",
      "Iteration 370: b = tensor([ 4.8233, -9.9789], grad_fn=<SubBackward0>), Loss = 0.18633246421813965\n",
      "tensor([-0.0056,  0.0111])\n",
      "loss = 0.1861780434846878\n",
      "Iteration 371: b = tensor([ 4.8288, -9.9900], grad_fn=<SubBackward0>), Loss = 0.1861780434846878\n",
      "tensor([-0.0056,  0.0111])\n",
      "loss = 0.18602433800697327\n",
      "Iteration 372: b = tensor([  4.8344, -10.0011], grad_fn=<SubBackward0>), Loss = 0.18602433800697327\n",
      "tensor([-0.0056,  0.0111])\n",
      "loss = 0.18587128818035126\n",
      "Iteration 373: b = tensor([  4.8400, -10.0121], grad_fn=<SubBackward0>), Loss = 0.18587128818035126\n",
      "tensor([-0.0056,  0.0110])\n",
      "loss = 0.18571889400482178\n",
      "Iteration 374: b = tensor([  4.8455, -10.0231], grad_fn=<SubBackward0>), Loss = 0.18571889400482178\n",
      "tensor([-0.0055,  0.0110])\n",
      "loss = 0.18556718528270721\n",
      "Iteration 375: b = tensor([  4.8510, -10.0341], grad_fn=<SubBackward0>), Loss = 0.18556718528270721\n",
      "tensor([-0.0055,  0.0110])\n",
      "loss = 0.185416117310524\n",
      "Iteration 376: b = tensor([  4.8566, -10.0451], grad_fn=<SubBackward0>), Loss = 0.185416117310524\n",
      "tensor([-0.0055,  0.0110])\n",
      "loss = 0.18526571989059448\n",
      "Iteration 377: b = tensor([  4.8621, -10.0560], grad_fn=<SubBackward0>), Loss = 0.18526571989059448\n",
      "tensor([-0.0055,  0.0109])\n",
      "loss = 0.1851159781217575\n",
      "Iteration 378: b = tensor([  4.8676, -10.0669], grad_fn=<SubBackward0>), Loss = 0.1851159781217575\n",
      "tensor([-0.0055,  0.0109])\n",
      "loss = 0.18496684730052948\n",
      "Iteration 379: b = tensor([  4.8730, -10.0778], grad_fn=<SubBackward0>), Loss = 0.18496684730052948\n",
      "tensor([-0.0055,  0.0109])\n",
      "loss = 0.18481840193271637\n",
      "Iteration 380: b = tensor([  4.8785, -10.0887], grad_fn=<SubBackward0>), Loss = 0.18481840193271637\n",
      "tensor([-0.0055,  0.0109])\n",
      "loss = 0.184670552611351\n",
      "Iteration 381: b = tensor([  4.8840, -10.0995], grad_fn=<SubBackward0>), Loss = 0.184670552611351\n",
      "tensor([-0.0055,  0.0108])\n",
      "loss = 0.1845233291387558\n",
      "Iteration 382: b = tensor([  4.8894, -10.1103], grad_fn=<SubBackward0>), Loss = 0.1845233291387558\n",
      "tensor([-0.0054,  0.0108])\n",
      "loss = 0.1843767911195755\n",
      "Iteration 383: b = tensor([  4.8948, -10.1211], grad_fn=<SubBackward0>), Loss = 0.1843767911195755\n",
      "tensor([-0.0054,  0.0108])\n",
      "loss = 0.18423078954219818\n",
      "Iteration 384: b = tensor([  4.9003, -10.1319], grad_fn=<SubBackward0>), Loss = 0.18423078954219818\n",
      "tensor([-0.0054,  0.0108])\n",
      "loss = 0.18408545851707458\n",
      "Iteration 385: b = tensor([  4.9057, -10.1426], grad_fn=<SubBackward0>), Loss = 0.18408545851707458\n",
      "tensor([-0.0054,  0.0108])\n",
      "loss = 0.18394073843955994\n",
      "Iteration 386: b = tensor([  4.9111, -10.1534], grad_fn=<SubBackward0>), Loss = 0.18394073843955994\n",
      "tensor([-0.0054,  0.0107])\n",
      "loss = 0.18379662930965424\n",
      "Iteration 387: b = tensor([  4.9165, -10.1641], grad_fn=<SubBackward0>), Loss = 0.18379662930965424\n",
      "tensor([-0.0054,  0.0107])\n",
      "loss = 0.18365313112735748\n",
      "Iteration 388: b = tensor([  4.9219, -10.1748], grad_fn=<SubBackward0>), Loss = 0.18365313112735748\n",
      "tensor([-0.0054,  0.0107])\n",
      "loss = 0.18351022899150848\n",
      "Iteration 389: b = tensor([  4.9272, -10.1854], grad_fn=<SubBackward0>), Loss = 0.18351022899150848\n",
      "tensor([-0.0054,  0.0107])\n",
      "loss = 0.18336792290210724\n",
      "Iteration 390: b = tensor([  4.9326, -10.1961], grad_fn=<SubBackward0>), Loss = 0.18336792290210724\n",
      "tensor([-0.0054,  0.0106])\n",
      "loss = 0.18322619795799255\n",
      "Iteration 391: b = tensor([  4.9379, -10.2067], grad_fn=<SubBackward0>), Loss = 0.18322619795799255\n",
      "tensor([-0.0053,  0.0106])\n",
      "loss = 0.18308506906032562\n",
      "Iteration 392: b = tensor([  4.9433, -10.2173], grad_fn=<SubBackward0>), Loss = 0.18308506906032562\n",
      "tensor([-0.0053,  0.0106])\n",
      "loss = 0.18294453620910645\n",
      "Iteration 393: b = tensor([  4.9486, -10.2278], grad_fn=<SubBackward0>), Loss = 0.18294453620910645\n",
      "tensor([-0.0053,  0.0106])\n",
      "loss = 0.18280456960201263\n",
      "Iteration 394: b = tensor([  4.9539, -10.2384], grad_fn=<SubBackward0>), Loss = 0.18280456960201263\n",
      "tensor([-0.0053,  0.0105])\n",
      "loss = 0.18266521394252777\n",
      "Iteration 395: b = tensor([  4.9592, -10.2489], grad_fn=<SubBackward0>), Loss = 0.18266521394252777\n",
      "tensor([-0.0053,  0.0105])\n",
      "loss = 0.18252640962600708\n",
      "Iteration 396: b = tensor([  4.9645, -10.2594], grad_fn=<SubBackward0>), Loss = 0.18252640962600708\n",
      "tensor([-0.0053,  0.0105])\n",
      "loss = 0.18238814175128937\n",
      "Iteration 397: b = tensor([  4.9698, -10.2699], grad_fn=<SubBackward0>), Loss = 0.18238814175128937\n",
      "tensor([-0.0053,  0.0105])\n",
      "loss = 0.182250514626503\n",
      "Iteration 398: b = tensor([  4.9750, -10.2804], grad_fn=<SubBackward0>), Loss = 0.182250514626503\n",
      "tensor([-0.0053,  0.0105])\n",
      "loss = 0.182113379240036\n",
      "Iteration 399: b = tensor([  4.9803, -10.2908], grad_fn=<SubBackward0>), Loss = 0.182113379240036\n",
      "tensor([-0.0053,  0.0104])\n",
      "loss = 0.18197686970233917\n",
      "Iteration 400: b = tensor([  4.9855, -10.3012], grad_fn=<SubBackward0>), Loss = 0.18197686970233917\n",
      "tensor([-0.0052,  0.0104])\n",
      "loss = 0.18184086680412292\n",
      "Iteration 401: b = tensor([  4.9908, -10.3116], grad_fn=<SubBackward0>), Loss = 0.18184086680412292\n",
      "tensor([-0.0052,  0.0104])\n",
      "loss = 0.18170543015003204\n",
      "Iteration 402: b = tensor([  4.9960, -10.3220], grad_fn=<SubBackward0>), Loss = 0.18170543015003204\n",
      "tensor([-0.0052,  0.0104])\n",
      "loss = 0.18157051503658295\n",
      "Iteration 403: b = tensor([  5.0012, -10.3324], grad_fn=<SubBackward0>), Loss = 0.18157051503658295\n",
      "tensor([-0.0052,  0.0104])\n",
      "loss = 0.18143615126609802\n",
      "Iteration 404: b = tensor([  5.0064, -10.3427], grad_fn=<SubBackward0>), Loss = 0.18143615126609802\n",
      "tensor([-0.0052,  0.0103])\n",
      "loss = 0.18130236864089966\n",
      "Iteration 405: b = tensor([  5.0116, -10.3530], grad_fn=<SubBackward0>), Loss = 0.18130236864089966\n",
      "tensor([-0.0052,  0.0103])\n",
      "loss = 0.18116910755634308\n",
      "Iteration 406: b = tensor([  5.0168, -10.3633], grad_fn=<SubBackward0>), Loss = 0.18116910755634308\n",
      "tensor([-0.0052,  0.0103])\n",
      "loss = 0.18103638291358948\n",
      "Iteration 407: b = tensor([  5.0220, -10.3736], grad_fn=<SubBackward0>), Loss = 0.18103638291358948\n",
      "tensor([-0.0052,  0.0103])\n",
      "loss = 0.18090416491031647\n",
      "Iteration 408: b = tensor([  5.0271, -10.3838], grad_fn=<SubBackward0>), Loss = 0.18090416491031647\n",
      "tensor([-0.0052,  0.0103])\n",
      "loss = 0.18077251315116882\n",
      "Iteration 409: b = tensor([  5.0323, -10.3941], grad_fn=<SubBackward0>), Loss = 0.18077251315116882\n",
      "tensor([-0.0052,  0.0102])\n",
      "loss = 0.18064135313034058\n",
      "Iteration 410: b = tensor([  5.0374, -10.4043], grad_fn=<SubBackward0>), Loss = 0.18064135313034058\n",
      "tensor([-0.0051,  0.0102])\n",
      "loss = 0.1805107444524765\n",
      "Iteration 411: b = tensor([  5.0426, -10.4145], grad_fn=<SubBackward0>), Loss = 0.1805107444524765\n",
      "tensor([-0.0051,  0.0102])\n",
      "loss = 0.18038061261177063\n",
      "Iteration 412: b = tensor([  5.0477, -10.4247], grad_fn=<SubBackward0>), Loss = 0.18038061261177063\n",
      "tensor([-0.0051,  0.0102])\n",
      "loss = 0.18025101721286774\n",
      "Iteration 413: b = tensor([  5.0528, -10.4348], grad_fn=<SubBackward0>), Loss = 0.18025101721286774\n",
      "tensor([-0.0051,  0.0102])\n",
      "loss = 0.18012191355228424\n",
      "Iteration 414: b = tensor([  5.0579, -10.4449], grad_fn=<SubBackward0>), Loss = 0.18012191355228424\n",
      "tensor([-0.0051,  0.0101])\n",
      "loss = 0.17999333143234253\n",
      "Iteration 415: b = tensor([  5.0630, -10.4551], grad_fn=<SubBackward0>), Loss = 0.17999333143234253\n",
      "tensor([-0.0051,  0.0101])\n",
      "loss = 0.1798652559518814\n",
      "Iteration 416: b = tensor([  5.0681, -10.4651], grad_fn=<SubBackward0>), Loss = 0.1798652559518814\n",
      "tensor([-0.0051,  0.0101])\n",
      "loss = 0.17973767220973969\n",
      "Iteration 417: b = tensor([  5.0732, -10.4752], grad_fn=<SubBackward0>), Loss = 0.17973767220973969\n",
      "tensor([-0.0051,  0.0101])\n",
      "loss = 0.17961062490940094\n",
      "Iteration 418: b = tensor([  5.0782, -10.4853], grad_fn=<SubBackward0>), Loss = 0.17961062490940094\n",
      "tensor([-0.0051,  0.0101])\n",
      "loss = 0.179484024643898\n",
      "Iteration 419: b = tensor([  5.0833, -10.4953], grad_fn=<SubBackward0>), Loss = 0.179484024643898\n",
      "tensor([-0.0051,  0.0100])\n",
      "loss = 0.17935793101787567\n",
      "Iteration 420: b = tensor([  5.0883, -10.5053], grad_fn=<SubBackward0>), Loss = 0.17935793101787567\n",
      "tensor([-0.0050,  0.0100])\n",
      "loss = 0.17923234403133392\n",
      "Iteration 421: b = tensor([  5.0934, -10.5153], grad_fn=<SubBackward0>), Loss = 0.17923234403133392\n",
      "tensor([-0.0050,  0.0100])\n",
      "loss = 0.17910723388195038\n",
      "Iteration 422: b = tensor([  5.0984, -10.5253], grad_fn=<SubBackward0>), Loss = 0.17910723388195038\n",
      "tensor([-0.0050,  0.0100])\n",
      "loss = 0.17898260056972504\n",
      "Iteration 423: b = tensor([  5.1034, -10.5353], grad_fn=<SubBackward0>), Loss = 0.17898260056972504\n",
      "tensor([-0.0050,  0.0100])\n",
      "loss = 0.1788584440946579\n",
      "Iteration 424: b = tensor([  5.1084, -10.5452], grad_fn=<SubBackward0>), Loss = 0.1788584440946579\n",
      "tensor([-0.0050,  0.0099])\n",
      "loss = 0.17873474955558777\n",
      "Iteration 425: b = tensor([  5.1134, -10.5551], grad_fn=<SubBackward0>), Loss = 0.17873474955558777\n",
      "tensor([-0.0050,  0.0099])\n",
      "loss = 0.17861154675483704\n",
      "Iteration 426: b = tensor([  5.1184, -10.5650], grad_fn=<SubBackward0>), Loss = 0.17861154675483704\n",
      "tensor([-0.0050,  0.0099])\n",
      "loss = 0.1784888058900833\n",
      "Iteration 427: b = tensor([  5.1234, -10.5749], grad_fn=<SubBackward0>), Loss = 0.1784888058900833\n",
      "tensor([-0.0050,  0.0099])\n",
      "loss = 0.178366556763649\n",
      "Iteration 428: b = tensor([  5.1283, -10.5847], grad_fn=<SubBackward0>), Loss = 0.178366556763649\n",
      "tensor([-0.0050,  0.0099])\n",
      "loss = 0.17824475467205048\n",
      "Iteration 429: b = tensor([  5.1333, -10.5946], grad_fn=<SubBackward0>), Loss = 0.17824475467205048\n",
      "tensor([-0.0050,  0.0098])\n",
      "loss = 0.17812342941761017\n",
      "Iteration 430: b = tensor([  5.1382, -10.6044], grad_fn=<SubBackward0>), Loss = 0.17812342941761017\n",
      "tensor([-0.0049,  0.0098])\n",
      "loss = 0.17800256609916687\n",
      "Iteration 431: b = tensor([  5.1432, -10.6142], grad_fn=<SubBackward0>), Loss = 0.17800256609916687\n",
      "tensor([-0.0049,  0.0098])\n",
      "loss = 0.1778821349143982\n",
      "Iteration 432: b = tensor([  5.1481, -10.6240], grad_fn=<SubBackward0>), Loss = 0.1778821349143982\n",
      "tensor([-0.0049,  0.0098])\n",
      "loss = 0.17776213586330414\n",
      "Iteration 433: b = tensor([  5.1530, -10.6338], grad_fn=<SubBackward0>), Loss = 0.17776213586330414\n",
      "tensor([-0.0049,  0.0098])\n",
      "loss = 0.17764265835285187\n",
      "Iteration 434: b = tensor([  5.1579, -10.6435], grad_fn=<SubBackward0>), Loss = 0.17764265835285187\n",
      "tensor([-0.0049,  0.0097])\n",
      "loss = 0.17752358317375183\n",
      "Iteration 435: b = tensor([  5.1628, -10.6533], grad_fn=<SubBackward0>), Loss = 0.17752358317375183\n",
      "tensor([-0.0049,  0.0097])\n",
      "loss = 0.1774049550294876\n",
      "Iteration 436: b = tensor([  5.1677, -10.6630], grad_fn=<SubBackward0>), Loss = 0.1774049550294876\n",
      "tensor([-0.0049,  0.0097])\n",
      "loss = 0.1772867739200592\n",
      "Iteration 437: b = tensor([  5.1726, -10.6727], grad_fn=<SubBackward0>), Loss = 0.1772867739200592\n",
      "tensor([-0.0049,  0.0097])\n",
      "loss = 0.1771690398454666\n",
      "Iteration 438: b = tensor([  5.1775, -10.6823], grad_fn=<SubBackward0>), Loss = 0.1771690398454666\n",
      "tensor([-0.0049,  0.0097])\n",
      "loss = 0.17705176770687103\n",
      "Iteration 439: b = tensor([  5.1823, -10.6920], grad_fn=<SubBackward0>), Loss = 0.17705176770687103\n",
      "tensor([-0.0049,  0.0097])\n",
      "loss = 0.1769348680973053\n",
      "Iteration 440: b = tensor([  5.1872, -10.7016], grad_fn=<SubBackward0>), Loss = 0.1769348680973053\n",
      "tensor([-0.0049,  0.0096])\n",
      "loss = 0.17681843042373657\n",
      "Iteration 441: b = tensor([  5.1920, -10.7113], grad_fn=<SubBackward0>), Loss = 0.17681843042373657\n",
      "tensor([-0.0048,  0.0096])\n",
      "loss = 0.17670242488384247\n",
      "Iteration 442: b = tensor([  5.1969, -10.7209], grad_fn=<SubBackward0>), Loss = 0.17670242488384247\n",
      "tensor([-0.0048,  0.0096])\n",
      "loss = 0.1765868365764618\n",
      "Iteration 443: b = tensor([  5.2017, -10.7305], grad_fn=<SubBackward0>), Loss = 0.1765868365764618\n",
      "tensor([-0.0048,  0.0096])\n",
      "loss = 0.17647169530391693\n",
      "Iteration 444: b = tensor([  5.2065, -10.7400], grad_fn=<SubBackward0>), Loss = 0.17647169530391693\n",
      "tensor([-0.0048,  0.0096])\n",
      "loss = 0.1763569712638855\n",
      "Iteration 445: b = tensor([  5.2114, -10.7496], grad_fn=<SubBackward0>), Loss = 0.1763569712638855\n",
      "tensor([-0.0048,  0.0096])\n",
      "loss = 0.1762426495552063\n",
      "Iteration 446: b = tensor([  5.2162, -10.7591], grad_fn=<SubBackward0>), Loss = 0.1762426495552063\n",
      "tensor([-0.0048,  0.0095])\n",
      "loss = 0.1761287897825241\n",
      "Iteration 447: b = tensor([  5.2209, -10.7686], grad_fn=<SubBackward0>), Loss = 0.1761287897825241\n",
      "tensor([-0.0048,  0.0095])\n",
      "loss = 0.17601530253887177\n",
      "Iteration 448: b = tensor([  5.2257, -10.7781], grad_fn=<SubBackward0>), Loss = 0.17601530253887177\n",
      "tensor([-0.0048,  0.0095])\n",
      "loss = 0.17590221762657166\n",
      "Iteration 449: b = tensor([  5.2305, -10.7876], grad_fn=<SubBackward0>), Loss = 0.17590221762657166\n",
      "tensor([-0.0048,  0.0095])\n",
      "loss = 0.17578957974910736\n",
      "Iteration 450: b = tensor([  5.2353, -10.7971], grad_fn=<SubBackward0>), Loss = 0.17578957974910736\n",
      "tensor([-0.0048,  0.0095])\n",
      "loss = 0.1756773144006729\n",
      "Iteration 451: b = tensor([  5.2400, -10.8065], grad_fn=<SubBackward0>), Loss = 0.1756773144006729\n",
      "tensor([-0.0048,  0.0094])\n",
      "loss = 0.17556548118591309\n",
      "Iteration 452: b = tensor([  5.2448, -10.8160], grad_fn=<SubBackward0>), Loss = 0.17556548118591309\n",
      "tensor([-0.0048,  0.0094])\n",
      "loss = 0.1754540503025055\n",
      "Iteration 453: b = tensor([  5.2495, -10.8254], grad_fn=<SubBackward0>), Loss = 0.1754540503025055\n",
      "tensor([-0.0047,  0.0094])\n",
      "loss = 0.17534299194812775\n",
      "Iteration 454: b = tensor([  5.2543, -10.8348], grad_fn=<SubBackward0>), Loss = 0.17534299194812775\n",
      "tensor([-0.0047,  0.0094])\n",
      "loss = 0.17523235082626343\n",
      "Iteration 455: b = tensor([  5.2590, -10.8442], grad_fn=<SubBackward0>), Loss = 0.17523235082626343\n",
      "tensor([-0.0047,  0.0094])\n",
      "loss = 0.17512212693691254\n",
      "Iteration 456: b = tensor([  5.2637, -10.8535], grad_fn=<SubBackward0>), Loss = 0.17512212693691254\n",
      "tensor([-0.0047,  0.0094])\n",
      "loss = 0.1750122606754303\n",
      "Iteration 457: b = tensor([  5.2684, -10.8629], grad_fn=<SubBackward0>), Loss = 0.1750122606754303\n",
      "tensor([-0.0047,  0.0093])\n",
      "loss = 0.17490282654762268\n",
      "Iteration 458: b = tensor([  5.2731, -10.8722], grad_fn=<SubBackward0>), Loss = 0.17490282654762268\n",
      "tensor([-0.0047,  0.0093])\n",
      "loss = 0.17479372024536133\n",
      "Iteration 459: b = tensor([  5.2778, -10.8815], grad_fn=<SubBackward0>), Loss = 0.17479372024536133\n",
      "tensor([-0.0047,  0.0093])\n",
      "loss = 0.17468507587909698\n",
      "Iteration 460: b = tensor([  5.2825, -10.8908], grad_fn=<SubBackward0>), Loss = 0.17468507587909698\n",
      "tensor([-0.0047,  0.0093])\n",
      "loss = 0.1745767593383789\n",
      "Iteration 461: b = tensor([  5.2872, -10.9001], grad_fn=<SubBackward0>), Loss = 0.1745767593383789\n",
      "tensor([-0.0047,  0.0093])\n",
      "loss = 0.17446883022785187\n",
      "Iteration 462: b = tensor([  5.2918, -10.9094], grad_fn=<SubBackward0>), Loss = 0.17446883022785187\n",
      "tensor([-0.0047,  0.0093])\n",
      "loss = 0.17436130344867706\n",
      "Iteration 463: b = tensor([  5.2965, -10.9186], grad_fn=<SubBackward0>), Loss = 0.17436130344867706\n",
      "tensor([-0.0047,  0.0092])\n",
      "loss = 0.1742541342973709\n",
      "Iteration 464: b = tensor([  5.3012, -10.9278], grad_fn=<SubBackward0>), Loss = 0.1742541342973709\n",
      "tensor([-0.0047,  0.0092])\n",
      "loss = 0.174147367477417\n",
      "Iteration 465: b = tensor([  5.3058, -10.9371], grad_fn=<SubBackward0>), Loss = 0.174147367477417\n",
      "tensor([-0.0046,  0.0092])\n",
      "loss = 0.17404095828533173\n",
      "Iteration 466: b = tensor([  5.3104, -10.9463], grad_fn=<SubBackward0>), Loss = 0.17404095828533173\n",
      "tensor([-0.0046,  0.0092])\n",
      "loss = 0.1739349067211151\n",
      "Iteration 467: b = tensor([  5.3151, -10.9554], grad_fn=<SubBackward0>), Loss = 0.1739349067211151\n",
      "tensor([-0.0046,  0.0092])\n",
      "loss = 0.17382925748825073\n",
      "Iteration 468: b = tensor([  5.3197, -10.9646], grad_fn=<SubBackward0>), Loss = 0.17382925748825073\n",
      "tensor([-0.0046,  0.0092])\n",
      "loss = 0.1737239509820938\n",
      "Iteration 469: b = tensor([  5.3243, -10.9738], grad_fn=<SubBackward0>), Loss = 0.1737239509820938\n",
      "tensor([-0.0046,  0.0092])\n",
      "loss = 0.17361903190612793\n",
      "Iteration 470: b = tensor([  5.3289, -10.9829], grad_fn=<SubBackward0>), Loss = 0.17361903190612793\n",
      "tensor([-0.0046,  0.0091])\n",
      "loss = 0.1735144406557083\n",
      "Iteration 471: b = tensor([  5.3335, -10.9920], grad_fn=<SubBackward0>), Loss = 0.1735144406557083\n",
      "tensor([-0.0046,  0.0091])\n",
      "loss = 0.17341025173664093\n",
      "Iteration 472: b = tensor([  5.3381, -11.0011], grad_fn=<SubBackward0>), Loss = 0.17341025173664093\n",
      "tensor([-0.0046,  0.0091])\n",
      "loss = 0.173306405544281\n",
      "Iteration 473: b = tensor([  5.3427, -11.0102], grad_fn=<SubBackward0>), Loss = 0.173306405544281\n",
      "tensor([-0.0046,  0.0091])\n",
      "loss = 0.17320291697978973\n",
      "Iteration 474: b = tensor([  5.3472, -11.0193], grad_fn=<SubBackward0>), Loss = 0.17320291697978973\n",
      "tensor([-0.0046,  0.0091])\n",
      "loss = 0.1730998009443283\n",
      "Iteration 475: b = tensor([  5.3518, -11.0283], grad_fn=<SubBackward0>), Loss = 0.1730998009443283\n",
      "tensor([-0.0046,  0.0091])\n",
      "loss = 0.17299699783325195\n",
      "Iteration 476: b = tensor([  5.3563, -11.0374], grad_fn=<SubBackward0>), Loss = 0.17299699783325195\n",
      "tensor([-0.0046,  0.0090])\n",
      "loss = 0.17289456725120544\n",
      "Iteration 477: b = tensor([  5.3609, -11.0464], grad_fn=<SubBackward0>), Loss = 0.17289456725120544\n",
      "tensor([-0.0045,  0.0090])\n",
      "loss = 0.17279250919818878\n",
      "Iteration 478: b = tensor([  5.3654, -11.0554], grad_fn=<SubBackward0>), Loss = 0.17279250919818878\n",
      "tensor([-0.0045,  0.0090])\n",
      "loss = 0.17269077897071838\n",
      "Iteration 479: b = tensor([  5.3700, -11.0644], grad_fn=<SubBackward0>), Loss = 0.17269077897071838\n",
      "tensor([-0.0045,  0.0090])\n",
      "loss = 0.17258937656879425\n",
      "Iteration 480: b = tensor([  5.3745, -11.0734], grad_fn=<SubBackward0>), Loss = 0.17258937656879425\n",
      "tensor([-0.0045,  0.0090])\n",
      "loss = 0.17248831689357758\n",
      "Iteration 481: b = tensor([  5.3790, -11.0824], grad_fn=<SubBackward0>), Loss = 0.17248831689357758\n",
      "tensor([-0.0045,  0.0090])\n",
      "loss = 0.17238762974739075\n",
      "Iteration 482: b = tensor([  5.3835, -11.0913], grad_fn=<SubBackward0>), Loss = 0.17238762974739075\n",
      "tensor([-0.0045,  0.0090])\n",
      "loss = 0.17228727042675018\n",
      "Iteration 483: b = tensor([  5.3880, -11.1003], grad_fn=<SubBackward0>), Loss = 0.17228727042675018\n",
      "tensor([-0.0045,  0.0089])\n",
      "loss = 0.17218723893165588\n",
      "Iteration 484: b = tensor([  5.3925, -11.1092], grad_fn=<SubBackward0>), Loss = 0.17218723893165588\n",
      "tensor([-0.0045,  0.0089])\n",
      "loss = 0.17208755016326904\n",
      "Iteration 485: b = tensor([  5.3970, -11.1181], grad_fn=<SubBackward0>), Loss = 0.17208755016326904\n",
      "tensor([-0.0045,  0.0089])\n",
      "loss = 0.17198820412158966\n",
      "Iteration 486: b = tensor([  5.4015, -11.1270], grad_fn=<SubBackward0>), Loss = 0.17198820412158966\n",
      "tensor([-0.0045,  0.0089])\n",
      "loss = 0.17188920080661774\n",
      "Iteration 487: b = tensor([  5.4060, -11.1358], grad_fn=<SubBackward0>), Loss = 0.17188920080661774\n",
      "tensor([-0.0045,  0.0089])\n",
      "loss = 0.17179051041603088\n",
      "Iteration 488: b = tensor([  5.4104, -11.1447], grad_fn=<SubBackward0>), Loss = 0.17179051041603088\n",
      "tensor([-0.0045,  0.0089])\n",
      "loss = 0.1716921329498291\n",
      "Iteration 489: b = tensor([  5.4149, -11.1535], grad_fn=<SubBackward0>), Loss = 0.1716921329498291\n",
      "tensor([-0.0045,  0.0088])\n",
      "loss = 0.17159412801265717\n",
      "Iteration 490: b = tensor([  5.4193, -11.1624], grad_fn=<SubBackward0>), Loss = 0.17159412801265717\n",
      "tensor([-0.0045,  0.0088])\n",
      "loss = 0.17149639129638672\n",
      "Iteration 491: b = tensor([  5.4238, -11.1712], grad_fn=<SubBackward0>), Loss = 0.17149639129638672\n",
      "tensor([-0.0044,  0.0088])\n",
      "loss = 0.1713990420103073\n",
      "Iteration 492: b = tensor([  5.4282, -11.1800], grad_fn=<SubBackward0>), Loss = 0.1713990420103073\n",
      "tensor([-0.0044,  0.0088])\n",
      "loss = 0.17130199074745178\n",
      "Iteration 493: b = tensor([  5.4326, -11.1888], grad_fn=<SubBackward0>), Loss = 0.17130199074745178\n",
      "tensor([-0.0044,  0.0088])\n",
      "loss = 0.17120525240898132\n",
      "Iteration 494: b = tensor([  5.4371, -11.1976], grad_fn=<SubBackward0>), Loss = 0.17120525240898132\n",
      "tensor([-0.0044,  0.0088])\n",
      "loss = 0.17110882699489594\n",
      "Iteration 495: b = tensor([  5.4415, -11.2063], grad_fn=<SubBackward0>), Loss = 0.17110882699489594\n",
      "tensor([-0.0044,  0.0088])\n",
      "loss = 0.17101271450519562\n",
      "Iteration 496: b = tensor([  5.4459, -11.2151], grad_fn=<SubBackward0>), Loss = 0.17101271450519562\n",
      "tensor([-0.0044,  0.0087])\n",
      "loss = 0.17091694474220276\n",
      "Iteration 497: b = tensor([  5.4503, -11.2238], grad_fn=<SubBackward0>), Loss = 0.17091694474220276\n",
      "tensor([-0.0044,  0.0087])\n",
      "loss = 0.17082147300243378\n",
      "Iteration 498: b = tensor([  5.4547, -11.2325], grad_fn=<SubBackward0>), Loss = 0.17082147300243378\n",
      "tensor([-0.0044,  0.0087])\n",
      "loss = 0.17072631418704987\n",
      "Iteration 499: b = tensor([  5.4591, -11.2412], grad_fn=<SubBackward0>), Loss = 0.17072631418704987\n",
      "tensor([-0.0044,  0.0087])\n",
      "loss = 0.17063148319721222\n",
      "Iteration 500: b = tensor([  5.4634, -11.2499], grad_fn=<SubBackward0>), Loss = 0.17063148319721222\n",
      "tensor([-0.0044,  0.0087])\n",
      "loss = 0.17053693532943726\n",
      "Iteration 501: b = tensor([  5.4678, -11.2586], grad_fn=<SubBackward0>), Loss = 0.17053693532943726\n",
      "tensor([-0.0044,  0.0087])\n",
      "loss = 0.17044271528720856\n",
      "Iteration 502: b = tensor([  5.4722, -11.2672], grad_fn=<SubBackward0>), Loss = 0.17044271528720856\n",
      "tensor([-0.0044,  0.0087])\n",
      "loss = 0.17034877836704254\n",
      "Iteration 503: b = tensor([  5.4765, -11.2759], grad_fn=<SubBackward0>), Loss = 0.17034877836704254\n",
      "tensor([-0.0044,  0.0086])\n",
      "loss = 0.1702551692724228\n",
      "Iteration 504: b = tensor([  5.4809, -11.2845], grad_fn=<SubBackward0>), Loss = 0.1702551692724228\n",
      "tensor([-0.0043,  0.0086])\n",
      "loss = 0.17016184329986572\n",
      "Iteration 505: b = tensor([  5.4852, -11.2931], grad_fn=<SubBackward0>), Loss = 0.17016184329986572\n",
      "tensor([-0.0043,  0.0086])\n",
      "loss = 0.17006880044937134\n",
      "Iteration 506: b = tensor([  5.4895, -11.3017], grad_fn=<SubBackward0>), Loss = 0.17006880044937134\n",
      "tensor([-0.0043,  0.0086])\n",
      "loss = 0.1699761003255844\n",
      "Iteration 507: b = tensor([  5.4939, -11.3103], grad_fn=<SubBackward0>), Loss = 0.1699761003255844\n",
      "tensor([-0.0043,  0.0086])\n",
      "loss = 0.16988368332386017\n",
      "Iteration 508: b = tensor([  5.4982, -11.3189], grad_fn=<SubBackward0>), Loss = 0.16988368332386017\n",
      "tensor([-0.0043,  0.0086])\n",
      "loss = 0.1697915494441986\n",
      "Iteration 509: b = tensor([  5.5025, -11.3274], grad_fn=<SubBackward0>), Loss = 0.1697915494441986\n",
      "tensor([-0.0043,  0.0086])\n",
      "loss = 0.1696997433900833\n",
      "Iteration 510: b = tensor([  5.5068, -11.3360], grad_fn=<SubBackward0>), Loss = 0.1696997433900833\n",
      "tensor([-0.0043,  0.0085])\n",
      "loss = 0.1696081906557083\n",
      "Iteration 511: b = tensor([  5.5111, -11.3445], grad_fn=<SubBackward0>), Loss = 0.1696081906557083\n",
      "tensor([-0.0043,  0.0085])\n",
      "loss = 0.16951695084571838\n",
      "Iteration 512: b = tensor([  5.5154, -11.3530], grad_fn=<SubBackward0>), Loss = 0.16951695084571838\n",
      "tensor([-0.0043,  0.0085])\n",
      "loss = 0.16942599415779114\n",
      "Iteration 513: b = tensor([  5.5197, -11.3615], grad_fn=<SubBackward0>), Loss = 0.16942599415779114\n",
      "tensor([-0.0043,  0.0085])\n",
      "loss = 0.16933532059192657\n",
      "Iteration 514: b = tensor([  5.5240, -11.3700], grad_fn=<SubBackward0>), Loss = 0.16933532059192657\n",
      "tensor([-0.0043,  0.0085])\n",
      "loss = 0.16924497485160828\n",
      "Iteration 515: b = tensor([  5.5283, -11.3785], grad_fn=<SubBackward0>), Loss = 0.16924497485160828\n",
      "tensor([-0.0043,  0.0085])\n",
      "loss = 0.16915486752986908\n",
      "Iteration 516: b = tensor([  5.5325, -11.3870], grad_fn=<SubBackward0>), Loss = 0.16915486752986908\n",
      "tensor([-0.0043,  0.0085])\n",
      "loss = 0.16906505823135376\n",
      "Iteration 517: b = tensor([  5.5368, -11.3954], grad_fn=<SubBackward0>), Loss = 0.16906505823135376\n",
      "tensor([-0.0043,  0.0085])\n",
      "loss = 0.16897554695606232\n",
      "Iteration 518: b = tensor([  5.5410, -11.4039], grad_fn=<SubBackward0>), Loss = 0.16897554695606232\n",
      "tensor([-0.0043,  0.0084])\n",
      "loss = 0.16888627409934998\n",
      "Iteration 519: b = tensor([  5.5453, -11.4123], grad_fn=<SubBackward0>), Loss = 0.16888627409934998\n",
      "tensor([-0.0042,  0.0084])\n",
      "loss = 0.1687973439693451\n",
      "Iteration 520: b = tensor([  5.5495, -11.4207], grad_fn=<SubBackward0>), Loss = 0.1687973439693451\n",
      "tensor([-0.0042,  0.0084])\n",
      "loss = 0.1687086522579193\n",
      "Iteration 521: b = tensor([  5.5538, -11.4291], grad_fn=<SubBackward0>), Loss = 0.1687086522579193\n",
      "tensor([-0.0042,  0.0084])\n",
      "loss = 0.16862022876739502\n",
      "Iteration 522: b = tensor([  5.5580, -11.4375], grad_fn=<SubBackward0>), Loss = 0.16862022876739502\n",
      "tensor([-0.0042,  0.0084])\n",
      "loss = 0.168532133102417\n",
      "Iteration 523: b = tensor([  5.5622, -11.4459], grad_fn=<SubBackward0>), Loss = 0.168532133102417\n",
      "tensor([-0.0042,  0.0084])\n",
      "loss = 0.16844426095485687\n",
      "Iteration 524: b = tensor([  5.5664, -11.4542], grad_fn=<SubBackward0>), Loss = 0.16844426095485687\n",
      "tensor([-0.0042,  0.0084])\n",
      "loss = 0.16835667192935944\n",
      "Iteration 525: b = tensor([  5.5706, -11.4626], grad_fn=<SubBackward0>), Loss = 0.16835667192935944\n",
      "tensor([-0.0042,  0.0083])\n",
      "loss = 0.16826938092708588\n",
      "Iteration 526: b = tensor([  5.5748, -11.4709], grad_fn=<SubBackward0>), Loss = 0.16826938092708588\n",
      "tensor([-0.0042,  0.0083])\n",
      "loss = 0.16818232834339142\n",
      "Iteration 527: b = tensor([  5.5790, -11.4792], grad_fn=<SubBackward0>), Loss = 0.16818232834339142\n",
      "tensor([-0.0042,  0.0083])\n",
      "loss = 0.16809555888175964\n",
      "Iteration 528: b = tensor([  5.5832, -11.4875], grad_fn=<SubBackward0>), Loss = 0.16809555888175964\n",
      "tensor([-0.0042,  0.0083])\n",
      "loss = 0.16800904273986816\n",
      "Iteration 529: b = tensor([  5.5874, -11.4958], grad_fn=<SubBackward0>), Loss = 0.16800904273986816\n",
      "tensor([-0.0042,  0.0083])\n",
      "loss = 0.16792283952236176\n",
      "Iteration 530: b = tensor([  5.5916, -11.5041], grad_fn=<SubBackward0>), Loss = 0.16792283952236176\n",
      "tensor([-0.0042,  0.0083])\n",
      "loss = 0.16783685982227325\n",
      "Iteration 531: b = tensor([  5.5957, -11.5124], grad_fn=<SubBackward0>), Loss = 0.16783685982227325\n",
      "tensor([-0.0042,  0.0083])\n",
      "loss = 0.16775114834308624\n",
      "Iteration 532: b = tensor([  5.5999, -11.5206], grad_fn=<SubBackward0>), Loss = 0.16775114834308624\n",
      "tensor([-0.0042,  0.0083])\n",
      "loss = 0.16766571998596191\n",
      "Iteration 533: b = tensor([  5.6041, -11.5289], grad_fn=<SubBackward0>), Loss = 0.16766571998596191\n",
      "tensor([-0.0042,  0.0082])\n",
      "loss = 0.1675805300474167\n",
      "Iteration 534: b = tensor([  5.6082, -11.5371], grad_fn=<SubBackward0>), Loss = 0.1675805300474167\n",
      "tensor([-0.0041,  0.0082])\n",
      "loss = 0.16749559342861176\n",
      "Iteration 535: b = tensor([  5.6124, -11.5453], grad_fn=<SubBackward0>), Loss = 0.16749559342861176\n",
      "tensor([-0.0041,  0.0082])\n",
      "loss = 0.1674109250307083\n",
      "Iteration 536: b = tensor([  5.6165, -11.5535], grad_fn=<SubBackward0>), Loss = 0.1674109250307083\n",
      "tensor([-0.0041,  0.0082])\n",
      "loss = 0.16732652485370636\n",
      "Iteration 537: b = tensor([  5.6206, -11.5617], grad_fn=<SubBackward0>), Loss = 0.16732652485370636\n",
      "tensor([-0.0041,  0.0082])\n",
      "loss = 0.16724234819412231\n",
      "Iteration 538: b = tensor([  5.6247, -11.5699], grad_fn=<SubBackward0>), Loss = 0.16724234819412231\n",
      "tensor([-0.0041,  0.0082])\n",
      "loss = 0.16715848445892334\n",
      "Iteration 539: b = tensor([  5.6289, -11.5781], grad_fn=<SubBackward0>), Loss = 0.16715848445892334\n",
      "tensor([-0.0041,  0.0082])\n",
      "loss = 0.16707482933998108\n",
      "Iteration 540: b = tensor([  5.6330, -11.5862], grad_fn=<SubBackward0>), Loss = 0.16707482933998108\n",
      "tensor([-0.0041,  0.0082])\n",
      "loss = 0.1669914722442627\n",
      "Iteration 541: b = tensor([  5.6371, -11.5944], grad_fn=<SubBackward0>), Loss = 0.1669914722442627\n",
      "tensor([-0.0041,  0.0081])\n",
      "loss = 0.16690830886363983\n",
      "Iteration 542: b = tensor([  5.6412, -11.6025], grad_fn=<SubBackward0>), Loss = 0.16690830886363983\n",
      "tensor([-0.0041,  0.0081])\n",
      "loss = 0.16682539880275726\n",
      "Iteration 543: b = tensor([  5.6453, -11.6106], grad_fn=<SubBackward0>), Loss = 0.16682539880275726\n",
      "tensor([-0.0041,  0.0081])\n",
      "loss = 0.16674277186393738\n",
      "Iteration 544: b = tensor([  5.6494, -11.6188], grad_fn=<SubBackward0>), Loss = 0.16674277186393738\n",
      "tensor([-0.0041,  0.0081])\n",
      "loss = 0.1666603833436966\n",
      "Iteration 545: b = tensor([  5.6534, -11.6268], grad_fn=<SubBackward0>), Loss = 0.1666603833436966\n",
      "tensor([-0.0041,  0.0081])\n",
      "loss = 0.1665782332420349\n",
      "Iteration 546: b = tensor([  5.6575, -11.6349], grad_fn=<SubBackward0>), Loss = 0.1665782332420349\n",
      "tensor([-0.0041,  0.0081])\n",
      "loss = 0.16649633646011353\n",
      "Iteration 547: b = tensor([  5.6616, -11.6430], grad_fn=<SubBackward0>), Loss = 0.16649633646011353\n",
      "tensor([-0.0041,  0.0081])\n",
      "loss = 0.16641466319561005\n",
      "Iteration 548: b = tensor([  5.6657, -11.6511], grad_fn=<SubBackward0>), Loss = 0.16641466319561005\n",
      "tensor([-0.0041,  0.0081])\n",
      "loss = 0.16633327305316925\n",
      "Iteration 549: b = tensor([  5.6697, -11.6591], grad_fn=<SubBackward0>), Loss = 0.16633327305316925\n",
      "tensor([-0.0041,  0.0080])\n",
      "loss = 0.16625209152698517\n",
      "Iteration 550: b = tensor([  5.6738, -11.6671], grad_fn=<SubBackward0>), Loss = 0.16625209152698517\n",
      "tensor([-0.0041,  0.0080])\n",
      "loss = 0.16617117822170258\n",
      "Iteration 551: b = tensor([  5.6778, -11.6752], grad_fn=<SubBackward0>), Loss = 0.16617117822170258\n",
      "tensor([-0.0040,  0.0080])\n",
      "loss = 0.1660904884338379\n",
      "Iteration 552: b = tensor([  5.6818, -11.6832], grad_fn=<SubBackward0>), Loss = 0.1660904884338379\n",
      "tensor([-0.0040,  0.0080])\n",
      "loss = 0.1660100221633911\n",
      "Iteration 553: b = tensor([  5.6859, -11.6912], grad_fn=<SubBackward0>), Loss = 0.1660100221633911\n",
      "tensor([-0.0040,  0.0080])\n",
      "loss = 0.16592980921268463\n",
      "Iteration 554: b = tensor([  5.6899, -11.6992], grad_fn=<SubBackward0>), Loss = 0.16592980921268463\n",
      "tensor([-0.0040,  0.0080])\n",
      "loss = 0.16584981977939606\n",
      "Iteration 555: b = tensor([  5.6939, -11.7072], grad_fn=<SubBackward0>), Loss = 0.16584981977939606\n",
      "tensor([-0.0040,  0.0080])\n",
      "loss = 0.16577008366584778\n",
      "Iteration 556: b = tensor([  5.6979, -11.7151], grad_fn=<SubBackward0>), Loss = 0.16577008366584778\n",
      "tensor([-0.0040,  0.0080])\n",
      "loss = 0.1656905561685562\n",
      "Iteration 557: b = tensor([  5.7020, -11.7231], grad_fn=<SubBackward0>), Loss = 0.1656905561685562\n",
      "tensor([-0.0040,  0.0080])\n",
      "loss = 0.16561129689216614\n",
      "Iteration 558: b = tensor([  5.7060, -11.7310], grad_fn=<SubBackward0>), Loss = 0.16561129689216614\n",
      "tensor([-0.0040,  0.0079])\n",
      "loss = 0.16553224623203278\n",
      "Iteration 559: b = tensor([  5.7100, -11.7389], grad_fn=<SubBackward0>), Loss = 0.16553224623203278\n",
      "tensor([-0.0040,  0.0079])\n",
      "loss = 0.16545340418815613\n",
      "Iteration 560: b = tensor([  5.7140, -11.7469], grad_fn=<SubBackward0>), Loss = 0.16545340418815613\n",
      "tensor([-0.0040,  0.0079])\n",
      "loss = 0.16537486016750336\n",
      "Iteration 561: b = tensor([  5.7179, -11.7548], grad_fn=<SubBackward0>), Loss = 0.16537486016750336\n",
      "tensor([-0.0040,  0.0079])\n",
      "loss = 0.16529648005962372\n",
      "Iteration 562: b = tensor([  5.7219, -11.7627], grad_fn=<SubBackward0>), Loss = 0.16529648005962372\n",
      "tensor([-0.0040,  0.0079])\n",
      "loss = 0.16521836817264557\n",
      "Iteration 563: b = tensor([  5.7259, -11.7705], grad_fn=<SubBackward0>), Loss = 0.16521836817264557\n",
      "tensor([-0.0040,  0.0079])\n",
      "loss = 0.16514045000076294\n",
      "Iteration 564: b = tensor([  5.7299, -11.7784], grad_fn=<SubBackward0>), Loss = 0.16514045000076294\n",
      "tensor([-0.0040,  0.0079])\n",
      "loss = 0.1650627702474594\n",
      "Iteration 565: b = tensor([  5.7338, -11.7863], grad_fn=<SubBackward0>), Loss = 0.1650627702474594\n",
      "tensor([-0.0040,  0.0079])\n",
      "loss = 0.16498532891273499\n",
      "Iteration 566: b = tensor([  5.7378, -11.7941], grad_fn=<SubBackward0>), Loss = 0.16498532891273499\n",
      "tensor([-0.0040,  0.0078])\n",
      "loss = 0.16490808129310608\n",
      "Iteration 567: b = tensor([  5.7417, -11.8020], grad_fn=<SubBackward0>), Loss = 0.16490808129310608\n",
      "tensor([-0.0040,  0.0078])\n",
      "loss = 0.16483108699321747\n",
      "Iteration 568: b = tensor([  5.7457, -11.8098], grad_fn=<SubBackward0>), Loss = 0.16483108699321747\n",
      "tensor([-0.0039,  0.0078])\n",
      "loss = 0.16475428640842438\n",
      "Iteration 569: b = tensor([  5.7496, -11.8176], grad_fn=<SubBackward0>), Loss = 0.16475428640842438\n",
      "tensor([-0.0039,  0.0078])\n",
      "loss = 0.1646777093410492\n",
      "Iteration 570: b = tensor([  5.7536, -11.8254], grad_fn=<SubBackward0>), Loss = 0.1646777093410492\n",
      "tensor([-0.0039,  0.0078])\n",
      "loss = 0.1646013855934143\n",
      "Iteration 571: b = tensor([  5.7575, -11.8332], grad_fn=<SubBackward0>), Loss = 0.1646013855934143\n",
      "tensor([-0.0039,  0.0078])\n",
      "loss = 0.16452525556087494\n",
      "Iteration 572: b = tensor([  5.7614, -11.8410], grad_fn=<SubBackward0>), Loss = 0.16452525556087494\n",
      "tensor([-0.0039,  0.0078])\n",
      "loss = 0.16444933414459229\n",
      "Iteration 573: b = tensor([  5.7653, -11.8488], grad_fn=<SubBackward0>), Loss = 0.16444933414459229\n",
      "tensor([-0.0039,  0.0078])\n",
      "loss = 0.16437362134456635\n",
      "Iteration 574: b = tensor([  5.7693, -11.8565], grad_fn=<SubBackward0>), Loss = 0.16437362134456635\n",
      "tensor([-0.0039,  0.0078])\n",
      "loss = 0.1642981320619583\n",
      "Iteration 575: b = tensor([  5.7732, -11.8643], grad_fn=<SubBackward0>), Loss = 0.1642981320619583\n",
      "tensor([-0.0039,  0.0077])\n",
      "loss = 0.16422288119792938\n",
      "Iteration 576: b = tensor([  5.7771, -11.8720], grad_fn=<SubBackward0>), Loss = 0.16422288119792938\n",
      "tensor([-0.0039,  0.0077])\n",
      "loss = 0.16414780914783478\n",
      "Iteration 577: b = tensor([  5.7810, -11.8797], grad_fn=<SubBackward0>), Loss = 0.16414780914783478\n",
      "tensor([-0.0039,  0.0077])\n",
      "loss = 0.16407297551631927\n",
      "Iteration 578: b = tensor([  5.7849, -11.8875], grad_fn=<SubBackward0>), Loss = 0.16407297551631927\n",
      "tensor([-0.0039,  0.0077])\n",
      "loss = 0.1639983206987381\n",
      "Iteration 579: b = tensor([  5.7887, -11.8952], grad_fn=<SubBackward0>), Loss = 0.1639983206987381\n",
      "tensor([-0.0039,  0.0077])\n",
      "loss = 0.16392391920089722\n",
      "Iteration 580: b = tensor([  5.7926, -11.9029], grad_fn=<SubBackward0>), Loss = 0.16392391920089722\n",
      "tensor([-0.0039,  0.0077])\n",
      "loss = 0.16384971141815186\n",
      "Iteration 581: b = tensor([  5.7965, -11.9105], grad_fn=<SubBackward0>), Loss = 0.16384971141815186\n",
      "tensor([-0.0039,  0.0077])\n",
      "loss = 0.1637757122516632\n",
      "Iteration 582: b = tensor([  5.8004, -11.9182], grad_fn=<SubBackward0>), Loss = 0.1637757122516632\n",
      "tensor([-0.0039,  0.0077])\n",
      "loss = 0.16370190680027008\n",
      "Iteration 583: b = tensor([  5.8042, -11.9259], grad_fn=<SubBackward0>), Loss = 0.16370190680027008\n",
      "tensor([-0.0039,  0.0077])\n",
      "loss = 0.16362830996513367\n",
      "Iteration 584: b = tensor([  5.8081, -11.9335], grad_fn=<SubBackward0>), Loss = 0.16362830996513367\n",
      "tensor([-0.0039,  0.0077])\n",
      "loss = 0.16355490684509277\n",
      "Iteration 585: b = tensor([  5.8119, -11.9412], grad_fn=<SubBackward0>), Loss = 0.16355490684509277\n",
      "tensor([-0.0039,  0.0076])\n",
      "loss = 0.16348174214363098\n",
      "Iteration 586: b = tensor([  5.8158, -11.9488], grad_fn=<SubBackward0>), Loss = 0.16348174214363098\n",
      "tensor([-0.0038,  0.0076])\n",
      "loss = 0.1634087711572647\n",
      "Iteration 587: b = tensor([  5.8196, -11.9564], grad_fn=<SubBackward0>), Loss = 0.1634087711572647\n",
      "tensor([-0.0038,  0.0076])\n",
      "loss = 0.16333599388599396\n",
      "Iteration 588: b = tensor([  5.8235, -11.9640], grad_fn=<SubBackward0>), Loss = 0.16333599388599396\n",
      "tensor([-0.0038,  0.0076])\n",
      "loss = 0.16326341032981873\n",
      "Iteration 589: b = tensor([  5.8273, -11.9716], grad_fn=<SubBackward0>), Loss = 0.16326341032981873\n",
      "tensor([-0.0038,  0.0076])\n",
      "loss = 0.1631910502910614\n",
      "Iteration 590: b = tensor([  5.8311, -11.9792], grad_fn=<SubBackward0>), Loss = 0.1631910502910614\n",
      "tensor([-0.0038,  0.0076])\n",
      "loss = 0.1631188690662384\n",
      "Iteration 591: b = tensor([  5.8349, -11.9868], grad_fn=<SubBackward0>), Loss = 0.1631188690662384\n",
      "tensor([-0.0038,  0.0076])\n",
      "loss = 0.1630469113588333\n",
      "Iteration 592: b = tensor([  5.8388, -11.9944], grad_fn=<SubBackward0>), Loss = 0.1630469113588333\n",
      "tensor([-0.0038,  0.0076])\n",
      "loss = 0.16297511756420135\n",
      "Iteration 593: b = tensor([  5.8426, -12.0019], grad_fn=<SubBackward0>), Loss = 0.16297511756420135\n",
      "tensor([-0.0038,  0.0076])\n",
      "loss = 0.1629035472869873\n",
      "Iteration 594: b = tensor([  5.8464, -12.0095], grad_fn=<SubBackward0>), Loss = 0.1629035472869873\n",
      "tensor([-0.0038,  0.0075])\n",
      "loss = 0.16283218562602997\n",
      "Iteration 595: b = tensor([  5.8502, -12.0170], grad_fn=<SubBackward0>), Loss = 0.16283218562602997\n",
      "tensor([-0.0038,  0.0075])\n",
      "loss = 0.16276098787784576\n",
      "Iteration 596: b = tensor([  5.8540, -12.0245], grad_fn=<SubBackward0>), Loss = 0.16276098787784576\n",
      "tensor([-0.0038,  0.0075])\n",
      "loss = 0.16268999874591827\n",
      "Iteration 597: b = tensor([  5.8578, -12.0320], grad_fn=<SubBackward0>), Loss = 0.16268999874591827\n",
      "tensor([-0.0038,  0.0075])\n",
      "loss = 0.1626192033290863\n",
      "Iteration 598: b = tensor([  5.8616, -12.0395], grad_fn=<SubBackward0>), Loss = 0.1626192033290863\n",
      "tensor([-0.0038,  0.0075])\n",
      "loss = 0.16254860162734985\n",
      "Iteration 599: b = tensor([  5.8653, -12.0470], grad_fn=<SubBackward0>), Loss = 0.16254860162734985\n",
      "tensor([-0.0038,  0.0075])\n",
      "loss = 0.16247819364070892\n",
      "Iteration 600: b = tensor([  5.8691, -12.0545], grad_fn=<SubBackward0>), Loss = 0.16247819364070892\n",
      "tensor([-0.0038,  0.0075])\n",
      "loss = 0.16240796446800232\n",
      "Iteration 601: b = tensor([  5.8729, -12.0620], grad_fn=<SubBackward0>), Loss = 0.16240796446800232\n",
      "tensor([-0.0038,  0.0075])\n",
      "loss = 0.16233795881271362\n",
      "Iteration 602: b = tensor([  5.8766, -12.0695], grad_fn=<SubBackward0>), Loss = 0.16233795881271362\n",
      "tensor([-0.0038,  0.0075])\n",
      "loss = 0.16226808726787567\n",
      "Iteration 603: b = tensor([  5.8804, -12.0769], grad_fn=<SubBackward0>), Loss = 0.16226808726787567\n",
      "tensor([-0.0038,  0.0075])\n",
      "loss = 0.16219846904277802\n",
      "Iteration 604: b = tensor([  5.8842, -12.0844], grad_fn=<SubBackward0>), Loss = 0.16219846904277802\n",
      "tensor([-0.0038,  0.0074])\n",
      "loss = 0.1621289998292923\n",
      "Iteration 605: b = tensor([  5.8879, -12.0918], grad_fn=<SubBackward0>), Loss = 0.1621289998292923\n",
      "tensor([-0.0037,  0.0074])\n",
      "loss = 0.1620597094297409\n",
      "Iteration 606: b = tensor([  5.8917, -12.0992], grad_fn=<SubBackward0>), Loss = 0.1620597094297409\n",
      "tensor([-0.0037,  0.0074])\n",
      "loss = 0.16199062764644623\n",
      "Iteration 607: b = tensor([  5.8954, -12.1066], grad_fn=<SubBackward0>), Loss = 0.16199062764644623\n",
      "tensor([-0.0037,  0.0074])\n",
      "loss = 0.16192173957824707\n",
      "Iteration 608: b = tensor([  5.8991, -12.1140], grad_fn=<SubBackward0>), Loss = 0.16192173957824707\n",
      "tensor([-0.0037,  0.0074])\n",
      "loss = 0.16185303032398224\n",
      "Iteration 609: b = tensor([  5.9029, -12.1214], grad_fn=<SubBackward0>), Loss = 0.16185303032398224\n",
      "tensor([-0.0037,  0.0074])\n",
      "loss = 0.16178448498249054\n",
      "Iteration 610: b = tensor([  5.9066, -12.1288], grad_fn=<SubBackward0>), Loss = 0.16178448498249054\n",
      "tensor([-0.0037,  0.0074])\n",
      "loss = 0.16171613335609436\n",
      "Iteration 611: b = tensor([  5.9103, -12.1362], grad_fn=<SubBackward0>), Loss = 0.16171613335609436\n",
      "tensor([-0.0037,  0.0074])\n",
      "loss = 0.1616479754447937\n",
      "Iteration 612: b = tensor([  5.9140, -12.1435], grad_fn=<SubBackward0>), Loss = 0.1616479754447937\n",
      "tensor([-0.0037,  0.0074])\n",
      "loss = 0.16157998144626617\n",
      "Iteration 613: b = tensor([  5.9177, -12.1509], grad_fn=<SubBackward0>), Loss = 0.16157998144626617\n",
      "tensor([-0.0037,  0.0074])\n",
      "loss = 0.16151216626167297\n",
      "Iteration 614: b = tensor([  5.9214, -12.1582], grad_fn=<SubBackward0>), Loss = 0.16151216626167297\n",
      "tensor([-0.0037,  0.0073])\n",
      "loss = 0.1614445596933365\n",
      "Iteration 615: b = tensor([  5.9251, -12.1656], grad_fn=<SubBackward0>), Loss = 0.1614445596933365\n",
      "tensor([-0.0037,  0.0073])\n",
      "loss = 0.16137710213661194\n",
      "Iteration 616: b = tensor([  5.9288, -12.1729], grad_fn=<SubBackward0>), Loss = 0.16137710213661194\n",
      "tensor([-0.0037,  0.0073])\n",
      "loss = 0.1613098382949829\n",
      "Iteration 617: b = tensor([  5.9325, -12.1802], grad_fn=<SubBackward0>), Loss = 0.1613098382949829\n",
      "tensor([-0.0037,  0.0073])\n",
      "loss = 0.1612427532672882\n",
      "Iteration 618: b = tensor([  5.9362, -12.1875], grad_fn=<SubBackward0>), Loss = 0.1612427532672882\n",
      "tensor([-0.0037,  0.0073])\n",
      "loss = 0.16117584705352783\n",
      "Iteration 619: b = tensor([  5.9399, -12.1948], grad_fn=<SubBackward0>), Loss = 0.16117584705352783\n",
      "tensor([-0.0037,  0.0073])\n",
      "loss = 0.16110911965370178\n",
      "Iteration 620: b = tensor([  5.9436, -12.2021], grad_fn=<SubBackward0>), Loss = 0.16110911965370178\n",
      "tensor([-0.0037,  0.0073])\n",
      "loss = 0.16104257106781006\n",
      "Iteration 621: b = tensor([  5.9472, -12.2094], grad_fn=<SubBackward0>), Loss = 0.16104257106781006\n",
      "tensor([-0.0037,  0.0073])\n",
      "loss = 0.16097618639469147\n",
      "Iteration 622: b = tensor([  5.9509, -12.2166], grad_fn=<SubBackward0>), Loss = 0.16097618639469147\n",
      "tensor([-0.0037,  0.0073])\n",
      "loss = 0.1609099805355072\n",
      "Iteration 623: b = tensor([  5.9546, -12.2239], grad_fn=<SubBackward0>), Loss = 0.1609099805355072\n",
      "tensor([-0.0037,  0.0073])\n",
      "loss = 0.16084393858909607\n",
      "Iteration 624: b = tensor([  5.9582, -12.2312], grad_fn=<SubBackward0>), Loss = 0.16084393858909607\n",
      "tensor([-0.0037,  0.0072])\n",
      "loss = 0.16077809035778046\n",
      "Iteration 625: b = tensor([  5.9619, -12.2384], grad_fn=<SubBackward0>), Loss = 0.16077809035778046\n",
      "tensor([-0.0037,  0.0072])\n",
      "loss = 0.16071239113807678\n",
      "Iteration 626: b = tensor([  5.9655, -12.2456], grad_fn=<SubBackward0>), Loss = 0.16071239113807678\n",
      "tensor([-0.0036,  0.0072])\n",
      "loss = 0.16064687073230743\n",
      "Iteration 627: b = tensor([  5.9692, -12.2528], grad_fn=<SubBackward0>), Loss = 0.16064687073230743\n",
      "tensor([-0.0036,  0.0072])\n",
      "loss = 0.16058151423931122\n",
      "Iteration 628: b = tensor([  5.9728, -12.2601], grad_fn=<SubBackward0>), Loss = 0.16058151423931122\n",
      "tensor([-0.0036,  0.0072])\n",
      "loss = 0.16051633656024933\n",
      "Iteration 629: b = tensor([  5.9764, -12.2673], grad_fn=<SubBackward0>), Loss = 0.16051633656024933\n",
      "tensor([-0.0036,  0.0072])\n",
      "loss = 0.16045133769512177\n",
      "Iteration 630: b = tensor([  5.9801, -12.2744], grad_fn=<SubBackward0>), Loss = 0.16045133769512177\n",
      "tensor([-0.0036,  0.0072])\n",
      "loss = 0.16038650274276733\n",
      "Iteration 631: b = tensor([  5.9837, -12.2816], grad_fn=<SubBackward0>), Loss = 0.16038650274276733\n",
      "tensor([-0.0036,  0.0072])\n",
      "loss = 0.16032183170318604\n",
      "Iteration 632: b = tensor([  5.9873, -12.2888], grad_fn=<SubBackward0>), Loss = 0.16032183170318604\n",
      "tensor([-0.0036,  0.0072])\n",
      "loss = 0.16025732457637787\n",
      "Iteration 633: b = tensor([  5.9909, -12.2960], grad_fn=<SubBackward0>), Loss = 0.16025732457637787\n",
      "tensor([-0.0036,  0.0072])\n",
      "loss = 0.16019298136234283\n",
      "Iteration 634: b = tensor([  5.9945, -12.3031], grad_fn=<SubBackward0>), Loss = 0.16019298136234283\n",
      "tensor([-0.0036,  0.0072])\n",
      "loss = 0.16012881696224213\n",
      "Iteration 635: b = tensor([  5.9981, -12.3103], grad_fn=<SubBackward0>), Loss = 0.16012881696224213\n",
      "tensor([-0.0036,  0.0071])\n",
      "loss = 0.16006480157375336\n",
      "Iteration 636: b = tensor([  6.0017, -12.3174], grad_fn=<SubBackward0>), Loss = 0.16006480157375336\n",
      "tensor([-0.0036,  0.0071])\n",
      "loss = 0.16000095009803772\n",
      "Iteration 637: b = tensor([  6.0053, -12.3245], grad_fn=<SubBackward0>), Loss = 0.16000095009803772\n",
      "tensor([-0.0036,  0.0071])\n",
      "loss = 0.15993726253509521\n",
      "Iteration 638: b = tensor([  6.0089, -12.3316], grad_fn=<SubBackward0>), Loss = 0.15993726253509521\n",
      "tensor([-0.0036,  0.0071])\n",
      "loss = 0.15987373888492584\n",
      "Iteration 639: b = tensor([  6.0125, -12.3388], grad_fn=<SubBackward0>), Loss = 0.15987373888492584\n",
      "tensor([-0.0036,  0.0071])\n",
      "loss = 0.1598103940486908\n",
      "Iteration 640: b = tensor([  6.0161, -12.3458], grad_fn=<SubBackward0>), Loss = 0.1598103940486908\n",
      "tensor([-0.0036,  0.0071])\n",
      "loss = 0.1597471982240677\n",
      "Iteration 641: b = tensor([  6.0197, -12.3529], grad_fn=<SubBackward0>), Loss = 0.1597471982240677\n",
      "tensor([-0.0036,  0.0071])\n",
      "loss = 0.1596841812133789\n",
      "Iteration 642: b = tensor([  6.0232, -12.3600], grad_fn=<SubBackward0>), Loss = 0.1596841812133789\n",
      "tensor([-0.0036,  0.0071])\n",
      "loss = 0.15962129831314087\n",
      "Iteration 643: b = tensor([  6.0268, -12.3671], grad_fn=<SubBackward0>), Loss = 0.15962129831314087\n",
      "tensor([-0.0036,  0.0071])\n",
      "loss = 0.15955857932567596\n",
      "Iteration 644: b = tensor([  6.0304, -12.3742], grad_fn=<SubBackward0>), Loss = 0.15955857932567596\n",
      "tensor([-0.0036,  0.0071])\n",
      "loss = 0.15949603915214539\n",
      "Iteration 645: b = tensor([  6.0339, -12.3812], grad_fn=<SubBackward0>), Loss = 0.15949603915214539\n",
      "tensor([-0.0036,  0.0071])\n",
      "loss = 0.15943364799022675\n",
      "Iteration 646: b = tensor([  6.0375, -12.3883], grad_fn=<SubBackward0>), Loss = 0.15943364799022675\n",
      "tensor([-0.0036,  0.0070])\n",
      "loss = 0.15937139093875885\n",
      "Iteration 647: b = tensor([  6.0410, -12.3953], grad_fn=<SubBackward0>), Loss = 0.15937139093875885\n",
      "tensor([-0.0036,  0.0070])\n",
      "loss = 0.15930931270122528\n",
      "Iteration 648: b = tensor([  6.0446, -12.4023], grad_fn=<SubBackward0>), Loss = 0.15930931270122528\n",
      "tensor([-0.0035,  0.0070])\n",
      "loss = 0.15924738347530365\n",
      "Iteration 649: b = tensor([  6.0481, -12.4093], grad_fn=<SubBackward0>), Loss = 0.15924738347530365\n",
      "tensor([-0.0035,  0.0070])\n",
      "loss = 0.15918564796447754\n",
      "Iteration 650: b = tensor([  6.0517, -12.4163], grad_fn=<SubBackward0>), Loss = 0.15918564796447754\n",
      "tensor([-0.0035,  0.0070])\n",
      "loss = 0.15912403166294098\n",
      "Iteration 651: b = tensor([  6.0552, -12.4234], grad_fn=<SubBackward0>), Loss = 0.15912403166294098\n",
      "tensor([-0.0035,  0.0070])\n",
      "loss = 0.15906257927417755\n",
      "Iteration 652: b = tensor([  6.0587, -12.4303], grad_fn=<SubBackward0>), Loss = 0.15906257927417755\n",
      "tensor([-0.0035,  0.0070])\n",
      "loss = 0.15900127589702606\n",
      "Iteration 653: b = tensor([  6.0622, -12.4373], grad_fn=<SubBackward0>), Loss = 0.15900127589702606\n",
      "tensor([-0.0035,  0.0070])\n",
      "loss = 0.1589401215314865\n",
      "Iteration 654: b = tensor([  6.0658, -12.4443], grad_fn=<SubBackward0>), Loss = 0.1589401215314865\n",
      "tensor([-0.0035,  0.0070])\n",
      "loss = 0.1588791161775589\n",
      "Iteration 655: b = tensor([  6.0693, -12.4513], grad_fn=<SubBackward0>), Loss = 0.1588791161775589\n",
      "tensor([-0.0035,  0.0070])\n",
      "loss = 0.15881825983524323\n",
      "Iteration 656: b = tensor([  6.0728, -12.4582], grad_fn=<SubBackward0>), Loss = 0.15881825983524323\n",
      "tensor([-0.0035,  0.0070])\n",
      "loss = 0.1587575525045395\n",
      "Iteration 657: b = tensor([  6.0763, -12.4652], grad_fn=<SubBackward0>), Loss = 0.1587575525045395\n",
      "tensor([-0.0035,  0.0069])\n",
      "loss = 0.15869703888893127\n",
      "Iteration 658: b = tensor([  6.0798, -12.4721], grad_fn=<SubBackward0>), Loss = 0.15869703888893127\n",
      "tensor([-0.0035,  0.0069])\n",
      "loss = 0.1586366444826126\n",
      "Iteration 659: b = tensor([  6.0833, -12.4790], grad_fn=<SubBackward0>), Loss = 0.1586366444826126\n",
      "tensor([-0.0035,  0.0069])\n",
      "loss = 0.1585763841867447\n",
      "Iteration 660: b = tensor([  6.0868, -12.4860], grad_fn=<SubBackward0>), Loss = 0.1585763841867447\n",
      "tensor([-0.0035,  0.0069])\n",
      "loss = 0.1585163027048111\n",
      "Iteration 661: b = tensor([  6.0903, -12.4929], grad_fn=<SubBackward0>), Loss = 0.1585163027048111\n",
      "tensor([-0.0035,  0.0069])\n",
      "loss = 0.15845635533332825\n",
      "Iteration 662: b = tensor([  6.0938, -12.4998], grad_fn=<SubBackward0>), Loss = 0.15845635533332825\n",
      "tensor([-0.0035,  0.0069])\n",
      "loss = 0.15839655697345734\n",
      "Iteration 663: b = tensor([  6.0972, -12.5067], grad_fn=<SubBackward0>), Loss = 0.15839655697345734\n",
      "tensor([-0.0035,  0.0069])\n",
      "loss = 0.15833690762519836\n",
      "Iteration 664: b = tensor([  6.1007, -12.5136], grad_fn=<SubBackward0>), Loss = 0.15833690762519836\n",
      "tensor([-0.0035,  0.0069])\n",
      "loss = 0.15827742218971252\n",
      "Iteration 665: b = tensor([  6.1042, -12.5205], grad_fn=<SubBackward0>), Loss = 0.15827742218971252\n",
      "tensor([-0.0035,  0.0069])\n",
      "loss = 0.15821804106235504\n",
      "Iteration 666: b = tensor([  6.1077, -12.5273], grad_fn=<SubBackward0>), Loss = 0.15821804106235504\n",
      "tensor([-0.0035,  0.0069])\n",
      "loss = 0.15815885365009308\n",
      "Iteration 667: b = tensor([  6.1111, -12.5342], grad_fn=<SubBackward0>), Loss = 0.15815885365009308\n",
      "tensor([-0.0035,  0.0069])\n",
      "loss = 0.15809975564479828\n",
      "Iteration 668: b = tensor([  6.1146, -12.5410], grad_fn=<SubBackward0>), Loss = 0.15809975564479828\n",
      "tensor([-0.0035,  0.0069])\n",
      "loss = 0.1580408662557602\n",
      "Iteration 669: b = tensor([  6.1180, -12.5479], grad_fn=<SubBackward0>), Loss = 0.1580408662557602\n",
      "tensor([-0.0035,  0.0068])\n",
      "loss = 0.15798206627368927\n",
      "Iteration 670: b = tensor([  6.1215, -12.5547], grad_fn=<SubBackward0>), Loss = 0.15798206627368927\n",
      "tensor([-0.0035,  0.0068])\n",
      "loss = 0.15792344510555267\n",
      "Iteration 671: b = tensor([  6.1249, -12.5616], grad_fn=<SubBackward0>), Loss = 0.15792344510555267\n",
      "tensor([-0.0034,  0.0068])\n",
      "loss = 0.15786495804786682\n",
      "Iteration 672: b = tensor([  6.1284, -12.5684], grad_fn=<SubBackward0>), Loss = 0.15786495804786682\n",
      "tensor([-0.0034,  0.0068])\n",
      "loss = 0.15780657529830933\n",
      "Iteration 673: b = tensor([  6.1318, -12.5752], grad_fn=<SubBackward0>), Loss = 0.15780657529830933\n",
      "tensor([-0.0034,  0.0068])\n",
      "loss = 0.15774838626384735\n",
      "Iteration 674: b = tensor([  6.1353, -12.5820], grad_fn=<SubBackward0>), Loss = 0.15774838626384735\n",
      "tensor([-0.0034,  0.0068])\n",
      "loss = 0.15769030153751373\n",
      "Iteration 675: b = tensor([  6.1387, -12.5888], grad_fn=<SubBackward0>), Loss = 0.15769030153751373\n",
      "tensor([-0.0034,  0.0068])\n",
      "loss = 0.15763239562511444\n",
      "Iteration 676: b = tensor([  6.1421, -12.5956], grad_fn=<SubBackward0>), Loss = 0.15763239562511444\n",
      "tensor([-0.0034,  0.0068])\n",
      "loss = 0.1575745940208435\n",
      "Iteration 677: b = tensor([  6.1455, -12.6024], grad_fn=<SubBackward0>), Loss = 0.1575745940208435\n",
      "tensor([-0.0034,  0.0068])\n",
      "loss = 0.1575169563293457\n",
      "Iteration 678: b = tensor([  6.1489, -12.6091], grad_fn=<SubBackward0>), Loss = 0.1575169563293457\n",
      "tensor([-0.0034,  0.0068])\n",
      "loss = 0.15745945274829865\n",
      "Iteration 679: b = tensor([  6.1524, -12.6159], grad_fn=<SubBackward0>), Loss = 0.15745945274829865\n",
      "tensor([-0.0034,  0.0068])\n",
      "loss = 0.15740206837654114\n",
      "Iteration 680: b = tensor([  6.1558, -12.6227], grad_fn=<SubBackward0>), Loss = 0.15740206837654114\n",
      "tensor([-0.0034,  0.0068])\n",
      "loss = 0.15734487771987915\n",
      "Iteration 681: b = tensor([  6.1592, -12.6294], grad_fn=<SubBackward0>), Loss = 0.15734487771987915\n",
      "tensor([-0.0034,  0.0067])\n",
      "loss = 0.15728776156902313\n",
      "Iteration 682: b = tensor([  6.1626, -12.6361], grad_fn=<SubBackward0>), Loss = 0.15728776156902313\n",
      "tensor([-0.0034,  0.0067])\n",
      "loss = 0.15723079442977905\n",
      "Iteration 683: b = tensor([  6.1660, -12.6429], grad_fn=<SubBackward0>), Loss = 0.15723079442977905\n",
      "tensor([-0.0034,  0.0067])\n",
      "loss = 0.15717396140098572\n",
      "Iteration 684: b = tensor([  6.1694, -12.6496], grad_fn=<SubBackward0>), Loss = 0.15717396140098572\n",
      "tensor([-0.0034,  0.0067])\n",
      "loss = 0.15711729228496552\n",
      "Iteration 685: b = tensor([  6.1728, -12.6563], grad_fn=<SubBackward0>), Loss = 0.15711729228496552\n",
      "tensor([-0.0034,  0.0067])\n",
      "loss = 0.15706074237823486\n",
      "Iteration 686: b = tensor([  6.1761, -12.6630], grad_fn=<SubBackward0>), Loss = 0.15706074237823486\n",
      "tensor([-0.0034,  0.0067])\n",
      "loss = 0.15700431168079376\n",
      "Iteration 687: b = tensor([  6.1795, -12.6697], grad_fn=<SubBackward0>), Loss = 0.15700431168079376\n",
      "tensor([-0.0034,  0.0067])\n",
      "loss = 0.1569480448961258\n",
      "Iteration 688: b = tensor([  6.1829, -12.6764], grad_fn=<SubBackward0>), Loss = 0.1569480448961258\n",
      "tensor([-0.0034,  0.0067])\n",
      "loss = 0.15689189732074738\n",
      "Iteration 689: b = tensor([  6.1863, -12.6831], grad_fn=<SubBackward0>), Loss = 0.15689189732074738\n",
      "tensor([-0.0034,  0.0067])\n",
      "loss = 0.1568358838558197\n",
      "Iteration 690: b = tensor([  6.1896, -12.6898], grad_fn=<SubBackward0>), Loss = 0.1568358838558197\n",
      "tensor([-0.0034,  0.0067])\n",
      "loss = 0.15678000450134277\n",
      "Iteration 691: b = tensor([  6.1930, -12.6964], grad_fn=<SubBackward0>), Loss = 0.15678000450134277\n",
      "tensor([-0.0034,  0.0067])\n",
      "loss = 0.1567242592573166\n",
      "Iteration 692: b = tensor([  6.1964, -12.7031], grad_fn=<SubBackward0>), Loss = 0.1567242592573166\n",
      "tensor([-0.0034,  0.0067])\n",
      "loss = 0.15666864812374115\n",
      "Iteration 693: b = tensor([  6.1997, -12.7097], grad_fn=<SubBackward0>), Loss = 0.15666864812374115\n",
      "tensor([-0.0034,  0.0067])\n",
      "loss = 0.15661315619945526\n",
      "Iteration 694: b = tensor([  6.2031, -12.7164], grad_fn=<SubBackward0>), Loss = 0.15661315619945526\n",
      "tensor([-0.0034,  0.0066])\n",
      "loss = 0.15655779838562012\n",
      "Iteration 695: b = tensor([  6.2064, -12.7230], grad_fn=<SubBackward0>), Loss = 0.15655779838562012\n",
      "tensor([-0.0033,  0.0066])\n",
      "loss = 0.15650257468223572\n",
      "Iteration 696: b = tensor([  6.2098, -12.7297], grad_fn=<SubBackward0>), Loss = 0.15650257468223572\n",
      "tensor([-0.0033,  0.0066])\n",
      "loss = 0.15644748508930206\n",
      "Iteration 697: b = tensor([  6.2131, -12.7363], grad_fn=<SubBackward0>), Loss = 0.15644748508930206\n",
      "tensor([-0.0033,  0.0066])\n",
      "loss = 0.15639251470565796\n",
      "Iteration 698: b = tensor([  6.2165, -12.7429], grad_fn=<SubBackward0>), Loss = 0.15639251470565796\n",
      "tensor([-0.0033,  0.0066])\n",
      "loss = 0.1563376784324646\n",
      "Iteration 699: b = tensor([  6.2198, -12.7495], grad_fn=<SubBackward0>), Loss = 0.1563376784324646\n",
      "tensor([-0.0033,  0.0066])\n",
      "loss = 0.15628297626972198\n",
      "Iteration 700: b = tensor([  6.2231, -12.7561], grad_fn=<SubBackward0>), Loss = 0.15628297626972198\n",
      "tensor([-0.0033,  0.0066])\n",
      "loss = 0.15622839331626892\n",
      "Iteration 701: b = tensor([  6.2264, -12.7627], grad_fn=<SubBackward0>), Loss = 0.15622839331626892\n",
      "tensor([-0.0033,  0.0066])\n",
      "loss = 0.1561739444732666\n",
      "Iteration 702: b = tensor([  6.2298, -12.7693], grad_fn=<SubBackward0>), Loss = 0.1561739444732666\n",
      "tensor([-0.0033,  0.0066])\n",
      "loss = 0.15611961483955383\n",
      "Iteration 703: b = tensor([  6.2331, -12.7758], grad_fn=<SubBackward0>), Loss = 0.15611961483955383\n",
      "tensor([-0.0033,  0.0066])\n",
      "loss = 0.15606540441513062\n",
      "Iteration 704: b = tensor([  6.2364, -12.7824], grad_fn=<SubBackward0>), Loss = 0.15606540441513062\n",
      "tensor([-0.0033,  0.0066])\n",
      "loss = 0.15601131319999695\n",
      "Iteration 705: b = tensor([  6.2397, -12.7890], grad_fn=<SubBackward0>), Loss = 0.15601131319999695\n",
      "tensor([-0.0033,  0.0066])\n",
      "loss = 0.1559573858976364\n",
      "Iteration 706: b = tensor([  6.2430, -12.7955], grad_fn=<SubBackward0>), Loss = 0.1559573858976364\n",
      "tensor([-0.0033,  0.0066])\n",
      "loss = 0.15590357780456543\n",
      "Iteration 707: b = tensor([  6.2463, -12.8021], grad_fn=<SubBackward0>), Loss = 0.15590357780456543\n",
      "tensor([-0.0033,  0.0065])\n",
      "loss = 0.1558498591184616\n",
      "Iteration 708: b = tensor([  6.2496, -12.8086], grad_fn=<SubBackward0>), Loss = 0.1558498591184616\n",
      "tensor([-0.0033,  0.0065])\n",
      "loss = 0.15579630434513092\n",
      "Iteration 709: b = tensor([  6.2529, -12.8151], grad_fn=<SubBackward0>), Loss = 0.15579630434513092\n",
      "tensor([-0.0033,  0.0065])\n",
      "loss = 0.1557428389787674\n",
      "Iteration 710: b = tensor([  6.2562, -12.8216], grad_fn=<SubBackward0>), Loss = 0.1557428389787674\n",
      "tensor([-0.0033,  0.0065])\n",
      "loss = 0.15568950772285461\n",
      "Iteration 711: b = tensor([  6.2595, -12.8282], grad_fn=<SubBackward0>), Loss = 0.15568950772285461\n",
      "tensor([-0.0033,  0.0065])\n",
      "loss = 0.15563631057739258\n",
      "Iteration 712: b = tensor([  6.2628, -12.8347], grad_fn=<SubBackward0>), Loss = 0.15563631057739258\n",
      "tensor([-0.0033,  0.0065])\n",
      "loss = 0.1555832177400589\n",
      "Iteration 713: b = tensor([  6.2661, -12.8412], grad_fn=<SubBackward0>), Loss = 0.1555832177400589\n",
      "tensor([-0.0033,  0.0065])\n",
      "loss = 0.15553025901317596\n",
      "Iteration 714: b = tensor([  6.2693, -12.8476], grad_fn=<SubBackward0>), Loss = 0.15553025901317596\n",
      "tensor([-0.0033,  0.0065])\n",
      "loss = 0.15547741949558258\n",
      "Iteration 715: b = tensor([  6.2726, -12.8541], grad_fn=<SubBackward0>), Loss = 0.15547741949558258\n",
      "tensor([-0.0033,  0.0065])\n",
      "loss = 0.15542468428611755\n",
      "Iteration 716: b = tensor([  6.2759, -12.8606], grad_fn=<SubBackward0>), Loss = 0.15542468428611755\n",
      "tensor([-0.0033,  0.0065])\n",
      "loss = 0.15537208318710327\n",
      "Iteration 717: b = tensor([  6.2792, -12.8671], grad_fn=<SubBackward0>), Loss = 0.15537208318710327\n",
      "tensor([-0.0033,  0.0065])\n",
      "loss = 0.15531961619853973\n",
      "Iteration 718: b = tensor([  6.2824, -12.8735], grad_fn=<SubBackward0>), Loss = 0.15531961619853973\n",
      "tensor([-0.0033,  0.0065])\n",
      "loss = 0.15526725351810455\n",
      "Iteration 719: b = tensor([  6.2857, -12.8800], grad_fn=<SubBackward0>), Loss = 0.15526725351810455\n",
      "tensor([-0.0033,  0.0065])\n",
      "loss = 0.15521502494812012\n",
      "Iteration 720: b = tensor([  6.2889, -12.8864], grad_fn=<SubBackward0>), Loss = 0.15521502494812012\n",
      "tensor([-0.0033,  0.0064])\n",
      "loss = 0.15516288578510284\n",
      "Iteration 721: b = tensor([  6.2922, -12.8929], grad_fn=<SubBackward0>), Loss = 0.15516288578510284\n",
      "tensor([-0.0033,  0.0064])\n",
      "loss = 0.1551109105348587\n",
      "Iteration 722: b = tensor([  6.2954, -12.8993], grad_fn=<SubBackward0>), Loss = 0.1551109105348587\n",
      "tensor([-0.0032,  0.0064])\n",
      "loss = 0.15505900979042053\n",
      "Iteration 723: b = tensor([  6.2987, -12.9057], grad_fn=<SubBackward0>), Loss = 0.15505900979042053\n",
      "tensor([-0.0032,  0.0064])\n",
      "loss = 0.1550072431564331\n",
      "Iteration 724: b = tensor([  6.3019, -12.9122], grad_fn=<SubBackward0>), Loss = 0.1550072431564331\n",
      "tensor([-0.0032,  0.0064])\n",
      "loss = 0.15495561063289642\n",
      "Iteration 725: b = tensor([  6.3051, -12.9186], grad_fn=<SubBackward0>), Loss = 0.15495561063289642\n",
      "tensor([-0.0032,  0.0064])\n",
      "loss = 0.1549040675163269\n",
      "Iteration 726: b = tensor([  6.3084, -12.9250], grad_fn=<SubBackward0>), Loss = 0.1549040675163269\n",
      "tensor([-0.0032,  0.0064])\n",
      "loss = 0.15485265851020813\n",
      "Iteration 727: b = tensor([  6.3116, -12.9314], grad_fn=<SubBackward0>), Loss = 0.15485265851020813\n",
      "tensor([-0.0032,  0.0064])\n",
      "loss = 0.15480132400989532\n",
      "Iteration 728: b = tensor([  6.3148, -12.9377], grad_fn=<SubBackward0>), Loss = 0.15480132400989532\n",
      "tensor([-0.0032,  0.0064])\n",
      "loss = 0.15475016832351685\n",
      "Iteration 729: b = tensor([  6.3181, -12.9441], grad_fn=<SubBackward0>), Loss = 0.15475016832351685\n",
      "tensor([-0.0032,  0.0064])\n",
      "loss = 0.15469908714294434\n",
      "Iteration 730: b = tensor([  6.3213, -12.9505], grad_fn=<SubBackward0>), Loss = 0.15469908714294434\n",
      "tensor([-0.0032,  0.0064])\n",
      "loss = 0.15464812517166138\n",
      "Iteration 731: b = tensor([  6.3245, -12.9569], grad_fn=<SubBackward0>), Loss = 0.15464812517166138\n",
      "tensor([-0.0032,  0.0064])\n",
      "loss = 0.15459728240966797\n",
      "Iteration 732: b = tensor([  6.3277, -12.9632], grad_fn=<SubBackward0>), Loss = 0.15459728240966797\n",
      "tensor([-0.0032,  0.0064])\n",
      "loss = 0.1545465588569641\n",
      "Iteration 733: b = tensor([  6.3309, -12.9696], grad_fn=<SubBackward0>), Loss = 0.1545465588569641\n",
      "tensor([-0.0032,  0.0064])\n",
      "loss = 0.1544959545135498\n",
      "Iteration 734: b = tensor([  6.3341, -12.9759], grad_fn=<SubBackward0>), Loss = 0.1544959545135498\n",
      "tensor([-0.0032,  0.0063])\n",
      "loss = 0.15444543957710266\n",
      "Iteration 735: b = tensor([  6.3373, -12.9823], grad_fn=<SubBackward0>), Loss = 0.15444543957710266\n",
      "tensor([-0.0032,  0.0063])\n",
      "loss = 0.15439502894878387\n",
      "Iteration 736: b = tensor([  6.3405, -12.9886], grad_fn=<SubBackward0>), Loss = 0.15439502894878387\n",
      "tensor([-0.0032,  0.0063])\n",
      "loss = 0.15434476733207703\n",
      "Iteration 737: b = tensor([  6.3437, -12.9949], grad_fn=<SubBackward0>), Loss = 0.15434476733207703\n",
      "tensor([-0.0032,  0.0063])\n",
      "loss = 0.15429459512233734\n",
      "Iteration 738: b = tensor([  6.3469, -13.0012], grad_fn=<SubBackward0>), Loss = 0.15429459512233734\n",
      "tensor([-0.0032,  0.0063])\n",
      "loss = 0.1542445421218872\n",
      "Iteration 739: b = tensor([  6.3501, -13.0076], grad_fn=<SubBackward0>), Loss = 0.1542445421218872\n",
      "tensor([-0.0032,  0.0063])\n",
      "loss = 0.15419459342956543\n",
      "Iteration 740: b = tensor([  6.3533, -13.0139], grad_fn=<SubBackward0>), Loss = 0.15419459342956543\n",
      "tensor([-0.0032,  0.0063])\n",
      "loss = 0.1541447639465332\n",
      "Iteration 741: b = tensor([  6.3564, -13.0201], grad_fn=<SubBackward0>), Loss = 0.1541447639465332\n",
      "tensor([-0.0032,  0.0063])\n",
      "loss = 0.15409505367279053\n",
      "Iteration 742: b = tensor([  6.3596, -13.0264], grad_fn=<SubBackward0>), Loss = 0.15409505367279053\n",
      "tensor([-0.0032,  0.0063])\n",
      "loss = 0.15404541790485382\n",
      "Iteration 743: b = tensor([  6.3628, -13.0327], grad_fn=<SubBackward0>), Loss = 0.15404541790485382\n",
      "tensor([-0.0032,  0.0063])\n",
      "loss = 0.15399591624736786\n",
      "Iteration 744: b = tensor([  6.3660, -13.0390], grad_fn=<SubBackward0>), Loss = 0.15399591624736786\n",
      "tensor([-0.0032,  0.0063])\n",
      "loss = 0.15394651889801025\n",
      "Iteration 745: b = tensor([  6.3691, -13.0453], grad_fn=<SubBackward0>), Loss = 0.15394651889801025\n",
      "tensor([-0.0032,  0.0063])\n",
      "loss = 0.153897225856781\n",
      "Iteration 746: b = tensor([  6.3723, -13.0515], grad_fn=<SubBackward0>), Loss = 0.153897225856781\n",
      "tensor([-0.0032,  0.0063])\n",
      "loss = 0.1538480818271637\n",
      "Iteration 747: b = tensor([  6.3754, -13.0578], grad_fn=<SubBackward0>), Loss = 0.1538480818271637\n",
      "tensor([-0.0032,  0.0063])\n",
      "loss = 0.15379898250102997\n",
      "Iteration 748: b = tensor([  6.3786, -13.0640], grad_fn=<SubBackward0>), Loss = 0.15379898250102997\n",
      "tensor([-0.0032,  0.0062])\n",
      "loss = 0.15375001728534698\n",
      "Iteration 749: b = tensor([  6.3817, -13.0703], grad_fn=<SubBackward0>), Loss = 0.15375001728534698\n",
      "tensor([-0.0032,  0.0062])\n",
      "loss = 0.15370115637779236\n",
      "Iteration 750: b = tensor([  6.3849, -13.0765], grad_fn=<SubBackward0>), Loss = 0.15370115637779236\n",
      "tensor([-0.0031,  0.0062])\n",
      "loss = 0.15365241467952728\n",
      "Iteration 751: b = tensor([  6.3880, -13.0827], grad_fn=<SubBackward0>), Loss = 0.15365241467952728\n",
      "tensor([-0.0031,  0.0062])\n",
      "loss = 0.15360377728939056\n",
      "Iteration 752: b = tensor([  6.3912, -13.0890], grad_fn=<SubBackward0>), Loss = 0.15360377728939056\n",
      "tensor([-0.0031,  0.0062])\n",
      "loss = 0.153555229306221\n",
      "Iteration 753: b = tensor([  6.3943, -13.0952], grad_fn=<SubBackward0>), Loss = 0.153555229306221\n",
      "tensor([-0.0031,  0.0062])\n",
      "loss = 0.153506800532341\n",
      "Iteration 754: b = tensor([  6.3975, -13.1014], grad_fn=<SubBackward0>), Loss = 0.153506800532341\n",
      "tensor([-0.0031,  0.0062])\n",
      "loss = 0.15345847606658936\n",
      "Iteration 755: b = tensor([  6.4006, -13.1076], grad_fn=<SubBackward0>), Loss = 0.15345847606658936\n",
      "tensor([-0.0031,  0.0062])\n",
      "loss = 0.15341025590896606\n",
      "Iteration 756: b = tensor([  6.4037, -13.1138], grad_fn=<SubBackward0>), Loss = 0.15341025590896606\n",
      "tensor([-0.0031,  0.0062])\n",
      "loss = 0.15336212515830994\n",
      "Iteration 757: b = tensor([  6.4068, -13.1200], grad_fn=<SubBackward0>), Loss = 0.15336212515830994\n",
      "tensor([-0.0031,  0.0062])\n",
      "loss = 0.15331409871578217\n",
      "Iteration 758: b = tensor([  6.4100, -13.1261], grad_fn=<SubBackward0>), Loss = 0.15331409871578217\n",
      "tensor([-0.0031,  0.0062])\n",
      "loss = 0.15326622128486633\n",
      "Iteration 759: b = tensor([  6.4131, -13.1323], grad_fn=<SubBackward0>), Loss = 0.15326622128486633\n",
      "tensor([-0.0031,  0.0062])\n",
      "loss = 0.15321838855743408\n",
      "Iteration 760: b = tensor([  6.4162, -13.1385], grad_fn=<SubBackward0>), Loss = 0.15321838855743408\n",
      "tensor([-0.0031,  0.0062])\n",
      "loss = 0.15317070484161377\n",
      "Iteration 761: b = tensor([  6.4193, -13.1446], grad_fn=<SubBackward0>), Loss = 0.15317070484161377\n",
      "tensor([-0.0031,  0.0062])\n",
      "loss = 0.15312311053276062\n",
      "Iteration 762: b = tensor([  6.4224, -13.1508], grad_fn=<SubBackward0>), Loss = 0.15312311053276062\n",
      "tensor([-0.0031,  0.0062])\n",
      "loss = 0.15307562053203583\n",
      "Iteration 763: b = tensor([  6.4255, -13.1569], grad_fn=<SubBackward0>), Loss = 0.15307562053203583\n",
      "tensor([-0.0031,  0.0061])\n",
      "loss = 0.1530282199382782\n",
      "Iteration 764: b = tensor([  6.4286, -13.1631], grad_fn=<SubBackward0>), Loss = 0.1530282199382782\n",
      "tensor([-0.0031,  0.0061])\n",
      "loss = 0.15298093855381012\n",
      "Iteration 765: b = tensor([  6.4317, -13.1692], grad_fn=<SubBackward0>), Loss = 0.15298093855381012\n",
      "tensor([-0.0031,  0.0061])\n",
      "loss = 0.152933731675148\n",
      "Iteration 766: b = tensor([  6.4348, -13.1753], grad_fn=<SubBackward0>), Loss = 0.152933731675148\n",
      "tensor([-0.0031,  0.0061])\n",
      "loss = 0.15288662910461426\n",
      "Iteration 767: b = tensor([  6.4379, -13.1815], grad_fn=<SubBackward0>), Loss = 0.15288662910461426\n",
      "tensor([-0.0031,  0.0061])\n",
      "loss = 0.15283964574337006\n",
      "Iteration 768: b = tensor([  6.4410, -13.1876], grad_fn=<SubBackward0>), Loss = 0.15283964574337006\n",
      "tensor([-0.0031,  0.0061])\n",
      "loss = 0.15279275178909302\n",
      "Iteration 769: b = tensor([  6.4441, -13.1937], grad_fn=<SubBackward0>), Loss = 0.15279275178909302\n",
      "tensor([-0.0031,  0.0061])\n",
      "loss = 0.15274596214294434\n",
      "Iteration 770: b = tensor([  6.4472, -13.1998], grad_fn=<SubBackward0>), Loss = 0.15274596214294434\n",
      "tensor([-0.0031,  0.0061])\n",
      "loss = 0.15269926190376282\n",
      "Iteration 771: b = tensor([  6.4502, -13.2059], grad_fn=<SubBackward0>), Loss = 0.15269926190376282\n",
      "tensor([-0.0031,  0.0061])\n",
      "loss = 0.15265266597270966\n",
      "Iteration 772: b = tensor([  6.4533, -13.2120], grad_fn=<SubBackward0>), Loss = 0.15265266597270966\n",
      "tensor([-0.0031,  0.0061])\n",
      "loss = 0.15260618925094604\n",
      "Iteration 773: b = tensor([  6.4564, -13.2180], grad_fn=<SubBackward0>), Loss = 0.15260618925094604\n",
      "tensor([-0.0031,  0.0061])\n",
      "loss = 0.1525598019361496\n",
      "Iteration 774: b = tensor([  6.4594, -13.2241], grad_fn=<SubBackward0>), Loss = 0.1525598019361496\n",
      "tensor([-0.0031,  0.0061])\n",
      "loss = 0.1525135040283203\n",
      "Iteration 775: b = tensor([  6.4625, -13.2302], grad_fn=<SubBackward0>), Loss = 0.1525135040283203\n",
      "tensor([-0.0031,  0.0061])\n",
      "loss = 0.1524672955274582\n",
      "Iteration 776: b = tensor([  6.4656, -13.2362], grad_fn=<SubBackward0>), Loss = 0.1524672955274582\n",
      "tensor([-0.0031,  0.0061])\n",
      "loss = 0.15242117643356323\n",
      "Iteration 777: b = tensor([  6.4686, -13.2423], grad_fn=<SubBackward0>), Loss = 0.15242117643356323\n",
      "tensor([-0.0031,  0.0061])\n",
      "loss = 0.15237519145011902\n",
      "Iteration 778: b = tensor([  6.4717, -13.2484], grad_fn=<SubBackward0>), Loss = 0.15237519145011902\n",
      "tensor([-0.0031,  0.0060])\n",
      "loss = 0.15232928097248077\n",
      "Iteration 779: b = tensor([  6.4747, -13.2544], grad_fn=<SubBackward0>), Loss = 0.15232928097248077\n",
      "tensor([-0.0031,  0.0060])\n",
      "loss = 0.1522834748029709\n",
      "Iteration 780: b = tensor([  6.4778, -13.2604], grad_fn=<SubBackward0>), Loss = 0.1522834748029709\n",
      "tensor([-0.0030,  0.0060])\n",
      "loss = 0.15223775804042816\n",
      "Iteration 781: b = tensor([  6.4808, -13.2665], grad_fn=<SubBackward0>), Loss = 0.15223775804042816\n",
      "tensor([-0.0030,  0.0060])\n",
      "loss = 0.1521921455860138\n",
      "Iteration 782: b = tensor([  6.4839, -13.2725], grad_fn=<SubBackward0>), Loss = 0.1521921455860138\n",
      "tensor([-0.0030,  0.0060])\n",
      "loss = 0.1521466076374054\n",
      "Iteration 783: b = tensor([  6.4869, -13.2785], grad_fn=<SubBackward0>), Loss = 0.1521466076374054\n",
      "tensor([-0.0030,  0.0060])\n",
      "loss = 0.15210117399692535\n",
      "Iteration 784: b = tensor([  6.4900, -13.2845], grad_fn=<SubBackward0>), Loss = 0.15210117399692535\n",
      "tensor([-0.0030,  0.0060])\n",
      "loss = 0.15205585956573486\n",
      "Iteration 785: b = tensor([  6.4930, -13.2905], grad_fn=<SubBackward0>), Loss = 0.15205585956573486\n",
      "tensor([-0.0030,  0.0060])\n",
      "loss = 0.15201063454151154\n",
      "Iteration 786: b = tensor([  6.4960, -13.2965], grad_fn=<SubBackward0>), Loss = 0.15201063454151154\n",
      "tensor([-0.0030,  0.0060])\n",
      "loss = 0.15196546912193298\n",
      "Iteration 787: b = tensor([  6.4990, -13.3025], grad_fn=<SubBackward0>), Loss = 0.15196546912193298\n",
      "tensor([-0.0030,  0.0060])\n",
      "loss = 0.15192042291164398\n",
      "Iteration 788: b = tensor([  6.5021, -13.3085], grad_fn=<SubBackward0>), Loss = 0.15192042291164398\n",
      "tensor([-0.0030,  0.0060])\n",
      "loss = 0.15187546610832214\n",
      "Iteration 789: b = tensor([  6.5051, -13.3145], grad_fn=<SubBackward0>), Loss = 0.15187546610832214\n",
      "tensor([-0.0030,  0.0060])\n",
      "loss = 0.15183062851428986\n",
      "Iteration 790: b = tensor([  6.5081, -13.3205], grad_fn=<SubBackward0>), Loss = 0.15183062851428986\n",
      "tensor([-0.0030,  0.0060])\n",
      "loss = 0.15178583562374115\n",
      "Iteration 791: b = tensor([  6.5111, -13.3264], grad_fn=<SubBackward0>), Loss = 0.15178583562374115\n",
      "tensor([-0.0030,  0.0060])\n",
      "loss = 0.1517411768436432\n",
      "Iteration 792: b = tensor([  6.5141, -13.3324], grad_fn=<SubBackward0>), Loss = 0.1517411768436432\n",
      "tensor([-0.0030,  0.0060])\n",
      "loss = 0.1516965925693512\n",
      "Iteration 793: b = tensor([  6.5171, -13.3383], grad_fn=<SubBackward0>), Loss = 0.1516965925693512\n",
      "tensor([-0.0030,  0.0060])\n",
      "loss = 0.15165209770202637\n",
      "Iteration 794: b = tensor([  6.5201, -13.3443], grad_fn=<SubBackward0>), Loss = 0.15165209770202637\n",
      "tensor([-0.0030,  0.0059])\n",
      "loss = 0.1516076773405075\n",
      "Iteration 795: b = tensor([  6.5231, -13.3502], grad_fn=<SubBackward0>), Loss = 0.1516076773405075\n",
      "tensor([-0.0030,  0.0059])\n",
      "loss = 0.1515633761882782\n",
      "Iteration 796: b = tensor([  6.5261, -13.3562], grad_fn=<SubBackward0>), Loss = 0.1515633761882782\n",
      "tensor([-0.0030,  0.0059])\n",
      "loss = 0.15151914954185486\n",
      "Iteration 797: b = tensor([  6.5291, -13.3621], grad_fn=<SubBackward0>), Loss = 0.15151914954185486\n",
      "tensor([-0.0030,  0.0059])\n",
      "loss = 0.15147502720355988\n",
      "Iteration 798: b = tensor([  6.5321, -13.3680], grad_fn=<SubBackward0>), Loss = 0.15147502720355988\n",
      "tensor([-0.0030,  0.0059])\n",
      "loss = 0.15143100917339325\n",
      "Iteration 799: b = tensor([  6.5351, -13.3739], grad_fn=<SubBackward0>), Loss = 0.15143100917339325\n",
      "tensor([-0.0030,  0.0059])\n",
      "loss = 0.1513870656490326\n",
      "Iteration 800: b = tensor([  6.5381, -13.3799], grad_fn=<SubBackward0>), Loss = 0.1513870656490326\n",
      "tensor([-0.0030,  0.0059])\n",
      "loss = 0.1513431966304779\n",
      "Iteration 801: b = tensor([  6.5411, -13.3858], grad_fn=<SubBackward0>), Loss = 0.1513431966304779\n",
      "tensor([-0.0030,  0.0059])\n",
      "loss = 0.15129944682121277\n",
      "Iteration 802: b = tensor([  6.5441, -13.3917], grad_fn=<SubBackward0>), Loss = 0.15129944682121277\n",
      "tensor([-0.0030,  0.0059])\n",
      "loss = 0.1512557715177536\n",
      "Iteration 803: b = tensor([  6.5471, -13.3976], grad_fn=<SubBackward0>), Loss = 0.1512557715177536\n",
      "tensor([-0.0030,  0.0059])\n",
      "loss = 0.1512121707201004\n",
      "Iteration 804: b = tensor([  6.5500, -13.4034], grad_fn=<SubBackward0>), Loss = 0.1512121707201004\n",
      "tensor([-0.0030,  0.0059])\n",
      "loss = 0.15116868913173676\n",
      "Iteration 805: b = tensor([  6.5530, -13.4093], grad_fn=<SubBackward0>), Loss = 0.15116868913173676\n",
      "tensor([-0.0030,  0.0059])\n",
      "loss = 0.15112528204917908\n",
      "Iteration 806: b = tensor([  6.5560, -13.4152], grad_fn=<SubBackward0>), Loss = 0.15112528204917908\n",
      "tensor([-0.0030,  0.0059])\n",
      "loss = 0.15108196437358856\n",
      "Iteration 807: b = tensor([  6.5589, -13.4211], grad_fn=<SubBackward0>), Loss = 0.15108196437358856\n",
      "tensor([-0.0030,  0.0059])\n",
      "loss = 0.15103872120380402\n",
      "Iteration 808: b = tensor([  6.5619, -13.4269], grad_fn=<SubBackward0>), Loss = 0.15103872120380402\n",
      "tensor([-0.0030,  0.0059])\n",
      "loss = 0.15099559724330902\n",
      "Iteration 809: b = tensor([  6.5649, -13.4328], grad_fn=<SubBackward0>), Loss = 0.15099559724330902\n",
      "tensor([-0.0030,  0.0059])\n",
      "loss = 0.1509525179862976\n",
      "Iteration 810: b = tensor([  6.5678, -13.4386], grad_fn=<SubBackward0>), Loss = 0.1509525179862976\n",
      "tensor([-0.0030,  0.0059])\n",
      "loss = 0.15090952813625336\n",
      "Iteration 811: b = tensor([  6.5708, -13.4445], grad_fn=<SubBackward0>), Loss = 0.15090952813625336\n",
      "tensor([-0.0030,  0.0058])\n",
      "loss = 0.15086667239665985\n",
      "Iteration 812: b = tensor([  6.5737, -13.4503], grad_fn=<SubBackward0>), Loss = 0.15086667239665985\n",
      "tensor([-0.0030,  0.0058])\n",
      "loss = 0.15082389116287231\n",
      "Iteration 813: b = tensor([  6.5767, -13.4562], grad_fn=<SubBackward0>), Loss = 0.15082389116287231\n",
      "tensor([-0.0029,  0.0058])\n",
      "loss = 0.15078116953372955\n",
      "Iteration 814: b = tensor([  6.5796, -13.4620], grad_fn=<SubBackward0>), Loss = 0.15078116953372955\n",
      "tensor([-0.0029,  0.0058])\n",
      "loss = 0.15073855221271515\n",
      "Iteration 815: b = tensor([  6.5825, -13.4678], grad_fn=<SubBackward0>), Loss = 0.15073855221271515\n",
      "tensor([-0.0029,  0.0058])\n",
      "loss = 0.1506960093975067\n",
      "Iteration 816: b = tensor([  6.5855, -13.4736], grad_fn=<SubBackward0>), Loss = 0.1506960093975067\n",
      "tensor([-0.0029,  0.0058])\n",
      "loss = 0.15065357089042664\n",
      "Iteration 817: b = tensor([  6.5884, -13.4794], grad_fn=<SubBackward0>), Loss = 0.15065357089042664\n",
      "tensor([-0.0029,  0.0058])\n",
      "loss = 0.15061119198799133\n",
      "Iteration 818: b = tensor([  6.5914, -13.4853], grad_fn=<SubBackward0>), Loss = 0.15061119198799133\n",
      "tensor([-0.0029,  0.0058])\n",
      "loss = 0.1505689024925232\n",
      "Iteration 819: b = tensor([  6.5943, -13.4911], grad_fn=<SubBackward0>), Loss = 0.1505689024925232\n",
      "tensor([-0.0029,  0.0058])\n",
      "loss = 0.1505267173051834\n",
      "Iteration 820: b = tensor([  6.5972, -13.4968], grad_fn=<SubBackward0>), Loss = 0.1505267173051834\n",
      "tensor([-0.0029,  0.0058])\n",
      "loss = 0.1504846215248108\n",
      "Iteration 821: b = tensor([  6.6001, -13.5026], grad_fn=<SubBackward0>), Loss = 0.1504846215248108\n",
      "tensor([-0.0029,  0.0058])\n",
      "loss = 0.15044258534908295\n",
      "Iteration 822: b = tensor([  6.6031, -13.5084], grad_fn=<SubBackward0>), Loss = 0.15044258534908295\n",
      "tensor([-0.0029,  0.0058])\n",
      "loss = 0.15040065348148346\n",
      "Iteration 823: b = tensor([  6.6060, -13.5142], grad_fn=<SubBackward0>), Loss = 0.15040065348148346\n",
      "tensor([-0.0029,  0.0058])\n",
      "loss = 0.15035878121852875\n",
      "Iteration 824: b = tensor([  6.6089, -13.5200], grad_fn=<SubBackward0>), Loss = 0.15035878121852875\n",
      "tensor([-0.0029,  0.0058])\n",
      "loss = 0.1503169983625412\n",
      "Iteration 825: b = tensor([  6.6118, -13.5257], grad_fn=<SubBackward0>), Loss = 0.1503169983625412\n",
      "tensor([-0.0029,  0.0058])\n",
      "loss = 0.1502753049135208\n",
      "Iteration 826: b = tensor([  6.6147, -13.5315], grad_fn=<SubBackward0>), Loss = 0.1502753049135208\n",
      "tensor([-0.0029,  0.0058])\n",
      "loss = 0.1502336859703064\n",
      "Iteration 827: b = tensor([  6.6176, -13.5372], grad_fn=<SubBackward0>), Loss = 0.1502336859703064\n",
      "tensor([-0.0029,  0.0058])\n",
      "loss = 0.15019217133522034\n",
      "Iteration 828: b = tensor([  6.6205, -13.5430], grad_fn=<SubBackward0>), Loss = 0.15019217133522034\n",
      "tensor([-0.0029,  0.0057])\n",
      "loss = 0.15015071630477905\n",
      "Iteration 829: b = tensor([  6.6234, -13.5487], grad_fn=<SubBackward0>), Loss = 0.15015071630477905\n",
      "tensor([-0.0029,  0.0057])\n",
      "loss = 0.15010936558246613\n",
      "Iteration 830: b = tensor([  6.6263, -13.5545], grad_fn=<SubBackward0>), Loss = 0.15010936558246613\n",
      "tensor([-0.0029,  0.0057])\n",
      "loss = 0.15006804466247559\n",
      "Iteration 831: b = tensor([  6.6292, -13.5602], grad_fn=<SubBackward0>), Loss = 0.15006804466247559\n",
      "tensor([-0.0029,  0.0057])\n",
      "loss = 0.15002687275409698\n",
      "Iteration 832: b = tensor([  6.6321, -13.5659], grad_fn=<SubBackward0>), Loss = 0.15002687275409698\n",
      "tensor([-0.0029,  0.0057])\n",
      "loss = 0.14998574554920197\n",
      "Iteration 833: b = tensor([  6.6350, -13.5716], grad_fn=<SubBackward0>), Loss = 0.14998574554920197\n",
      "tensor([-0.0029,  0.0057])\n",
      "loss = 0.1499447077512741\n",
      "Iteration 834: b = tensor([  6.6379, -13.5774], grad_fn=<SubBackward0>), Loss = 0.1499447077512741\n",
      "tensor([-0.0029,  0.0057])\n",
      "loss = 0.14990374445915222\n",
      "Iteration 835: b = tensor([  6.6408, -13.5831], grad_fn=<SubBackward0>), Loss = 0.14990374445915222\n",
      "tensor([-0.0029,  0.0057])\n",
      "loss = 0.1498628705739975\n",
      "Iteration 836: b = tensor([  6.6437, -13.5888], grad_fn=<SubBackward0>), Loss = 0.1498628705739975\n",
      "tensor([-0.0029,  0.0057])\n",
      "loss = 0.14982210099697113\n",
      "Iteration 837: b = tensor([  6.6465, -13.5945], grad_fn=<SubBackward0>), Loss = 0.14982210099697113\n",
      "tensor([-0.0029,  0.0057])\n",
      "loss = 0.14978137612342834\n",
      "Iteration 838: b = tensor([  6.6494, -13.6002], grad_fn=<SubBackward0>), Loss = 0.14978137612342834\n",
      "tensor([-0.0029,  0.0057])\n",
      "loss = 0.14974074065685272\n",
      "Iteration 839: b = tensor([  6.6523, -13.6058], grad_fn=<SubBackward0>), Loss = 0.14974074065685272\n",
      "tensor([-0.0029,  0.0057])\n",
      "loss = 0.14970017969608307\n",
      "Iteration 840: b = tensor([  6.6552, -13.6115], grad_fn=<SubBackward0>), Loss = 0.14970017969608307\n",
      "tensor([-0.0029,  0.0057])\n",
      "loss = 0.14965970814228058\n",
      "Iteration 841: b = tensor([  6.6580, -13.6172], grad_fn=<SubBackward0>), Loss = 0.14965970814228058\n",
      "tensor([-0.0029,  0.0057])\n",
      "loss = 0.14961932599544525\n",
      "Iteration 842: b = tensor([  6.6609, -13.6229], grad_fn=<SubBackward0>), Loss = 0.14961932599544525\n",
      "tensor([-0.0029,  0.0057])\n",
      "loss = 0.1495789736509323\n",
      "Iteration 843: b = tensor([  6.6637, -13.6285], grad_fn=<SubBackward0>), Loss = 0.1495789736509323\n",
      "tensor([-0.0029,  0.0057])\n",
      "loss = 0.14953875541687012\n",
      "Iteration 844: b = tensor([  6.6666, -13.6342], grad_fn=<SubBackward0>), Loss = 0.14953875541687012\n",
      "tensor([-0.0029,  0.0057])\n",
      "loss = 0.1494985669851303\n",
      "Iteration 845: b = tensor([  6.6695, -13.6398], grad_fn=<SubBackward0>), Loss = 0.1494985669851303\n",
      "tensor([-0.0029,  0.0057])\n",
      "loss = 0.14945849776268005\n",
      "Iteration 846: b = tensor([  6.6723, -13.6455], grad_fn=<SubBackward0>), Loss = 0.14945849776268005\n",
      "tensor([-0.0029,  0.0056])\n",
      "loss = 0.14941850304603577\n",
      "Iteration 847: b = tensor([  6.6752, -13.6511], grad_fn=<SubBackward0>), Loss = 0.14941850304603577\n",
      "tensor([-0.0029,  0.0056])\n",
      "loss = 0.14937858283519745\n",
      "Iteration 848: b = tensor([  6.6780, -13.6568], grad_fn=<SubBackward0>), Loss = 0.14937858283519745\n",
      "tensor([-0.0028,  0.0056])\n",
      "loss = 0.1493387073278427\n",
      "Iteration 849: b = tensor([  6.6809, -13.6624], grad_fn=<SubBackward0>), Loss = 0.1493387073278427\n",
      "tensor([-0.0028,  0.0056])\n",
      "loss = 0.14929893612861633\n",
      "Iteration 850: b = tensor([  6.6837, -13.6680], grad_fn=<SubBackward0>), Loss = 0.14929893612861633\n",
      "tensor([-0.0028,  0.0056])\n",
      "loss = 0.14925923943519592\n",
      "Iteration 851: b = tensor([  6.6865, -13.6736], grad_fn=<SubBackward0>), Loss = 0.14925923943519592\n",
      "tensor([-0.0028,  0.0056])\n",
      "loss = 0.14921963214874268\n",
      "Iteration 852: b = tensor([  6.6894, -13.6792], grad_fn=<SubBackward0>), Loss = 0.14921963214874268\n",
      "tensor([-0.0028,  0.0056])\n",
      "loss = 0.149180069565773\n",
      "Iteration 853: b = tensor([  6.6922, -13.6848], grad_fn=<SubBackward0>), Loss = 0.149180069565773\n",
      "tensor([-0.0028,  0.0056])\n",
      "loss = 0.1491406112909317\n",
      "Iteration 854: b = tensor([  6.6950, -13.6905], grad_fn=<SubBackward0>), Loss = 0.1491406112909317\n",
      "tensor([-0.0028,  0.0056])\n",
      "loss = 0.14910121262073517\n",
      "Iteration 855: b = tensor([  6.6979, -13.6960], grad_fn=<SubBackward0>), Loss = 0.14910121262073517\n",
      "tensor([-0.0028,  0.0056])\n",
      "loss = 0.1490619033575058\n",
      "Iteration 856: b = tensor([  6.7007, -13.7016], grad_fn=<SubBackward0>), Loss = 0.1490619033575058\n",
      "tensor([-0.0028,  0.0056])\n",
      "loss = 0.1490226835012436\n",
      "Iteration 857: b = tensor([  6.7035, -13.7072], grad_fn=<SubBackward0>), Loss = 0.1490226835012436\n",
      "tensor([-0.0028,  0.0056])\n",
      "loss = 0.14898352324962616\n",
      "Iteration 858: b = tensor([  6.7063, -13.7128], grad_fn=<SubBackward0>), Loss = 0.14898352324962616\n",
      "tensor([-0.0028,  0.0056])\n",
      "loss = 0.1489444226026535\n",
      "Iteration 859: b = tensor([  6.7092, -13.7184], grad_fn=<SubBackward0>), Loss = 0.1489444226026535\n",
      "tensor([-0.0028,  0.0056])\n",
      "loss = 0.1489054262638092\n",
      "Iteration 860: b = tensor([  6.7120, -13.7240], grad_fn=<SubBackward0>), Loss = 0.1489054262638092\n",
      "tensor([-0.0028,  0.0056])\n",
      "loss = 0.1488664597272873\n",
      "Iteration 861: b = tensor([  6.7148, -13.7295], grad_fn=<SubBackward0>), Loss = 0.1488664597272873\n",
      "tensor([-0.0028,  0.0056])\n",
      "loss = 0.14882759749889374\n",
      "Iteration 862: b = tensor([  6.7176, -13.7351], grad_fn=<SubBackward0>), Loss = 0.14882759749889374\n",
      "tensor([-0.0028,  0.0056])\n",
      "loss = 0.14878880977630615\n",
      "Iteration 863: b = tensor([  6.7204, -13.7406], grad_fn=<SubBackward0>), Loss = 0.14878880977630615\n",
      "tensor([-0.0028,  0.0056])\n",
      "loss = 0.14875008165836334\n",
      "Iteration 864: b = tensor([  6.7232, -13.7462], grad_fn=<SubBackward0>), Loss = 0.14875008165836334\n",
      "tensor([-0.0028,  0.0055])\n",
      "loss = 0.1487114429473877\n",
      "Iteration 865: b = tensor([  6.7260, -13.7517], grad_fn=<SubBackward0>), Loss = 0.1487114429473877\n",
      "tensor([-0.0028,  0.0055])\n",
      "loss = 0.14867286384105682\n",
      "Iteration 866: b = tensor([  6.7288, -13.7573], grad_fn=<SubBackward0>), Loss = 0.14867286384105682\n",
      "tensor([-0.0028,  0.0055])\n",
      "loss = 0.14863435924053192\n",
      "Iteration 867: b = tensor([  6.7316, -13.7628], grad_fn=<SubBackward0>), Loss = 0.14863435924053192\n",
      "tensor([-0.0028,  0.0055])\n",
      "loss = 0.14859597384929657\n",
      "Iteration 868: b = tensor([  6.7344, -13.7683], grad_fn=<SubBackward0>), Loss = 0.14859597384929657\n",
      "tensor([-0.0028,  0.0055])\n",
      "loss = 0.1485576033592224\n",
      "Iteration 869: b = tensor([  6.7372, -13.7739], grad_fn=<SubBackward0>), Loss = 0.1485576033592224\n",
      "tensor([-0.0028,  0.0055])\n",
      "loss = 0.14851932227611542\n",
      "Iteration 870: b = tensor([  6.7400, -13.7794], grad_fn=<SubBackward0>), Loss = 0.14851932227611542\n",
      "tensor([-0.0028,  0.0055])\n",
      "loss = 0.14848113059997559\n",
      "Iteration 871: b = tensor([  6.7428, -13.7849], grad_fn=<SubBackward0>), Loss = 0.14848113059997559\n",
      "tensor([-0.0028,  0.0055])\n",
      "loss = 0.14844298362731934\n",
      "Iteration 872: b = tensor([  6.7455, -13.7904], grad_fn=<SubBackward0>), Loss = 0.14844298362731934\n",
      "tensor([-0.0028,  0.0055])\n",
      "loss = 0.14840491116046906\n",
      "Iteration 873: b = tensor([  6.7483, -13.7959], grad_fn=<SubBackward0>), Loss = 0.14840491116046906\n",
      "tensor([-0.0028,  0.0055])\n",
      "loss = 0.14836692810058594\n",
      "Iteration 874: b = tensor([  6.7511, -13.8014], grad_fn=<SubBackward0>), Loss = 0.14836692810058594\n",
      "tensor([-0.0028,  0.0055])\n",
      "loss = 0.1483290195465088\n",
      "Iteration 875: b = tensor([  6.7539, -13.8069], grad_fn=<SubBackward0>), Loss = 0.1483290195465088\n",
      "tensor([-0.0028,  0.0055])\n",
      "loss = 0.14829117059707642\n",
      "Iteration 876: b = tensor([  6.7567, -13.8124], grad_fn=<SubBackward0>), Loss = 0.14829117059707642\n",
      "tensor([-0.0028,  0.0055])\n",
      "loss = 0.14825338125228882\n",
      "Iteration 877: b = tensor([  6.7594, -13.8179], grad_fn=<SubBackward0>), Loss = 0.14825338125228882\n",
      "tensor([-0.0028,  0.0055])\n",
      "loss = 0.14821569621562958\n",
      "Iteration 878: b = tensor([  6.7622, -13.8233], grad_fn=<SubBackward0>), Loss = 0.14821569621562958\n",
      "tensor([-0.0028,  0.0055])\n",
      "loss = 0.14817804098129272\n",
      "Iteration 879: b = tensor([  6.7650, -13.8288], grad_fn=<SubBackward0>), Loss = 0.14817804098129272\n",
      "tensor([-0.0028,  0.0055])\n",
      "loss = 0.14814049005508423\n",
      "Iteration 880: b = tensor([  6.7677, -13.8343], grad_fn=<SubBackward0>), Loss = 0.14814049005508423\n",
      "tensor([-0.0028,  0.0055])\n",
      "loss = 0.1481029987335205\n",
      "Iteration 881: b = tensor([  6.7705, -13.8397], grad_fn=<SubBackward0>), Loss = 0.1481029987335205\n",
      "tensor([-0.0028,  0.0055])\n",
      "loss = 0.14806556701660156\n",
      "Iteration 882: b = tensor([  6.7732, -13.8452], grad_fn=<SubBackward0>), Loss = 0.14806556701660156\n",
      "tensor([-0.0028,  0.0055])\n",
      "loss = 0.1480282098054886\n",
      "Iteration 883: b = tensor([  6.7760, -13.8506], grad_fn=<SubBackward0>), Loss = 0.1480282098054886\n",
      "tensor([-0.0028,  0.0055])\n",
      "loss = 0.14799092710018158\n",
      "Iteration 884: b = tensor([  6.7787, -13.8561], grad_fn=<SubBackward0>), Loss = 0.14799092710018158\n",
      "tensor([-0.0028,  0.0054])\n",
      "loss = 0.14795370399951935\n",
      "Iteration 885: b = tensor([  6.7815, -13.8615], grad_fn=<SubBackward0>), Loss = 0.14795370399951935\n",
      "tensor([-0.0027,  0.0054])\n",
      "loss = 0.14791657030582428\n",
      "Iteration 886: b = tensor([  6.7842, -13.8670], grad_fn=<SubBackward0>), Loss = 0.14791657030582428\n",
      "tensor([-0.0027,  0.0054])\n",
      "loss = 0.1478794664144516\n",
      "Iteration 887: b = tensor([  6.7870, -13.8724], grad_fn=<SubBackward0>), Loss = 0.1478794664144516\n",
      "tensor([-0.0027,  0.0054])\n",
      "loss = 0.14784248173236847\n",
      "Iteration 888: b = tensor([  6.7897, -13.8778], grad_fn=<SubBackward0>), Loss = 0.14784248173236847\n",
      "tensor([-0.0027,  0.0054])\n",
      "loss = 0.14780554175376892\n",
      "Iteration 889: b = tensor([  6.7925, -13.8832], grad_fn=<SubBackward0>), Loss = 0.14780554175376892\n",
      "tensor([-0.0027,  0.0054])\n",
      "loss = 0.14776867628097534\n",
      "Iteration 890: b = tensor([  6.7952, -13.8886], grad_fn=<SubBackward0>), Loss = 0.14776867628097534\n",
      "tensor([-0.0027,  0.0054])\n",
      "loss = 0.14773187041282654\n",
      "Iteration 891: b = tensor([  6.7979, -13.8941], grad_fn=<SubBackward0>), Loss = 0.14773187041282654\n",
      "tensor([-0.0027,  0.0054])\n",
      "loss = 0.1476951390504837\n",
      "Iteration 892: b = tensor([  6.8007, -13.8995], grad_fn=<SubBackward0>), Loss = 0.1476951390504837\n",
      "tensor([-0.0027,  0.0054])\n",
      "loss = 0.14765845239162445\n",
      "Iteration 893: b = tensor([  6.8034, -13.9049], grad_fn=<SubBackward0>), Loss = 0.14765845239162445\n",
      "tensor([-0.0027,  0.0054])\n",
      "loss = 0.14762187004089355\n",
      "Iteration 894: b = tensor([  6.8061, -13.9103], grad_fn=<SubBackward0>), Loss = 0.14762187004089355\n",
      "tensor([-0.0027,  0.0054])\n",
      "loss = 0.14758531749248505\n",
      "Iteration 895: b = tensor([  6.8089, -13.9157], grad_fn=<SubBackward0>), Loss = 0.14758531749248505\n",
      "tensor([-0.0027,  0.0054])\n",
      "loss = 0.1475488543510437\n",
      "Iteration 896: b = tensor([  6.8116, -13.9210], grad_fn=<SubBackward0>), Loss = 0.1475488543510437\n",
      "tensor([-0.0027,  0.0054])\n",
      "loss = 0.14751245081424713\n",
      "Iteration 897: b = tensor([  6.8143, -13.9264], grad_fn=<SubBackward0>), Loss = 0.14751245081424713\n",
      "tensor([-0.0027,  0.0054])\n",
      "loss = 0.14747612178325653\n",
      "Iteration 898: b = tensor([  6.8170, -13.9318], grad_fn=<SubBackward0>), Loss = 0.14747612178325653\n",
      "tensor([-0.0027,  0.0054])\n",
      "loss = 0.1474398821592331\n",
      "Iteration 899: b = tensor([  6.8197, -13.9372], grad_fn=<SubBackward0>), Loss = 0.1474398821592331\n",
      "tensor([-0.0027,  0.0054])\n",
      "loss = 0.14740367233753204\n",
      "Iteration 900: b = tensor([  6.8224, -13.9425], grad_fn=<SubBackward0>), Loss = 0.14740367233753204\n",
      "tensor([-0.0027,  0.0054])\n",
      "loss = 0.14736755192279816\n",
      "Iteration 901: b = tensor([  6.8252, -13.9479], grad_fn=<SubBackward0>), Loss = 0.14736755192279816\n",
      "tensor([-0.0027,  0.0054])\n",
      "loss = 0.14733147621154785\n",
      "Iteration 902: b = tensor([  6.8279, -13.9532], grad_fn=<SubBackward0>), Loss = 0.14733147621154785\n",
      "tensor([-0.0027,  0.0054])\n",
      "loss = 0.14729546010494232\n",
      "Iteration 903: b = tensor([  6.8306, -13.9586], grad_fn=<SubBackward0>), Loss = 0.14729546010494232\n",
      "tensor([-0.0027,  0.0054])\n",
      "loss = 0.14725954830646515\n",
      "Iteration 904: b = tensor([  6.8333, -13.9639], grad_fn=<SubBackward0>), Loss = 0.14725954830646515\n",
      "tensor([-0.0027,  0.0053])\n",
      "loss = 0.14722368121147156\n",
      "Iteration 905: b = tensor([  6.8360, -13.9693], grad_fn=<SubBackward0>), Loss = 0.14722368121147156\n",
      "tensor([-0.0027,  0.0053])\n",
      "loss = 0.14718785881996155\n",
      "Iteration 906: b = tensor([  6.8387, -13.9746], grad_fn=<SubBackward0>), Loss = 0.14718785881996155\n",
      "tensor([-0.0027,  0.0053])\n",
      "loss = 0.1471521407365799\n",
      "Iteration 907: b = tensor([  6.8414, -13.9800], grad_fn=<SubBackward0>), Loss = 0.1471521407365799\n",
      "tensor([-0.0027,  0.0053])\n",
      "loss = 0.14711645245552063\n",
      "Iteration 908: b = tensor([  6.8441, -13.9853], grad_fn=<SubBackward0>), Loss = 0.14711645245552063\n",
      "tensor([-0.0027,  0.0053])\n",
      "loss = 0.14708085358142853\n",
      "Iteration 909: b = tensor([  6.8467, -13.9906], grad_fn=<SubBackward0>), Loss = 0.14708085358142853\n",
      "tensor([-0.0027,  0.0053])\n",
      "loss = 0.1470453292131424\n",
      "Iteration 910: b = tensor([  6.8494, -13.9959], grad_fn=<SubBackward0>), Loss = 0.1470453292131424\n",
      "tensor([-0.0027,  0.0053])\n",
      "loss = 0.14700981974601746\n",
      "Iteration 911: b = tensor([  6.8521, -14.0012], grad_fn=<SubBackward0>), Loss = 0.14700981974601746\n",
      "tensor([-0.0027,  0.0053])\n",
      "loss = 0.14697441458702087\n",
      "Iteration 912: b = tensor([  6.8548, -14.0065], grad_fn=<SubBackward0>), Loss = 0.14697441458702087\n",
      "tensor([-0.0027,  0.0053])\n",
      "loss = 0.14693905413150787\n",
      "Iteration 913: b = tensor([  6.8575, -14.0118], grad_fn=<SubBackward0>), Loss = 0.14693905413150787\n",
      "tensor([-0.0027,  0.0053])\n",
      "loss = 0.14690375328063965\n",
      "Iteration 914: b = tensor([  6.8602, -14.0171], grad_fn=<SubBackward0>), Loss = 0.14690375328063965\n",
      "tensor([-0.0027,  0.0053])\n",
      "loss = 0.1468685418367386\n",
      "Iteration 915: b = tensor([  6.8628, -14.0224], grad_fn=<SubBackward0>), Loss = 0.1468685418367386\n",
      "tensor([-0.0027,  0.0053])\n",
      "loss = 0.1468334048986435\n",
      "Iteration 916: b = tensor([  6.8655, -14.0277], grad_fn=<SubBackward0>), Loss = 0.1468334048986435\n",
      "tensor([-0.0027,  0.0053])\n",
      "loss = 0.1467982977628708\n",
      "Iteration 917: b = tensor([  6.8682, -14.0330], grad_fn=<SubBackward0>), Loss = 0.1467982977628708\n",
      "tensor([-0.0027,  0.0053])\n",
      "loss = 0.14676325023174286\n",
      "Iteration 918: b = tensor([  6.8708, -14.0383], grad_fn=<SubBackward0>), Loss = 0.14676325023174286\n",
      "tensor([-0.0027,  0.0053])\n",
      "loss = 0.1467282921075821\n",
      "Iteration 919: b = tensor([  6.8735, -14.0436], grad_fn=<SubBackward0>), Loss = 0.1467282921075821\n",
      "tensor([-0.0027,  0.0053])\n",
      "loss = 0.1466933786869049\n",
      "Iteration 920: b = tensor([  6.8762, -14.0488], grad_fn=<SubBackward0>), Loss = 0.1466933786869049\n",
      "tensor([-0.0027,  0.0053])\n",
      "loss = 0.1466585248708725\n",
      "Iteration 921: b = tensor([  6.8788, -14.0541], grad_fn=<SubBackward0>), Loss = 0.1466585248708725\n",
      "tensor([-0.0027,  0.0053])\n",
      "loss = 0.14662374556064606\n",
      "Iteration 922: b = tensor([  6.8815, -14.0593], grad_fn=<SubBackward0>), Loss = 0.14662374556064606\n",
      "tensor([-0.0027,  0.0053])\n",
      "loss = 0.14658904075622559\n",
      "Iteration 923: b = tensor([  6.8842, -14.0646], grad_fn=<SubBackward0>), Loss = 0.14658904075622559\n",
      "tensor([-0.0027,  0.0053])\n",
      "loss = 0.1465543806552887\n",
      "Iteration 924: b = tensor([  6.8868, -14.0699], grad_fn=<SubBackward0>), Loss = 0.1465543806552887\n",
      "tensor([-0.0027,  0.0053])\n",
      "loss = 0.1465197652578354\n",
      "Iteration 925: b = tensor([  6.8895, -14.0751], grad_fn=<SubBackward0>), Loss = 0.1465197652578354\n",
      "tensor([-0.0027,  0.0052])\n",
      "loss = 0.14648523926734924\n",
      "Iteration 926: b = tensor([  6.8921, -14.0803], grad_fn=<SubBackward0>), Loss = 0.14648523926734924\n",
      "tensor([-0.0026,  0.0052])\n",
      "loss = 0.14645075798034668\n",
      "Iteration 927: b = tensor([  6.8948, -14.0856], grad_fn=<SubBackward0>), Loss = 0.14645075798034668\n",
      "tensor([-0.0026,  0.0052])\n",
      "loss = 0.1464163362979889\n",
      "Iteration 928: b = tensor([  6.8974, -14.0908], grad_fn=<SubBackward0>), Loss = 0.1464163362979889\n",
      "tensor([-0.0026,  0.0052])\n",
      "loss = 0.14638197422027588\n",
      "Iteration 929: b = tensor([  6.9000, -14.0960], grad_fn=<SubBackward0>), Loss = 0.14638197422027588\n",
      "tensor([-0.0026,  0.0052])\n",
      "loss = 0.14634770154953003\n",
      "Iteration 930: b = tensor([  6.9027, -14.1013], grad_fn=<SubBackward0>), Loss = 0.14634770154953003\n",
      "tensor([-0.0026,  0.0052])\n",
      "loss = 0.14631347358226776\n",
      "Iteration 931: b = tensor([  6.9053, -14.1065], grad_fn=<SubBackward0>), Loss = 0.14631347358226776\n",
      "tensor([-0.0026,  0.0052])\n",
      "loss = 0.14627930521965027\n",
      "Iteration 932: b = tensor([  6.9080, -14.1117], grad_fn=<SubBackward0>), Loss = 0.14627930521965027\n",
      "tensor([-0.0026,  0.0052])\n",
      "loss = 0.14624519646167755\n",
      "Iteration 933: b = tensor([  6.9106, -14.1169], grad_fn=<SubBackward0>), Loss = 0.14624519646167755\n",
      "tensor([-0.0026,  0.0052])\n",
      "loss = 0.1462111473083496\n",
      "Iteration 934: b = tensor([  6.9132, -14.1221], grad_fn=<SubBackward0>), Loss = 0.1462111473083496\n",
      "tensor([-0.0026,  0.0052])\n",
      "loss = 0.14617715775966644\n",
      "Iteration 935: b = tensor([  6.9159, -14.1273], grad_fn=<SubBackward0>), Loss = 0.14617715775966644\n",
      "tensor([-0.0026,  0.0052])\n",
      "loss = 0.14614321291446686\n",
      "Iteration 936: b = tensor([  6.9185, -14.1325], grad_fn=<SubBackward0>), Loss = 0.14614321291446686\n",
      "tensor([-0.0026,  0.0052])\n",
      "loss = 0.14610934257507324\n",
      "Iteration 937: b = tensor([  6.9211, -14.1377], grad_fn=<SubBackward0>), Loss = 0.14610934257507324\n",
      "tensor([-0.0026,  0.0052])\n",
      "loss = 0.1460755318403244\n",
      "Iteration 938: b = tensor([  6.9237, -14.1429], grad_fn=<SubBackward0>), Loss = 0.1460755318403244\n",
      "tensor([-0.0026,  0.0052])\n",
      "loss = 0.14604179561138153\n",
      "Iteration 939: b = tensor([  6.9263, -14.1481], grad_fn=<SubBackward0>), Loss = 0.14604179561138153\n",
      "tensor([-0.0026,  0.0052])\n",
      "loss = 0.14600808918476105\n",
      "Iteration 940: b = tensor([  6.9290, -14.1532], grad_fn=<SubBackward0>), Loss = 0.14600808918476105\n",
      "tensor([-0.0026,  0.0052])\n",
      "loss = 0.14597445726394653\n",
      "Iteration 941: b = tensor([  6.9316, -14.1584], grad_fn=<SubBackward0>), Loss = 0.14597445726394653\n",
      "tensor([-0.0026,  0.0052])\n",
      "loss = 0.1459408849477768\n",
      "Iteration 942: b = tensor([  6.9342, -14.1636], grad_fn=<SubBackward0>), Loss = 0.1459408849477768\n",
      "tensor([-0.0026,  0.0052])\n",
      "loss = 0.14590738713741302\n",
      "Iteration 943: b = tensor([  6.9368, -14.1687], grad_fn=<SubBackward0>), Loss = 0.14590738713741302\n",
      "tensor([-0.0026,  0.0052])\n",
      "loss = 0.14587394893169403\n",
      "Iteration 944: b = tensor([  6.9394, -14.1739], grad_fn=<SubBackward0>), Loss = 0.14587394893169403\n",
      "tensor([-0.0026,  0.0052])\n",
      "loss = 0.14584052562713623\n",
      "Iteration 945: b = tensor([  6.9420, -14.1790], grad_fn=<SubBackward0>), Loss = 0.14584052562713623\n",
      "tensor([-0.0026,  0.0052])\n",
      "loss = 0.1458071917295456\n",
      "Iteration 946: b = tensor([  6.9446, -14.1842], grad_fn=<SubBackward0>), Loss = 0.1458071917295456\n",
      "tensor([-0.0026,  0.0051])\n",
      "loss = 0.14577391743659973\n",
      "Iteration 947: b = tensor([  6.9472, -14.1893], grad_fn=<SubBackward0>), Loss = 0.14577391743659973\n",
      "tensor([-0.0026,  0.0051])\n",
      "loss = 0.14574067294597626\n",
      "Iteration 948: b = tensor([  6.9498, -14.1945], grad_fn=<SubBackward0>), Loss = 0.14574067294597626\n",
      "tensor([-0.0026,  0.0051])\n",
      "loss = 0.14570750296115875\n",
      "Iteration 949: b = tensor([  6.9524, -14.1996], grad_fn=<SubBackward0>), Loss = 0.14570750296115875\n",
      "tensor([-0.0026,  0.0051])\n",
      "loss = 0.1456744223833084\n",
      "Iteration 950: b = tensor([  6.9550, -14.2047], grad_fn=<SubBackward0>), Loss = 0.1456744223833084\n",
      "tensor([-0.0026,  0.0051])\n",
      "loss = 0.14564135670661926\n",
      "Iteration 951: b = tensor([  6.9576, -14.2099], grad_fn=<SubBackward0>), Loss = 0.14564135670661926\n",
      "tensor([-0.0026,  0.0051])\n",
      "loss = 0.14560838043689728\n",
      "Iteration 952: b = tensor([  6.9602, -14.2150], grad_fn=<SubBackward0>), Loss = 0.14560838043689728\n",
      "tensor([-0.0026,  0.0051])\n",
      "loss = 0.14557544887065887\n",
      "Iteration 953: b = tensor([  6.9628, -14.2201], grad_fn=<SubBackward0>), Loss = 0.14557544887065887\n",
      "tensor([-0.0026,  0.0051])\n",
      "loss = 0.14554254710674286\n",
      "Iteration 954: b = tensor([  6.9654, -14.2252], grad_fn=<SubBackward0>), Loss = 0.14554254710674286\n",
      "tensor([-0.0026,  0.0051])\n",
      "loss = 0.145509734749794\n",
      "Iteration 955: b = tensor([  6.9680, -14.2303], grad_fn=<SubBackward0>), Loss = 0.145509734749794\n",
      "tensor([-0.0026,  0.0051])\n",
      "loss = 0.14547698199748993\n",
      "Iteration 956: b = tensor([  6.9705, -14.2354], grad_fn=<SubBackward0>), Loss = 0.14547698199748993\n",
      "tensor([-0.0026,  0.0051])\n",
      "loss = 0.14544427394866943\n",
      "Iteration 957: b = tensor([  6.9731, -14.2405], grad_fn=<SubBackward0>), Loss = 0.14544427394866943\n",
      "tensor([-0.0026,  0.0051])\n",
      "loss = 0.14541159570217133\n",
      "Iteration 958: b = tensor([  6.9757, -14.2456], grad_fn=<SubBackward0>), Loss = 0.14541159570217133\n",
      "tensor([-0.0026,  0.0051])\n",
      "loss = 0.1453789919614792\n",
      "Iteration 959: b = tensor([  6.9783, -14.2507], grad_fn=<SubBackward0>), Loss = 0.1453789919614792\n",
      "tensor([-0.0026,  0.0051])\n",
      "loss = 0.14534644782543182\n",
      "Iteration 960: b = tensor([  6.9808, -14.2558], grad_fn=<SubBackward0>), Loss = 0.14534644782543182\n",
      "tensor([-0.0026,  0.0051])\n",
      "loss = 0.14531399309635162\n",
      "Iteration 961: b = tensor([  6.9834, -14.2609], grad_fn=<SubBackward0>), Loss = 0.14531399309635162\n",
      "tensor([-0.0026,  0.0051])\n",
      "loss = 0.14528155326843262\n",
      "Iteration 962: b = tensor([  6.9860, -14.2660], grad_fn=<SubBackward0>), Loss = 0.14528155326843262\n",
      "tensor([-0.0026,  0.0051])\n",
      "loss = 0.14524920284748077\n",
      "Iteration 963: b = tensor([  6.9885, -14.2711], grad_fn=<SubBackward0>), Loss = 0.14524920284748077\n",
      "tensor([-0.0026,  0.0051])\n",
      "loss = 0.14521686732769012\n",
      "Iteration 964: b = tensor([  6.9911, -14.2761], grad_fn=<SubBackward0>), Loss = 0.14521686732769012\n",
      "tensor([-0.0026,  0.0051])\n",
      "loss = 0.14518462121486664\n",
      "Iteration 965: b = tensor([  6.9937, -14.2812], grad_fn=<SubBackward0>), Loss = 0.14518462121486664\n",
      "tensor([-0.0026,  0.0051])\n",
      "loss = 0.14515241980552673\n",
      "Iteration 966: b = tensor([  6.9962, -14.2863], grad_fn=<SubBackward0>), Loss = 0.14515241980552673\n",
      "tensor([-0.0026,  0.0051])\n",
      "loss = 0.14512024819850922\n",
      "Iteration 967: b = tensor([  6.9988, -14.2913], grad_fn=<SubBackward0>), Loss = 0.14512024819850922\n",
      "tensor([-0.0026,  0.0051])\n",
      "loss = 0.14508816599845886\n",
      "Iteration 968: b = tensor([  7.0013, -14.2964], grad_fn=<SubBackward0>), Loss = 0.14508816599845886\n",
      "tensor([-0.0026,  0.0051])\n",
      "loss = 0.14505614340305328\n",
      "Iteration 969: b = tensor([  7.0039, -14.3014], grad_fn=<SubBackward0>), Loss = 0.14505614340305328\n",
      "tensor([-0.0026,  0.0050])\n",
      "loss = 0.1450241357088089\n",
      "Iteration 970: b = tensor([  7.0064, -14.3065], grad_fn=<SubBackward0>), Loss = 0.1450241357088089\n",
      "tensor([-0.0026,  0.0050])\n",
      "loss = 0.14499220252037048\n",
      "Iteration 971: b = tensor([  7.0090, -14.3115], grad_fn=<SubBackward0>), Loss = 0.14499220252037048\n",
      "tensor([-0.0025,  0.0050])\n",
      "loss = 0.14496032893657684\n",
      "Iteration 972: b = tensor([  7.0115, -14.3165], grad_fn=<SubBackward0>), Loss = 0.14496032893657684\n",
      "tensor([-0.0025,  0.0050])\n",
      "loss = 0.14492851495742798\n",
      "Iteration 973: b = tensor([  7.0141, -14.3216], grad_fn=<SubBackward0>), Loss = 0.14492851495742798\n",
      "tensor([-0.0025,  0.0050])\n",
      "loss = 0.1448967456817627\n",
      "Iteration 974: b = tensor([  7.0166, -14.3266], grad_fn=<SubBackward0>), Loss = 0.1448967456817627\n",
      "tensor([-0.0025,  0.0050])\n",
      "loss = 0.144865021109581\n",
      "Iteration 975: b = tensor([  7.0192, -14.3316], grad_fn=<SubBackward0>), Loss = 0.144865021109581\n",
      "tensor([-0.0025,  0.0050])\n",
      "loss = 0.14483337104320526\n",
      "Iteration 976: b = tensor([  7.0217, -14.3366], grad_fn=<SubBackward0>), Loss = 0.14483337104320526\n",
      "tensor([-0.0025,  0.0050])\n",
      "loss = 0.1448017656803131\n",
      "Iteration 977: b = tensor([  7.0242, -14.3416], grad_fn=<SubBackward0>), Loss = 0.1448017656803131\n",
      "tensor([-0.0025,  0.0050])\n",
      "loss = 0.14477021992206573\n",
      "Iteration 978: b = tensor([  7.0268, -14.3466], grad_fn=<SubBackward0>), Loss = 0.14477021992206573\n",
      "tensor([-0.0025,  0.0050])\n",
      "loss = 0.14473871886730194\n",
      "Iteration 979: b = tensor([  7.0293, -14.3517], grad_fn=<SubBackward0>), Loss = 0.14473871886730194\n",
      "tensor([-0.0025,  0.0050])\n",
      "loss = 0.14470726251602173\n",
      "Iteration 980: b = tensor([  7.0318, -14.3567], grad_fn=<SubBackward0>), Loss = 0.14470726251602173\n",
      "tensor([-0.0025,  0.0050])\n",
      "loss = 0.1446758657693863\n",
      "Iteration 981: b = tensor([  7.0344, -14.3617], grad_fn=<SubBackward0>), Loss = 0.1446758657693863\n",
      "tensor([-0.0025,  0.0050])\n",
      "loss = 0.14464451372623444\n",
      "Iteration 982: b = tensor([  7.0369, -14.3666], grad_fn=<SubBackward0>), Loss = 0.14464451372623444\n",
      "tensor([-0.0025,  0.0050])\n",
      "loss = 0.14461323618888855\n",
      "Iteration 983: b = tensor([  7.0394, -14.3716], grad_fn=<SubBackward0>), Loss = 0.14461323618888855\n",
      "tensor([-0.0025,  0.0050])\n",
      "loss = 0.14458200335502625\n",
      "Iteration 984: b = tensor([  7.0419, -14.3766], grad_fn=<SubBackward0>), Loss = 0.14458200335502625\n",
      "tensor([-0.0025,  0.0050])\n",
      "loss = 0.14455081522464752\n",
      "Iteration 985: b = tensor([  7.0444, -14.3816], grad_fn=<SubBackward0>), Loss = 0.14455081522464752\n",
      "tensor([-0.0025,  0.0050])\n",
      "loss = 0.1445196568965912\n",
      "Iteration 986: b = tensor([  7.0470, -14.3866], grad_fn=<SubBackward0>), Loss = 0.1445196568965912\n",
      "tensor([-0.0025,  0.0050])\n",
      "loss = 0.1444886028766632\n",
      "Iteration 987: b = tensor([  7.0495, -14.3915], grad_fn=<SubBackward0>), Loss = 0.1444886028766632\n",
      "tensor([-0.0025,  0.0050])\n",
      "loss = 0.14445757865905762\n",
      "Iteration 988: b = tensor([  7.0520, -14.3965], grad_fn=<SubBackward0>), Loss = 0.14445757865905762\n",
      "tensor([-0.0025,  0.0050])\n",
      "loss = 0.1444265991449356\n",
      "Iteration 989: b = tensor([  7.0545, -14.4015], grad_fn=<SubBackward0>), Loss = 0.1444265991449356\n",
      "tensor([-0.0025,  0.0050])\n",
      "loss = 0.14439566433429718\n",
      "Iteration 990: b = tensor([  7.0570, -14.4064], grad_fn=<SubBackward0>), Loss = 0.14439566433429718\n",
      "tensor([-0.0025,  0.0050])\n",
      "loss = 0.14436477422714233\n",
      "Iteration 991: b = tensor([  7.0595, -14.4114], grad_fn=<SubBackward0>), Loss = 0.14436477422714233\n",
      "tensor([-0.0025,  0.0050])\n",
      "loss = 0.14433397352695465\n",
      "Iteration 992: b = tensor([  7.0620, -14.4163], grad_fn=<SubBackward0>), Loss = 0.14433397352695465\n",
      "tensor([-0.0025,  0.0050])\n",
      "loss = 0.14430320262908936\n",
      "Iteration 993: b = tensor([  7.0645, -14.4213], grad_fn=<SubBackward0>), Loss = 0.14430320262908936\n",
      "tensor([-0.0025,  0.0049])\n",
      "loss = 0.14427247643470764\n",
      "Iteration 994: b = tensor([  7.0670, -14.4262], grad_fn=<SubBackward0>), Loss = 0.14427247643470764\n",
      "tensor([-0.0025,  0.0049])\n",
      "loss = 0.1442418098449707\n",
      "Iteration 995: b = tensor([  7.0695, -14.4312], grad_fn=<SubBackward0>), Loss = 0.1442418098449707\n",
      "tensor([-0.0025,  0.0049])\n",
      "loss = 0.14421120285987854\n",
      "Iteration 996: b = tensor([  7.0720, -14.4361], grad_fn=<SubBackward0>), Loss = 0.14421120285987854\n",
      "tensor([-0.0025,  0.0049])\n",
      "loss = 0.14418064057826996\n",
      "Iteration 997: b = tensor([  7.0745, -14.4410], grad_fn=<SubBackward0>), Loss = 0.14418064057826996\n",
      "tensor([-0.0025,  0.0049])\n",
      "loss = 0.14415013790130615\n",
      "Iteration 998: b = tensor([  7.0770, -14.4460], grad_fn=<SubBackward0>), Loss = 0.14415013790130615\n",
      "tensor([-0.0025,  0.0049])\n",
      "loss = 0.14411965012550354\n",
      "Iteration 999: b = tensor([  7.0795, -14.4509], grad_fn=<SubBackward0>), Loss = 0.14411965012550354\n",
      "tensor([-0.0025,  0.0049])\n",
      "loss = 0.14408926665782928\n",
      "Iteration 1000: b = tensor([  7.0820, -14.4558], grad_fn=<SubBackward0>), Loss = 0.14408926665782928\n",
      "tensor([-0.0025,  0.0049])\n",
      "Optimal b: -14.455801963806152 Optimal intercept: 7.081967830657959\n"
     ]
    }
   ],
   "source": [
    "# Helper function for finding the optimal b\n",
    "def grad_descent(y, b, lr=1):  # Learning rate for Log. Reg. needed to be larger than linear regression\n",
    "    grad = torch.autograd.grad(y, b)[0]\n",
    "\n",
    "    return b - lr * grad, grad\n",
    "\n",
    "# Helper function for representing the objective function\n",
    "def obj_fun(b):\n",
    "    \n",
    "    # h is the probability parameter here\n",
    "    h = 1/(1 + torch.exp((b[1]*X_train + b[0])))\n",
    "\n",
    "    # We want to leverage h in the appropriate loss function for log. reg.\n",
    "    return -1*torch.sum(y_train*torch.log(h) + (1-y_train)*torch.log(1 - h))/len(X_train)\n",
    "\n",
    "# Initial guess for b\n",
    "b = torch.tensor([1.0, 1.0], requires_grad=True)\n",
    "\n",
    "# Optimization loop\n",
    "for i in range(1000):  # Increased the number of iterations for better convergence\n",
    "    loss = obj_fun(b)\n",
    "    print(F\"loss = {loss}\")\n",
    "    b, grad = grad_descent(loss, b)\n",
    "    print(f\"Iteration {i+1}: b = {b}, Loss = {loss.item()}\")\n",
    "    print(grad)\n",
    "\n",
    "# Final result\n",
    "print(\"Optimal b:\", b[1].item(), \"Optimal intercept:\", b[0].item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the regression function\n",
    "\n",
    "def regression_line(b, x_values):\n",
    "\n",
    "    y_pred = []\n",
    "\n",
    "    for x in x_values:\n",
    "        y_pred.append(1/(1 + torch.exp(float(b[0]) + float(b[1])*x)))\n",
    "\n",
    "    return y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Call the function and get y_pred\n",
    "\n",
    "y_pred  = regression_line(b, X_train)\n",
    "\n",
    "plt.plot(X_train, y_train, \"o\", color=\"blue\")\n",
    "plt.plot(X_train, y_pred, \"o\", color=\"red\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Call the function and get y_pred\n",
    "\n",
    "y_pred  = regression_line(b, X_test)\n",
    "\n",
    "plt.plot(X_test, y_test, \"o\", color=\"blue\")\n",
    "plt.plot(X_test, y_pred, \"o\", color=\"red\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lets calculate the mean squared error\n",
    "\n",
    "def MSE(y_pred, y_true):\n",
    "\n",
    "    residuals = 0\n",
    "\n",
    "    for i in range(len(y_pred)):\n",
    "        residuals += (y_pred[i] - y_true[i])**2\n",
    "\n",
    "    return residuals/len(y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.0056)"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "MSE(y_pred, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x2afbb83d430>]"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(y_test, y_pred, \"o\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
