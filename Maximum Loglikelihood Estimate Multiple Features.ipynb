{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import random as rand\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "x1_points = []\n",
    "x2_points = []\n",
    "y_points = []\n",
    "\n",
    "for i in range(100):\n",
    "    x1_points.append(float(i))\n",
    "    x2_points.append(float(i))\n",
    "    y_points.append(rand.uniform(x1_points[-1], x2_points[-1]*x1_points[-1]))\n",
    "\n",
    "\n",
    "plt.plot(x1_points, y_points, \"o\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lets create a new joint x_points list\n",
    "\n",
    "x_points = []\n",
    "\n",
    "for i in range(len(x1_points)):\n",
    "    x_points.append([x1_points[i], x2_points[i]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data into training and validation sets\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(x_points, y_points, test_size=0.3, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now that we've generated some points at random, its time to use torch \n",
    "# to execute linear regression\n",
    "\n",
    "X_train = torch.tensor(X_train)\n",
    "y_train = torch.tensor(y_train)\n",
    "\n",
    "X_test = torch.tensor(X_test)\n",
    "y_test = torch.tensor(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1: b = tensor([1.0000, 0.9998, 0.9860], grad_fn=<SubBackward0>), Loss = 427385408.0\n",
      "tensor([2.4228e+05, 1.7773e+07, 1.3952e+09])\n",
      "Iteration 2: b = tensor([1.0000, 0.9996, 0.9725], grad_fn=<SubBackward0>), Loss = 408197888.0\n",
      "tensor([2.3562e+05, 1.7271e+07, 1.3549e+09])\n",
      "Iteration 3: b = tensor([1.0000, 0.9995, 0.9593], grad_fn=<SubBackward0>), Loss = 390102976.0\n",
      "tensor([2.2916e+05, 1.6784e+07, 1.3157e+09])\n",
      "Iteration 4: b = tensor([1.0000, 0.9993, 0.9466], grad_fn=<SubBackward0>), Loss = 373038496.0\n",
      "tensor([2.2288e+05, 1.6311e+07, 1.2777e+09])\n",
      "Iteration 5: b = tensor([1.0000, 0.9992, 0.9342], grad_fn=<SubBackward0>), Loss = 356945728.0\n",
      "tensor([2.1679e+05, 1.5852e+07, 1.2408e+09])\n",
      "Iteration 6: b = tensor([1.0000, 0.9990, 0.9221], grad_fn=<SubBackward0>), Loss = 341769280.0\n",
      "tensor([2.1087e+05, 1.5406e+07, 1.2050e+09])\n",
      "Iteration 7: b = tensor([1.0000, 0.9989, 0.9104], grad_fn=<SubBackward0>), Loss = 327457120.0\n",
      "tensor([2.0512e+05, 1.4973e+07, 1.1702e+09])\n",
      "Iteration 8: b = tensor([1.0000, 0.9987, 0.8990], grad_fn=<SubBackward0>), Loss = 313959968.0\n",
      "tensor([1.9954e+05, 1.4552e+07, 1.1364e+09])\n",
      "Iteration 9: b = tensor([1.0000, 0.9986, 0.8880], grad_fn=<SubBackward0>), Loss = 301231424.0\n",
      "tensor([1.9412e+05, 1.4143e+07, 1.1035e+09])\n",
      "Iteration 10: b = tensor([1.0000, 0.9984, 0.8773], grad_fn=<SubBackward0>), Loss = 289227616.0\n",
      "tensor([1.8886e+05, 1.3747e+07, 1.0716e+09])\n",
      "Iteration 11: b = tensor([1.0000, 0.9983, 0.8669], grad_fn=<SubBackward0>), Loss = 277907424.0\n",
      "tensor([1.8375e+05, 1.3361e+07, 1.0407e+09])\n",
      "Iteration 12: b = tensor([1.0000, 0.9982, 0.8568], grad_fn=<SubBackward0>), Loss = 267231888.0\n",
      "tensor([1.7878e+05, 1.2987e+07, 1.0106e+09])\n",
      "Iteration 13: b = tensor([1.0000, 0.9980, 0.8470], grad_fn=<SubBackward0>), Loss = 257164224.0\n",
      "tensor([1.7396e+05, 1.2624e+07, 9.8142e+08])\n",
      "Iteration 14: b = tensor([1.0000, 0.9979, 0.8374], grad_fn=<SubBackward0>), Loss = 247669904.0\n",
      "tensor([1.6928e+05, 1.2271e+07, 9.5307e+08])\n",
      "Iteration 15: b = tensor([1.0000, 0.9978, 0.8282], grad_fn=<SubBackward0>), Loss = 238716176.0\n",
      "tensor([1.6473e+05, 1.1929e+07, 9.2554e+08])\n",
      "Iteration 16: b = tensor([1.0000, 0.9977, 0.8192], grad_fn=<SubBackward0>), Loss = 230272304.0\n",
      "tensor([1.6032e+05, 1.1596e+07, 8.9880e+08])\n",
      "Iteration 17: b = tensor([1.0000, 0.9976, 0.8105], grad_fn=<SubBackward0>), Loss = 222309360.0\n",
      "tensor([1.5603e+05, 1.1273e+07, 8.7283e+08])\n",
      "Iteration 18: b = tensor([1.0000, 0.9975, 0.8020], grad_fn=<SubBackward0>), Loss = 214799776.0\n",
      "tensor([1.5187e+05, 1.0959e+07, 8.4762e+08])\n",
      "Iteration 19: b = tensor([1.0000, 0.9974, 0.7938], grad_fn=<SubBackward0>), Loss = 207717856.0\n",
      "tensor([1.4783e+05, 1.0654e+07, 8.2313e+08])\n",
      "Iteration 20: b = tensor([1.0000, 0.9973, 0.7858], grad_fn=<SubBackward0>), Loss = 201039232.0\n",
      "tensor([1.4390e+05, 1.0358e+07, 7.9935e+08])\n",
      "Iteration 21: b = tensor([1.0000, 0.9972, 0.7780], grad_fn=<SubBackward0>), Loss = 194740896.0\n",
      "tensor([1.4009e+05, 1.0071e+07, 7.7626e+08])\n",
      "Iteration 22: b = tensor([1.0000, 0.9971, 0.7705], grad_fn=<SubBackward0>), Loss = 188801216.0\n",
      "tensor([1.3639e+05, 9.7919e+06, 7.5383e+08])\n",
      "Iteration 23: b = tensor([1.0000, 0.9970, 0.7631], grad_fn=<SubBackward0>), Loss = 183199760.0\n",
      "tensor([1.3279e+05, 9.5209e+06, 7.3205e+08])\n",
      "Iteration 24: b = tensor([1.0000, 0.9969, 0.7560], grad_fn=<SubBackward0>), Loss = 177917264.0\n",
      "tensor([1.2930e+05, 9.2578e+06, 7.1090e+08])\n",
      "Iteration 25: b = tensor([1.0000, 0.9968, 0.7491], grad_fn=<SubBackward0>), Loss = 172935584.0\n",
      "tensor([1.2591e+05, 9.0022e+06, 6.9036e+08])\n",
      "Iteration 26: b = tensor([1.0000, 0.9967, 0.7424], grad_fn=<SubBackward0>), Loss = 168237616.0\n",
      "tensor([1.2261e+05, 8.7540e+06, 6.7042e+08])\n",
      "Iteration 27: b = tensor([1.0000, 0.9966, 0.7359], grad_fn=<SubBackward0>), Loss = 163807120.0\n",
      "tensor([1.1942e+05, 8.5130e+06, 6.5105e+08])\n",
      "Iteration 28: b = tensor([1.0000, 0.9965, 0.7296], grad_fn=<SubBackward0>), Loss = 159628960.0\n",
      "tensor([1.1631e+05, 8.2790e+06, 6.3224e+08])\n",
      "Iteration 29: b = tensor([1.0000, 0.9964, 0.7234], grad_fn=<SubBackward0>), Loss = 155688720.0\n",
      "tensor([1.1330e+05, 8.0517e+06, 6.1398e+08])\n",
      "Iteration 30: b = tensor([0.9999, 0.9964, 0.7175], grad_fn=<SubBackward0>), Loss = 151972864.0\n",
      "tensor([1.1037e+05, 7.8310e+06, 5.9624e+08])\n",
      "Iteration 31: b = tensor([0.9999, 0.9963, 0.7117], grad_fn=<SubBackward0>), Loss = 148468608.0\n",
      "tensor([1.0752e+05, 7.6166e+06, 5.7901e+08])\n",
      "Iteration 32: b = tensor([0.9999, 0.9962, 0.7061], grad_fn=<SubBackward0>), Loss = 145163856.0\n",
      "tensor([1.0476e+05, 7.4085e+06, 5.6229e+08])\n",
      "Iteration 33: b = tensor([0.9999, 0.9961, 0.7006], grad_fn=<SubBackward0>), Loss = 142047328.0\n",
      "tensor([1.0208e+05, 7.2063e+06, 5.4604e+08])\n",
      "Iteration 34: b = tensor([0.9999, 0.9961, 0.6953], grad_fn=<SubBackward0>), Loss = 139108256.0\n",
      "tensor([9.9475e+04, 7.0100e+06, 5.3027e+08])\n",
      "Iteration 35: b = tensor([0.9999, 0.9960, 0.6902], grad_fn=<SubBackward0>), Loss = 136336560.0\n",
      "tensor([9.6946e+04, 6.8194e+06, 5.1495e+08])\n",
      "Iteration 36: b = tensor([0.9999, 0.9959, 0.6852], grad_fn=<SubBackward0>), Loss = 133722696.0\n",
      "tensor([9.4490e+04, 6.6343e+06, 5.0007e+08])\n",
      "Iteration 37: b = tensor([0.9999, 0.9959, 0.6803], grad_fn=<SubBackward0>), Loss = 131257672.0\n",
      "tensor([9.2104e+04, 6.4545e+06, 4.8562e+08])\n",
      "Iteration 38: b = tensor([0.9999, 0.9958, 0.6756], grad_fn=<SubBackward0>), Loss = 128933024.0\n",
      "tensor([8.9788e+04, 6.2799e+06, 4.7159e+08])\n",
      "Iteration 39: b = tensor([0.9999, 0.9957, 0.6710], grad_fn=<SubBackward0>), Loss = 126740760.0\n",
      "tensor([8.7539e+04, 6.1104e+06, 4.5797e+08])\n",
      "Iteration 40: b = tensor([0.9999, 0.9957, 0.6666], grad_fn=<SubBackward0>), Loss = 124673312.0\n",
      "tensor([8.5354e+04, 5.9457e+06, 4.4474e+08])\n",
      "Iteration 41: b = tensor([0.9999, 0.9956, 0.6622], grad_fn=<SubBackward0>), Loss = 122723600.0\n",
      "tensor([8.3233e+04, 5.7859e+06, 4.3189e+08])\n",
      "Iteration 42: b = tensor([0.9999, 0.9956, 0.6580], grad_fn=<SubBackward0>), Loss = 120884920.0\n",
      "tensor([8.1173e+04, 5.6306e+06, 4.1941e+08])\n",
      "Iteration 43: b = tensor([0.9999, 0.9955, 0.6540], grad_fn=<SubBackward0>), Loss = 119150936.0\n",
      "tensor([7.9172e+04, 5.4798e+06, 4.0730e+08])\n",
      "Iteration 44: b = tensor([0.9999, 0.9955, 0.6500], grad_fn=<SubBackward0>), Loss = 117515696.0\n",
      "tensor([7.7230e+04, 5.3334e+06, 3.9553e+08])\n",
      "Iteration 45: b = tensor([0.9999, 0.9954, 0.6462], grad_fn=<SubBackward0>), Loss = 115973584.0\n",
      "tensor([7.5343e+04, 5.1912e+06, 3.8410e+08])\n",
      "Iteration 46: b = tensor([0.9999, 0.9954, 0.6424], grad_fn=<SubBackward0>), Loss = 114519280.0\n",
      "tensor([7.3511e+04, 5.0531e+06, 3.7301e+08])\n",
      "Iteration 47: b = tensor([0.9999, 0.9953, 0.6388], grad_fn=<SubBackward0>), Loss = 113147760.0\n",
      "tensor([7.1732e+04, 4.9190e+06, 3.6223e+08])\n",
      "Iteration 48: b = tensor([0.9999, 0.9953, 0.6353], grad_fn=<SubBackward0>), Loss = 111854392.0\n",
      "tensor([7.0004e+04, 4.7888e+06, 3.5177e+08])\n",
      "Iteration 49: b = tensor([0.9999, 0.9952, 0.6319], grad_fn=<SubBackward0>), Loss = 110634656.0\n",
      "tensor([6.8326e+04, 4.6623e+06, 3.4160e+08])\n",
      "Iteration 50: b = tensor([0.9999, 0.9952, 0.6286], grad_fn=<SubBackward0>), Loss = 109484384.0\n",
      "tensor([6.6697e+04, 4.5395e+06, 3.3173e+08])\n",
      "Iteration 51: b = tensor([0.9999, 0.9951, 0.6254], grad_fn=<SubBackward0>), Loss = 108399592.0\n",
      "tensor([6.5115e+04, 4.4203e+06, 3.2215e+08])\n",
      "Iteration 52: b = tensor([0.9999, 0.9951, 0.6222], grad_fn=<SubBackward0>), Loss = 107376600.0\n",
      "tensor([6.3578e+04, 4.3045e+06, 3.1284e+08])\n",
      "Iteration 53: b = tensor([0.9999, 0.9950, 0.6192], grad_fn=<SubBackward0>), Loss = 106411824.0\n",
      "tensor([6.2086e+04, 4.1920e+06, 3.0381e+08])\n",
      "Iteration 54: b = tensor([0.9999, 0.9950, 0.6162], grad_fn=<SubBackward0>), Loss = 105502008.0\n",
      "tensor([6.0637e+04, 4.0828e+06, 2.9503e+08])\n",
      "Iteration 55: b = tensor([0.9999, 0.9950, 0.6134], grad_fn=<SubBackward0>), Loss = 104644000.0\n",
      "tensor([5.9229e+04, 3.9767e+06, 2.8650e+08])\n",
      "Iteration 56: b = tensor([0.9999, 0.9949, 0.6106], grad_fn=<SubBackward0>), Loss = 103834840.0\n",
      "tensor([5.7863e+04, 3.8737e+06, 2.7823e+08])\n",
      "Iteration 57: b = tensor([0.9999, 0.9949, 0.6079], grad_fn=<SubBackward0>), Loss = 103071776.0\n",
      "tensor([5.6536e+04, 3.7737e+06, 2.7019e+08])\n",
      "Iteration 58: b = tensor([0.9999, 0.9948, 0.6053], grad_fn=<SubBackward0>), Loss = 102352152.0\n",
      "tensor([5.5247e+04, 3.6766e+06, 2.6238e+08])\n",
      "Iteration 59: b = tensor([0.9999, 0.9948, 0.6027], grad_fn=<SubBackward0>), Loss = 101673504.0\n",
      "tensor([5.3995e+04, 3.5822e+06, 2.5480e+08])\n",
      "Iteration 60: b = tensor([0.9999, 0.9948, 0.6002], grad_fn=<SubBackward0>), Loss = 101033504.0\n",
      "tensor([5.2780e+04, 3.4906e+06, 2.4744e+08])\n",
      "Iteration 61: b = tensor([0.9999, 0.9947, 0.5978], grad_fn=<SubBackward0>), Loss = 100429968.0\n",
      "tensor([5.1600e+04, 3.4017e+06, 2.4029e+08])\n",
      "Iteration 62: b = tensor([0.9999, 0.9947, 0.5955], grad_fn=<SubBackward0>), Loss = 99860776.0\n",
      "tensor([5.0454e+04, 3.3153e+06, 2.3335e+08])\n",
      "Iteration 63: b = tensor([0.9999, 0.9947, 0.5932], grad_fn=<SubBackward0>), Loss = 99323992.0\n",
      "tensor([4.9341e+04, 3.2314e+06, 2.2661e+08])\n",
      "Iteration 64: b = tensor([0.9999, 0.9946, 0.5910], grad_fn=<SubBackward0>), Loss = 98817792.0\n",
      "tensor([4.8260e+04, 3.1499e+06, 2.2006e+08])\n",
      "Iteration 65: b = tensor([0.9999, 0.9946, 0.5889], grad_fn=<SubBackward0>), Loss = 98340416.0\n",
      "tensor([4.7210e+04, 3.0708e+06, 2.1370e+08])\n",
      "Iteration 66: b = tensor([0.9999, 0.9946, 0.5868], grad_fn=<SubBackward0>), Loss = 97890216.0\n",
      "tensor([4.6191e+04, 2.9940e+06, 2.0753e+08])\n",
      "Iteration 67: b = tensor([0.9999, 0.9946, 0.5848], grad_fn=<SubBackward0>), Loss = 97465656.0\n",
      "tensor([4.5201e+04, 2.9194e+06, 2.0154e+08])\n",
      "Iteration 68: b = tensor([0.9999, 0.9945, 0.5829], grad_fn=<SubBackward0>), Loss = 97065288.0\n",
      "tensor([4.4239e+04, 2.8470e+06, 1.9571e+08])\n",
      "Iteration 69: b = tensor([0.9999, 0.9945, 0.5810], grad_fn=<SubBackward0>), Loss = 96687696.0\n",
      "tensor([4.3306e+04, 2.7766e+06, 1.9006e+08])\n",
      "Iteration 70: b = tensor([0.9999, 0.9945, 0.5791], grad_fn=<SubBackward0>), Loss = 96331616.0\n",
      "tensor([4.2399e+04, 2.7083e+06, 1.8457e+08])\n",
      "Iteration 71: b = tensor([0.9999, 0.9944, 0.5773], grad_fn=<SubBackward0>), Loss = 95995816.0\n",
      "tensor([4.1519e+04, 2.6419e+06, 1.7924e+08])\n",
      "Iteration 72: b = tensor([0.9999, 0.9944, 0.5756], grad_fn=<SubBackward0>), Loss = 95679128.0\n",
      "tensor([4.0664e+04, 2.5775e+06, 1.7406e+08])\n",
      "Iteration 73: b = tensor([0.9999, 0.9944, 0.5739], grad_fn=<SubBackward0>), Loss = 95380472.0\n",
      "tensor([3.9834e+04, 2.5149e+06, 1.6903e+08])\n",
      "Iteration 74: b = tensor([0.9999, 0.9944, 0.5722], grad_fn=<SubBackward0>), Loss = 95098840.0\n",
      "tensor([3.9028e+04, 2.4541e+06, 1.6415e+08])\n",
      "Iteration 75: b = tensor([0.9999, 0.9943, 0.5706], grad_fn=<SubBackward0>), Loss = 94833248.0\n",
      "tensor([3.8245e+04, 2.3951e+06, 1.5940e+08])\n",
      "Iteration 76: b = tensor([0.9999, 0.9943, 0.5691], grad_fn=<SubBackward0>), Loss = 94582752.0\n",
      "tensor([3.7484e+04, 2.3378e+06, 1.5480e+08])\n",
      "Iteration 77: b = tensor([0.9999, 0.9943, 0.5676], grad_fn=<SubBackward0>), Loss = 94346544.0\n",
      "tensor([3.6746e+04, 2.2822e+06, 1.5033e+08])\n",
      "Iteration 78: b = tensor([0.9999, 0.9943, 0.5661], grad_fn=<SubBackward0>), Loss = 94123768.0\n",
      "tensor([3.6029e+04, 2.2281e+06, 1.4598e+08])\n",
      "Iteration 79: b = tensor([0.9999, 0.9943, 0.5647], grad_fn=<SubBackward0>), Loss = 93913688.0\n",
      "tensor([3.5333e+04, 2.1756e+06, 1.4177e+08])\n",
      "Iteration 80: b = tensor([0.9999, 0.9942, 0.5633], grad_fn=<SubBackward0>), Loss = 93715584.0\n",
      "tensor([3.4656e+04, 2.1247e+06, 1.3767e+08])\n",
      "Iteration 81: b = tensor([0.9999, 0.9942, 0.5620], grad_fn=<SubBackward0>), Loss = 93528744.0\n",
      "tensor([3.4000e+04, 2.0752e+06, 1.3369e+08])\n",
      "Iteration 82: b = tensor([0.9999, 0.9942, 0.5607], grad_fn=<SubBackward0>), Loss = 93352552.0\n",
      "tensor([3.3362e+04, 2.0271e+06, 1.2983e+08])\n",
      "Iteration 83: b = tensor([0.9999, 0.9942, 0.5594], grad_fn=<SubBackward0>), Loss = 93186376.0\n",
      "tensor([3.2743e+04, 1.9805e+06, 1.2608e+08])\n",
      "Iteration 84: b = tensor([0.9999, 0.9942, 0.5582], grad_fn=<SubBackward0>), Loss = 93029680.0\n",
      "tensor([3.2141e+04, 1.9351e+06, 1.2244e+08])\n",
      "Iteration 85: b = tensor([0.9999, 0.9941, 0.5570], grad_fn=<SubBackward0>), Loss = 92881904.0\n",
      "tensor([3.1557e+04, 1.8911e+06, 1.1890e+08])\n",
      "Iteration 86: b = tensor([0.9999, 0.9941, 0.5559], grad_fn=<SubBackward0>), Loss = 92742544.0\n",
      "tensor([3.0990e+04, 1.8484e+06, 1.1546e+08])\n",
      "Iteration 87: b = tensor([0.9999, 0.9941, 0.5548], grad_fn=<SubBackward0>), Loss = 92611120.0\n",
      "tensor([3.0439e+04, 1.8069e+06, 1.1213e+08])\n",
      "Iteration 88: b = tensor([0.9999, 0.9941, 0.5537], grad_fn=<SubBackward0>), Loss = 92487160.0\n",
      "tensor([2.9905e+04, 1.7665e+06, 1.0889e+08])\n",
      "Iteration 89: b = tensor([0.9999, 0.9941, 0.5526], grad_fn=<SubBackward0>), Loss = 92370288.0\n",
      "tensor([2.9385e+04, 1.7274e+06, 1.0574e+08])\n",
      "Iteration 90: b = tensor([0.9999, 0.9940, 0.5516], grad_fn=<SubBackward0>), Loss = 92260056.0\n",
      "tensor([2.8881e+04, 1.6894e+06, 1.0269e+08])\n",
      "Iteration 91: b = tensor([0.9999, 0.9940, 0.5506], grad_fn=<SubBackward0>), Loss = 92156096.0\n",
      "tensor([2.8391e+04, 1.6525e+06, 9.9721e+07])\n",
      "Iteration 92: b = tensor([0.9999, 0.9940, 0.5496], grad_fn=<SubBackward0>), Loss = 92058064.0\n",
      "tensor([2.7915e+04, 1.6166e+06, 9.6840e+07])\n",
      "Iteration 93: b = tensor([0.9999, 0.9940, 0.5487], grad_fn=<SubBackward0>), Loss = 91965616.0\n",
      "tensor([2.7453e+04, 1.5818e+06, 9.4042e+07])\n",
      "Iteration 94: b = tensor([0.9999, 0.9940, 0.5478], grad_fn=<SubBackward0>), Loss = 91878432.0\n",
      "tensor([2.7005e+04, 1.5480e+06, 9.1325e+07])\n",
      "Iteration 95: b = tensor([0.9999, 0.9940, 0.5469], grad_fn=<SubBackward0>), Loss = 91796216.0\n",
      "tensor([2.6569e+04, 1.5152e+06, 8.8687e+07])\n",
      "Iteration 96: b = tensor([0.9999, 0.9940, 0.5460], grad_fn=<SubBackward0>), Loss = 91718664.0\n",
      "tensor([2.6146e+04, 1.4833e+06, 8.6124e+07])\n",
      "Iteration 97: b = tensor([0.9999, 0.9939, 0.5452], grad_fn=<SubBackward0>), Loss = 91645544.0\n",
      "tensor([2.5735e+04, 1.4523e+06, 8.3636e+07])\n",
      "Iteration 98: b = tensor([0.9999, 0.9939, 0.5444], grad_fn=<SubBackward0>), Loss = 91576584.0\n",
      "tensor([2.5336e+04, 1.4222e+06, 8.1220e+07])\n",
      "Iteration 99: b = tensor([0.9999, 0.9939, 0.5436], grad_fn=<SubBackward0>), Loss = 91511560.0\n",
      "tensor([2.4949e+04, 1.3930e+06, 7.8873e+07])\n",
      "Iteration 100: b = tensor([0.9999, 0.9939, 0.5428], grad_fn=<SubBackward0>), Loss = 91450224.0\n",
      "tensor([2.4573e+04, 1.3647e+06, 7.6594e+07])\n",
      "Iteration 101: b = tensor([0.9999, 0.9939, 0.5421], grad_fn=<SubBackward0>), Loss = 91392392.0\n",
      "tensor([2.4207e+04, 1.3371e+06, 7.4382e+07])\n",
      "Iteration 102: b = tensor([0.9999, 0.9939, 0.5413], grad_fn=<SubBackward0>), Loss = 91337840.0\n",
      "tensor([2.3853e+04, 1.3104e+06, 7.2233e+07])\n",
      "Iteration 103: b = tensor([0.9999, 0.9939, 0.5406], grad_fn=<SubBackward0>), Loss = 91286408.0\n",
      "tensor([2.3508e+04, 1.2844e+06, 7.0146e+07])\n",
      "Iteration 104: b = tensor([0.9999, 0.9938, 0.5400], grad_fn=<SubBackward0>), Loss = 91237896.0\n",
      "tensor([2.3173e+04, 1.2592e+06, 6.8119e+07])\n",
      "Iteration 105: b = tensor([0.9999, 0.9938, 0.5393], grad_fn=<SubBackward0>), Loss = 91192160.0\n",
      "tensor([2.2848e+04, 1.2347e+06, 6.6151e+07])\n",
      "Iteration 106: b = tensor([0.9999, 0.9938, 0.5387], grad_fn=<SubBackward0>), Loss = 91149008.0\n",
      "tensor([2.2533e+04, 1.2109e+06, 6.4240e+07])\n",
      "Iteration 107: b = tensor([0.9999, 0.9938, 0.5380], grad_fn=<SubBackward0>), Loss = 91108320.0\n",
      "tensor([2.2226e+04, 1.1879e+06, 6.2384e+07])\n",
      "Iteration 108: b = tensor([0.9999, 0.9938, 0.5374], grad_fn=<SubBackward0>), Loss = 91069960.0\n",
      "tensor([2.1929e+04, 1.1654e+06, 6.0581e+07])\n",
      "Iteration 109: b = tensor([0.9999, 0.9938, 0.5368], grad_fn=<SubBackward0>), Loss = 91033768.0\n",
      "tensor([2.1640e+04, 1.1436e+06, 5.8831e+07])\n",
      "Iteration 110: b = tensor([0.9999, 0.9938, 0.5363], grad_fn=<SubBackward0>), Loss = 90999640.0\n",
      "tensor([2.1359e+04, 1.1225e+06, 5.7131e+07])\n",
      "Iteration 111: b = tensor([0.9999, 0.9938, 0.5357], grad_fn=<SubBackward0>), Loss = 90967464.0\n",
      "tensor([2.1087e+04, 1.1020e+06, 5.5481e+07])\n",
      "Iteration 112: b = tensor([0.9999, 0.9938, 0.5352], grad_fn=<SubBackward0>), Loss = 90937120.0\n",
      "tensor([2.0822e+04, 1.0820e+06, 5.3878e+07])\n",
      "Iteration 113: b = tensor([0.9999, 0.9937, 0.5347], grad_fn=<SubBackward0>), Loss = 90908496.0\n",
      "tensor([2.0565e+04, 1.0626e+06, 5.2321e+07])\n",
      "Iteration 114: b = tensor([0.9999, 0.9937, 0.5341], grad_fn=<SubBackward0>), Loss = 90881496.0\n",
      "tensor([2.0315e+04, 1.0438e+06, 5.0810e+07])\n",
      "Iteration 115: b = tensor([0.9999, 0.9937, 0.5337], grad_fn=<SubBackward0>), Loss = 90856048.0\n",
      "tensor([2.0073e+04, 1.0256e+06, 4.9342e+07])\n",
      "Iteration 116: b = tensor([0.9999, 0.9937, 0.5332], grad_fn=<SubBackward0>), Loss = 90832048.0\n",
      "tensor([1.9838e+04, 1.0078e+06, 4.7916e+07])\n",
      "Iteration 117: b = tensor([0.9999, 0.9937, 0.5327], grad_fn=<SubBackward0>), Loss = 90809408.0\n",
      "tensor([1.9609e+04, 9.9059e+05, 4.6532e+07])\n",
      "Iteration 118: b = tensor([0.9999, 0.9937, 0.5323], grad_fn=<SubBackward0>), Loss = 90788064.0\n",
      "tensor([1.9387e+04, 9.7386e+05, 4.5187e+07])\n",
      "Iteration 119: b = tensor([0.9999, 0.9937, 0.5318], grad_fn=<SubBackward0>), Loss = 90767936.0\n",
      "tensor([1.9172e+04, 9.5762e+05, 4.3881e+07])\n",
      "Iteration 120: b = tensor([0.9999, 0.9937, 0.5314], grad_fn=<SubBackward0>), Loss = 90748936.0\n",
      "tensor([1.8962e+04, 9.4184e+05, 4.2614e+07])\n",
      "Iteration 121: b = tensor([0.9999, 0.9937, 0.5310], grad_fn=<SubBackward0>), Loss = 90731040.0\n",
      "tensor([1.8759e+04, 9.2652e+05, 4.1382e+07])\n",
      "Iteration 122: b = tensor([0.9999, 0.9937, 0.5306], grad_fn=<SubBackward0>), Loss = 90714144.0\n",
      "tensor([1.8562e+04, 9.1164e+05, 4.0187e+07])\n",
      "Iteration 123: b = tensor([0.9999, 0.9936, 0.5302], grad_fn=<SubBackward0>), Loss = 90698224.0\n",
      "tensor([1.8370e+04, 8.9719e+05, 3.9026e+07])\n",
      "Iteration 124: b = tensor([0.9999, 0.9936, 0.5298], grad_fn=<SubBackward0>), Loss = 90683216.0\n",
      "tensor([1.8184e+04, 8.8316e+05, 3.7898e+07])\n",
      "Iteration 125: b = tensor([0.9999, 0.9936, 0.5294], grad_fn=<SubBackward0>), Loss = 90669048.0\n",
      "tensor([1.8003e+04, 8.6954e+05, 3.6803e+07])\n",
      "Iteration 126: b = tensor([0.9999, 0.9936, 0.5291], grad_fn=<SubBackward0>), Loss = 90655696.0\n",
      "tensor([1.7827e+04, 8.5630e+05, 3.5740e+07])\n",
      "Iteration 127: b = tensor([0.9999, 0.9936, 0.5287], grad_fn=<SubBackward0>), Loss = 90643096.0\n",
      "tensor([1.7657e+04, 8.4345e+05, 3.4707e+07])\n",
      "Iteration 128: b = tensor([0.9999, 0.9936, 0.5284], grad_fn=<SubBackward0>), Loss = 90631216.0\n",
      "tensor([1.7491e+04, 8.3097e+05, 3.3704e+07])\n",
      "Iteration 129: b = tensor([0.9999, 0.9936, 0.5281], grad_fn=<SubBackward0>), Loss = 90620008.0\n",
      "tensor([1.7331e+04, 8.1885e+05, 3.2730e+07])\n",
      "Iteration 130: b = tensor([0.9999, 0.9936, 0.5278], grad_fn=<SubBackward0>), Loss = 90609448.0\n",
      "tensor([1.7174e+04, 8.0709e+05, 3.1785e+07])\n",
      "Iteration 131: b = tensor([0.9999, 0.9936, 0.5274], grad_fn=<SubBackward0>), Loss = 90599488.0\n",
      "tensor([1.7023e+04, 7.9566e+05, 3.0866e+07])\n",
      "Iteration 132: b = tensor([0.9999, 0.9936, 0.5271], grad_fn=<SubBackward0>), Loss = 90590096.0\n",
      "tensor([1.6876e+04, 7.8456e+05, 2.9974e+07])\n",
      "Iteration 133: b = tensor([0.9999, 0.9936, 0.5269], grad_fn=<SubBackward0>), Loss = 90581224.0\n",
      "tensor([1.6733e+04, 7.7378e+05, 2.9108e+07])\n",
      "Iteration 134: b = tensor([0.9999, 0.9936, 0.5266], grad_fn=<SubBackward0>), Loss = 90572864.0\n",
      "tensor([1.6594e+04, 7.6332e+05, 2.8267e+07])\n",
      "Iteration 135: b = tensor([0.9999, 0.9935, 0.5263], grad_fn=<SubBackward0>), Loss = 90564992.0\n",
      "tensor([1.6459e+04, 7.5315e+05, 2.7451e+07])\n",
      "Iteration 136: b = tensor([0.9999, 0.9935, 0.5260], grad_fn=<SubBackward0>), Loss = 90557560.0\n",
      "tensor([1.6328e+04, 7.4328e+05, 2.6657e+07])\n",
      "Iteration 137: b = tensor([0.9999, 0.9935, 0.5258], grad_fn=<SubBackward0>), Loss = 90550544.0\n",
      "tensor([1.6201e+04, 7.3370e+05, 2.5887e+07])\n",
      "Iteration 138: b = tensor([0.9999, 0.9935, 0.5255], grad_fn=<SubBackward0>), Loss = 90543936.0\n",
      "tensor([1.6077e+04, 7.2439e+05, 2.5139e+07])\n",
      "Iteration 139: b = tensor([0.9999, 0.9935, 0.5253], grad_fn=<SubBackward0>), Loss = 90537712.0\n",
      "tensor([1.5957e+04, 7.1535e+05, 2.4413e+07])\n",
      "Iteration 140: b = tensor([0.9999, 0.9935, 0.5250], grad_fn=<SubBackward0>), Loss = 90531824.0\n",
      "tensor([1.5841e+04, 7.0657e+05, 2.3707e+07])\n",
      "Iteration 141: b = tensor([0.9999, 0.9935, 0.5248], grad_fn=<SubBackward0>), Loss = 90526280.0\n",
      "tensor([1.5728e+04, 6.9805e+05, 2.3022e+07])\n",
      "Iteration 142: b = tensor([0.9999, 0.9935, 0.5246], grad_fn=<SubBackward0>), Loss = 90521056.0\n",
      "tensor([1.5618e+04, 6.8977e+05, 2.2357e+07])\n",
      "Iteration 143: b = tensor([0.9999, 0.9935, 0.5244], grad_fn=<SubBackward0>), Loss = 90516128.0\n",
      "tensor([1.5511e+04, 6.8173e+05, 2.1711e+07])\n",
      "Iteration 144: b = tensor([0.9999, 0.9935, 0.5242], grad_fn=<SubBackward0>), Loss = 90511472.0\n",
      "tensor([1.5408e+04, 6.7393e+05, 2.1084e+07])\n",
      "Iteration 145: b = tensor([0.9999, 0.9935, 0.5240], grad_fn=<SubBackward0>), Loss = 90507088.0\n",
      "tensor([1.5307e+04, 6.6634e+05, 2.0474e+07])\n",
      "Iteration 146: b = tensor([0.9999, 0.9935, 0.5238], grad_fn=<SubBackward0>), Loss = 90502944.0\n",
      "tensor([1.5209e+04, 6.5898e+05, 1.9883e+07])\n",
      "Iteration 147: b = tensor([0.9999, 0.9935, 0.5236], grad_fn=<SubBackward0>), Loss = 90499040.0\n",
      "tensor([1.5114e+04, 6.5183e+05, 1.9308e+07])\n",
      "Iteration 148: b = tensor([0.9999, 0.9935, 0.5234], grad_fn=<SubBackward0>), Loss = 90495368.0\n",
      "tensor([1.5022e+04, 6.4489e+05, 1.8750e+07])\n",
      "Iteration 149: b = tensor([0.9999, 0.9935, 0.5232], grad_fn=<SubBackward0>), Loss = 90491904.0\n",
      "tensor([1.4933e+04, 6.3815e+05, 1.8208e+07])\n",
      "Iteration 150: b = tensor([0.9999, 0.9934, 0.5230], grad_fn=<SubBackward0>), Loss = 90488640.0\n",
      "tensor([1.4846e+04, 6.3160e+05, 1.7682e+07])\n",
      "Iteration 151: b = tensor([0.9999, 0.9934, 0.5228], grad_fn=<SubBackward0>), Loss = 90485552.0\n",
      "tensor([1.4762e+04, 6.2524e+05, 1.7171e+07])\n",
      "Iteration 152: b = tensor([0.9999, 0.9934, 0.5227], grad_fn=<SubBackward0>), Loss = 90482640.0\n",
      "tensor([1.4680e+04, 6.1906e+05, 1.6675e+07])\n",
      "Iteration 153: b = tensor([0.9999, 0.9934, 0.5225], grad_fn=<SubBackward0>), Loss = 90479888.0\n",
      "tensor([1.4600e+04, 6.1307e+05, 1.6193e+07])\n",
      "Iteration 154: b = tensor([0.9999, 0.9934, 0.5224], grad_fn=<SubBackward0>), Loss = 90477296.0\n",
      "tensor([1.4523e+04, 6.0724e+05, 1.5725e+07])\n",
      "Iteration 155: b = tensor([0.9999, 0.9934, 0.5222], grad_fn=<SubBackward0>), Loss = 90474872.0\n",
      "tensor([1.4448e+04, 6.0159e+05, 1.5271e+07])\n",
      "Iteration 156: b = tensor([0.9999, 0.9934, 0.5221], grad_fn=<SubBackward0>), Loss = 90472568.0\n",
      "tensor([1.4375e+04, 5.9610e+05, 1.4829e+07])\n",
      "Iteration 157: b = tensor([0.9999, 0.9934, 0.5219], grad_fn=<SubBackward0>), Loss = 90470384.0\n",
      "tensor([1.4304e+04, 5.9076e+05, 1.4401e+07])\n",
      "Iteration 158: b = tensor([0.9999, 0.9934, 0.5218], grad_fn=<SubBackward0>), Loss = 90468336.0\n",
      "tensor([   14235.4199,   585585.3125, 13984512.0000])\n",
      "Iteration 159: b = tensor([0.9999, 0.9934, 0.5216], grad_fn=<SubBackward0>), Loss = 90466416.0\n",
      "tensor([   14168.6895,   580556.2500, 13580352.0000])\n",
      "Iteration 160: b = tensor([0.9999, 0.9934, 0.5215], grad_fn=<SubBackward0>), Loss = 90464592.0\n",
      "tensor([   14103.9004,   575673.3125, 13187952.0000])\n",
      "Iteration 161: b = tensor([0.9999, 0.9934, 0.5214], grad_fn=<SubBackward0>), Loss = 90462872.0\n",
      "tensor([   14040.9541,   570929.1250, 12806752.0000])\n",
      "Iteration 162: b = tensor([0.9999, 0.9934, 0.5213], grad_fn=<SubBackward0>), Loss = 90461256.0\n",
      "tensor([   13979.8330,   566322.6250, 12436560.0000])\n",
      "Iteration 163: b = tensor([0.9999, 0.9934, 0.5211], grad_fn=<SubBackward0>), Loss = 90459728.0\n",
      "tensor([   13920.4668,   561848.8750, 12077016.0000])\n",
      "Iteration 164: b = tensor([0.9999, 0.9934, 0.5210], grad_fn=<SubBackward0>), Loss = 90458288.0\n",
      "tensor([   13862.8389,   557505.5000, 11728008.0000])\n",
      "Iteration 165: b = tensor([0.9999, 0.9934, 0.5209], grad_fn=<SubBackward0>), Loss = 90456928.0\n",
      "tensor([   13806.8584,   553286.6875, 11388992.0000])\n",
      "Iteration 166: b = tensor([0.9999, 0.9934, 0.5208], grad_fn=<SubBackward0>), Loss = 90455648.0\n",
      "tensor([   13752.5049,   549190.2500, 11059776.0000])\n",
      "Iteration 167: b = tensor([0.9999, 0.9933, 0.5207], grad_fn=<SubBackward0>), Loss = 90454432.0\n",
      "tensor([   13699.7109,   545211.4375, 10740048.0000])\n",
      "Iteration 168: b = tensor([0.9999, 0.9933, 0.5206], grad_fn=<SubBackward0>), Loss = 90453296.0\n",
      "tensor([   13648.4502,   541347.8125, 10429568.0000])\n",
      "Iteration 169: b = tensor([0.9999, 0.9933, 0.5205], grad_fn=<SubBackward0>), Loss = 90452224.0\n",
      "tensor([   13598.6680,   537596.0000, 10128096.0000])\n",
      "Iteration 170: b = tensor([0.9999, 0.9933, 0.5204], grad_fn=<SubBackward0>), Loss = 90451208.0\n",
      "tensor([  13550.3340,  533953.6875, 9835376.0000])\n",
      "Iteration 171: b = tensor([0.9999, 0.9933, 0.5203], grad_fn=<SubBackward0>), Loss = 90450248.0\n",
      "tensor([  13503.3975,  530416.3750, 9551104.0000])\n",
      "Iteration 172: b = tensor([0.9999, 0.9933, 0.5202], grad_fn=<SubBackward0>), Loss = 90449344.0\n",
      "tensor([  13457.8252,  526981.4375, 9275120.0000])\n",
      "Iteration 173: b = tensor([0.9999, 0.9933, 0.5201], grad_fn=<SubBackward0>), Loss = 90448504.0\n",
      "tensor([  13413.5586,  523645.7500, 9007048.0000])\n",
      "Iteration 174: b = tensor([0.9999, 0.9933, 0.5200], grad_fn=<SubBackward0>), Loss = 90447696.0\n",
      "tensor([  13370.5732,  520406.0000, 8746688.0000])\n",
      "Iteration 175: b = tensor([0.9999, 0.9933, 0.5199], grad_fn=<SubBackward0>), Loss = 90446944.0\n",
      "tensor([  13328.8340,  517260.3125, 8493888.0000])\n",
      "Iteration 176: b = tensor([0.9999, 0.9933, 0.5198], grad_fn=<SubBackward0>), Loss = 90446224.0\n",
      "tensor([  13288.2949,  514205.1875, 8248416.0000])\n",
      "Iteration 177: b = tensor([0.9999, 0.9933, 0.5198], grad_fn=<SubBackward0>), Loss = 90445552.0\n",
      "tensor([  13248.9199,  511237.8750, 8009952.0000])\n",
      "Iteration 178: b = tensor([0.9999, 0.9933, 0.5197], grad_fn=<SubBackward0>), Loss = 90444912.0\n",
      "tensor([  13210.6797,  508355.8125, 7778352.0000])\n",
      "Iteration 179: b = tensor([0.9999, 0.9933, 0.5196], grad_fn=<SubBackward0>), Loss = 90444320.0\n",
      "tensor([  13173.5527,  505558.0000, 7553496.0000])\n",
      "Iteration 180: b = tensor([0.9999, 0.9933, 0.5195], grad_fn=<SubBackward0>), Loss = 90443752.0\n",
      "tensor([  13137.4990,  502840.7500, 7335144.0000])\n",
      "Iteration 181: b = tensor([0.9999, 0.9933, 0.5195], grad_fn=<SubBackward0>), Loss = 90443216.0\n",
      "tensor([  13102.4746,  500201.5000, 7123056.0000])\n",
      "Iteration 182: b = tensor([0.9999, 0.9933, 0.5194], grad_fn=<SubBackward0>), Loss = 90442720.0\n",
      "tensor([  13068.4717,  497638.7500, 6917128.0000])\n",
      "Iteration 183: b = tensor([0.9999, 0.9933, 0.5193], grad_fn=<SubBackward0>), Loss = 90442240.0\n",
      "tensor([  13035.4336,  495149.0625, 6717072.0000])\n",
      "Iteration 184: b = tensor([0.9999, 0.9933, 0.5193], grad_fn=<SubBackward0>), Loss = 90441808.0\n",
      "tensor([  13003.3643,  492732.1875, 6522864.0000])\n",
      "Iteration 185: b = tensor([0.9999, 0.9933, 0.5192], grad_fn=<SubBackward0>), Loss = 90441384.0\n",
      "tensor([  12972.2363,  490386.2500, 6334304.0000])\n",
      "Iteration 186: b = tensor([0.9999, 0.9933, 0.5191], grad_fn=<SubBackward0>), Loss = 90440976.0\n",
      "tensor([  12941.9814,  488106.2500, 6151112.0000])\n",
      "Iteration 187: b = tensor([0.9999, 0.9932, 0.5191], grad_fn=<SubBackward0>), Loss = 90440592.0\n",
      "tensor([  12912.6152,  485893.1875, 5973272.0000])\n",
      "Iteration 188: b = tensor([0.9999, 0.9932, 0.5190], grad_fn=<SubBackward0>), Loss = 90440240.0\n",
      "tensor([  12884.0967,  483744.0625, 5800576.0000])\n",
      "Iteration 189: b = tensor([0.9999, 0.9932, 0.5190], grad_fn=<SubBackward0>), Loss = 90439920.0\n",
      "tensor([  12856.4082,  481657.3750, 5632888.0000])\n",
      "Iteration 190: b = tensor([0.9999, 0.9932, 0.5189], grad_fn=<SubBackward0>), Loss = 90439608.0\n",
      "tensor([  12829.5098,  479630.3750, 5469992.0000])\n",
      "Iteration 191: b = tensor([0.9999, 0.9932, 0.5189], grad_fn=<SubBackward0>), Loss = 90439296.0\n",
      "tensor([  12803.3877,  477661.7500, 5311816.0000])\n",
      "Iteration 192: b = tensor([0.9999, 0.9932, 0.5188], grad_fn=<SubBackward0>), Loss = 90439024.0\n",
      "tensor([  12778.0225,  475750.1250, 5158208.0000])\n",
      "Iteration 193: b = tensor([0.9999, 0.9932, 0.5188], grad_fn=<SubBackward0>), Loss = 90438760.0\n",
      "tensor([  12753.4023,  473895.0000, 5009128.0000])\n",
      "Iteration 194: b = tensor([0.9999, 0.9932, 0.5187], grad_fn=<SubBackward0>), Loss = 90438520.0\n",
      "tensor([  12729.4941,  472093.4375, 4864352.0000])\n",
      "Iteration 195: b = tensor([0.9999, 0.9932, 0.5187], grad_fn=<SubBackward0>), Loss = 90438272.0\n",
      "tensor([  12706.2656,  470342.5000, 4723664.0000])\n",
      "Iteration 196: b = tensor([0.9999, 0.9932, 0.5186], grad_fn=<SubBackward0>), Loss = 90438056.0\n",
      "tensor([  12683.7188,  468643.3750, 4587144.0000])\n",
      "Iteration 197: b = tensor([0.9999, 0.9932, 0.5186], grad_fn=<SubBackward0>), Loss = 90437840.0\n",
      "tensor([  12661.8027,  466992.0000, 4454416.0000])\n",
      "Iteration 198: b = tensor([0.9999, 0.9932, 0.5185], grad_fn=<SubBackward0>), Loss = 90437648.0\n",
      "tensor([  12640.5322,  465389.4375, 4325648.0000])\n",
      "Iteration 199: b = tensor([0.9999, 0.9932, 0.5185], grad_fn=<SubBackward0>), Loss = 90437456.0\n",
      "tensor([  12619.8682,  463831.8750, 4200496.0000])\n",
      "Iteration 200: b = tensor([0.9999, 0.9932, 0.5184], grad_fn=<SubBackward0>), Loss = 90437280.0\n",
      "tensor([  12599.7930,  462319.1250, 4078920.0000])\n",
      "Iteration 201: b = tensor([0.9999, 0.9932, 0.5184], grad_fn=<SubBackward0>), Loss = 90437120.0\n",
      "tensor([  12580.3184,  460851.6250, 3961000.0000])\n",
      "Iteration 202: b = tensor([0.9999, 0.9932, 0.5184], grad_fn=<SubBackward0>), Loss = 90436960.0\n",
      "tensor([  12561.3818,  459424.3125, 3846320.0000])\n",
      "Iteration 203: b = tensor([0.9999, 0.9932, 0.5183], grad_fn=<SubBackward0>), Loss = 90436816.0\n",
      "tensor([  12543.0127,  458040.4375, 3735096.0000])\n",
      "Iteration 204: b = tensor([0.9999, 0.9932, 0.5183], grad_fn=<SubBackward0>), Loss = 90436672.0\n",
      "tensor([  12525.1572,  456694.6250, 3626968.0000])\n",
      "Iteration 205: b = tensor([0.9999, 0.9932, 0.5183], grad_fn=<SubBackward0>), Loss = 90436536.0\n",
      "tensor([  12507.8135,  455387.8750, 3521952.0000])\n",
      "Iteration 206: b = tensor([0.9999, 0.9932, 0.5182], grad_fn=<SubBackward0>), Loss = 90436416.0\n",
      "tensor([  12490.9902,  454120.2500, 3420096.0000])\n",
      "Iteration 207: b = tensor([0.9999, 0.9932, 0.5182], grad_fn=<SubBackward0>), Loss = 90436296.0\n",
      "tensor([  12474.6387,  452888.0625, 3321088.0000])\n",
      "Iteration 208: b = tensor([0.9999, 0.9931, 0.5182], grad_fn=<SubBackward0>), Loss = 90436184.0\n",
      "tensor([  12458.7754,  451692.6250, 3225024.0000])\n",
      "Iteration 209: b = tensor([0.9999, 0.9931, 0.5181], grad_fn=<SubBackward0>), Loss = 90436080.0\n",
      "tensor([  12443.3633,  450531.2500, 3131712.0000])\n",
      "Iteration 210: b = tensor([0.9999, 0.9931, 0.5181], grad_fn=<SubBackward0>), Loss = 90435976.0\n",
      "tensor([  12428.4102,  449404.6250, 3041160.0000])\n",
      "Iteration 211: b = tensor([0.9999, 0.9931, 0.5181], grad_fn=<SubBackward0>), Loss = 90435888.0\n",
      "tensor([  12413.8789,  448309.5625, 2953200.0000])\n",
      "Iteration 212: b = tensor([0.9999, 0.9931, 0.5180], grad_fn=<SubBackward0>), Loss = 90435800.0\n",
      "tensor([  12399.7754,  447247.0625, 2867808.0000])\n",
      "Iteration 213: b = tensor([0.9999, 0.9931, 0.5180], grad_fn=<SubBackward0>), Loss = 90435720.0\n",
      "tensor([  12386.0771,  446214.3750, 2784832.0000])\n",
      "Iteration 214: b = tensor([0.9999, 0.9931, 0.5180], grad_fn=<SubBackward0>), Loss = 90435640.0\n",
      "tensor([  12372.7676,  445211.7500, 2704256.0000])\n",
      "Iteration 215: b = tensor([0.9999, 0.9931, 0.5180], grad_fn=<SubBackward0>), Loss = 90435568.0\n",
      "tensor([  12359.8311,  444237.3125, 2625944.0000])\n",
      "Iteration 216: b = tensor([0.9999, 0.9931, 0.5179], grad_fn=<SubBackward0>), Loss = 90435496.0\n",
      "tensor([  12347.2646,  443290.2500, 2549872.0000])\n",
      "Iteration 217: b = tensor([0.9999, 0.9931, 0.5179], grad_fn=<SubBackward0>), Loss = 90435424.0\n",
      "tensor([  12335.0703,  442371.3750, 2476032.0000])\n",
      "Iteration 218: b = tensor([0.9999, 0.9931, 0.5179], grad_fn=<SubBackward0>), Loss = 90435368.0\n",
      "tensor([  12323.2441,  441480.0000, 2404400.0000])\n",
      "Iteration 219: b = tensor([0.9999, 0.9931, 0.5179], grad_fn=<SubBackward0>), Loss = 90435312.0\n",
      "tensor([  12311.7559,  440614.5000, 2334864.0000])\n",
      "Iteration 220: b = tensor([0.9999, 0.9931, 0.5178], grad_fn=<SubBackward0>), Loss = 90435264.0\n",
      "tensor([  12300.5830,  439772.5625, 2267240.0000])\n",
      "Iteration 221: b = tensor([0.9999, 0.9931, 0.5178], grad_fn=<SubBackward0>), Loss = 90435208.0\n",
      "tensor([  12289.7500,  438956.5000, 2201648.0000])\n",
      "Iteration 222: b = tensor([0.9999, 0.9931, 0.5178], grad_fn=<SubBackward0>), Loss = 90435152.0\n",
      "tensor([  12279.2285,  438163.7500, 2137952.0000])\n",
      "Iteration 223: b = tensor([0.9999, 0.9931, 0.5178], grad_fn=<SubBackward0>), Loss = 90435112.0\n",
      "tensor([  12268.9912,  437392.3750, 2075968.0000])\n",
      "Iteration 224: b = tensor([0.9999, 0.9931, 0.5178], grad_fn=<SubBackward0>), Loss = 90435056.0\n",
      "tensor([  12259.0703,  436644.6875, 2015912.0000])\n",
      "Iteration 225: b = tensor([0.9999, 0.9931, 0.5177], grad_fn=<SubBackward0>), Loss = 90435016.0\n",
      "tensor([  12249.4287,  435918.5000, 1957544.0000])\n",
      "Iteration 226: b = tensor([0.9999, 0.9931, 0.5177], grad_fn=<SubBackward0>), Loss = 90434976.0\n",
      "tensor([  12240.0762,  435213.8750, 1900960.0000])\n",
      "Iteration 227: b = tensor([0.9999, 0.9931, 0.5177], grad_fn=<SubBackward0>), Loss = 90434936.0\n",
      "tensor([  12230.9805,  434528.3750, 1845840.0000])\n",
      "Iteration 228: b = tensor([0.9999, 0.9931, 0.5177], grad_fn=<SubBackward0>), Loss = 90434912.0\n",
      "tensor([  12222.1367,  433862.2500, 1792328.0000])\n",
      "Iteration 229: b = tensor([0.9999, 0.9931, 0.5177], grad_fn=<SubBackward0>), Loss = 90434880.0\n",
      "tensor([  12213.5488,  433215.0625, 1740352.0000])\n",
      "Iteration 230: b = tensor([0.9999, 0.9931, 0.5176], grad_fn=<SubBackward0>), Loss = 90434848.0\n",
      "tensor([  12205.2188,  432587.5000, 1689904.0000])\n",
      "Iteration 231: b = tensor([0.9999, 0.9930, 0.5176], grad_fn=<SubBackward0>), Loss = 90434808.0\n",
      "tensor([  12197.1152,  431977.1250, 1640824.0000])\n",
      "Iteration 232: b = tensor([0.9999, 0.9930, 0.5176], grad_fn=<SubBackward0>), Loss = 90434776.0\n",
      "tensor([  12189.2656,  431385.5625, 1593336.0000])\n",
      "Iteration 233: b = tensor([0.9999, 0.9930, 0.5176], grad_fn=<SubBackward0>), Loss = 90434760.0\n",
      "tensor([  12181.6484,  430811.6875, 1547248.0000])\n",
      "Iteration 234: b = tensor([0.9999, 0.9930, 0.5176], grad_fn=<SubBackward0>), Loss = 90434728.0\n",
      "tensor([  12174.2295,  430252.7500, 1502336.0000])\n",
      "Iteration 235: b = tensor([0.9999, 0.9930, 0.5176], grad_fn=<SubBackward0>), Loss = 90434704.0\n",
      "tensor([  12167.0342,  429710.6250, 1458768.0000])\n",
      "Iteration 236: b = tensor([0.9999, 0.9930, 0.5175], grad_fn=<SubBackward0>), Loss = 90434680.0\n",
      "tensor([  12160.0381,  429183.7500, 1416424.0000])\n",
      "Iteration 237: b = tensor([0.9999, 0.9930, 0.5175], grad_fn=<SubBackward0>), Loss = 90434656.0\n",
      "tensor([  12153.2422,  428672.0000, 1375320.0000])\n",
      "Iteration 238: b = tensor([0.9999, 0.9930, 0.5175], grad_fn=<SubBackward0>), Loss = 90434640.0\n",
      "tensor([  12146.6455,  428174.6250, 1335360.0000])\n",
      "Iteration 239: b = tensor([0.9999, 0.9930, 0.5175], grad_fn=<SubBackward0>), Loss = 90434624.0\n",
      "tensor([  12140.2520,  427693.1250, 1296672.0000])\n",
      "Iteration 240: b = tensor([0.9999, 0.9930, 0.5175], grad_fn=<SubBackward0>), Loss = 90434600.0\n",
      "tensor([  12134.0215,  427224.1875, 1258984.0000])\n",
      "Iteration 241: b = tensor([0.9999, 0.9930, 0.5175], grad_fn=<SubBackward0>), Loss = 90434592.0\n",
      "tensor([  12127.9961,  426769.8750, 1222512.0000])\n",
      "Iteration 242: b = tensor([0.9999, 0.9930, 0.5175], grad_fn=<SubBackward0>), Loss = 90434560.0\n",
      "tensor([  12122.1396,  426328.9375, 1187064.0000])\n",
      "Iteration 243: b = tensor([0.9999, 0.9930, 0.5175], grad_fn=<SubBackward0>), Loss = 90434544.0\n",
      "tensor([  12116.4531,  425900.2500, 1152624.0000])\n",
      "Iteration 244: b = tensor([0.9999, 0.9930, 0.5174], grad_fn=<SubBackward0>), Loss = 90434528.0\n",
      "tensor([  12110.9355,  425484.8125, 1119264.0000])\n",
      "Iteration 245: b = tensor([0.9999, 0.9930, 0.5174], grad_fn=<SubBackward0>), Loss = 90434520.0\n",
      "tensor([  12105.5625,  425080.0000, 1086752.0000])\n",
      "Iteration 246: b = tensor([0.9999, 0.9930, 0.5174], grad_fn=<SubBackward0>), Loss = 90434520.0\n",
      "tensor([  12100.3613,  424688.2500, 1055272.0000])\n",
      "Iteration 247: b = tensor([0.9999, 0.9930, 0.5174], grad_fn=<SubBackward0>), Loss = 90434512.0\n",
      "tensor([  12095.2959,  424306.8125, 1024608.0000])\n",
      "Iteration 248: b = tensor([0.9999, 0.9930, 0.5174], grad_fn=<SubBackward0>), Loss = 90434488.0\n",
      "tensor([ 12090.3789, 423936.1875, 994848.0000])\n",
      "Iteration 249: b = tensor([0.9999, 0.9930, 0.5174], grad_fn=<SubBackward0>), Loss = 90434480.0\n",
      "tensor([ 12085.5996, 423576.3750, 965936.0000])\n",
      "Iteration 250: b = tensor([0.9999, 0.9930, 0.5174], grad_fn=<SubBackward0>), Loss = 90434464.0\n",
      "tensor([ 12080.9639, 423227.5000, 937896.0000])\n",
      "Iteration 251: b = tensor([0.9999, 0.9930, 0.5174], grad_fn=<SubBackward0>), Loss = 90434448.0\n",
      "tensor([ 12076.4756, 422889.3125, 910744.0000])\n",
      "Iteration 252: b = tensor([0.9999, 0.9930, 0.5174], grad_fn=<SubBackward0>), Loss = 90434440.0\n",
      "tensor([ 12072.0918, 422559.2500, 884232.0000])\n",
      "Iteration 253: b = tensor([0.9999, 0.9930, 0.5174], grad_fn=<SubBackward0>), Loss = 90434432.0\n",
      "tensor([ 12067.8545, 422240.1875, 858576.0000])\n",
      "Iteration 254: b = tensor([0.9999, 0.9929, 0.5174], grad_fn=<SubBackward0>), Loss = 90434416.0\n",
      "tensor([ 12063.7373, 421929.6250, 833656.0000])\n",
      "Iteration 255: b = tensor([0.9999, 0.9929, 0.5173], grad_fn=<SubBackward0>), Loss = 90434416.0\n",
      "tensor([ 12059.7217, 421627.5625, 809384.0000])\n",
      "Iteration 256: b = tensor([0.9999, 0.9929, 0.5173], grad_fn=<SubBackward0>), Loss = 90434400.0\n",
      "tensor([ 12055.8320, 421334.3750, 785832.0000])\n",
      "Iteration 257: b = tensor([0.9999, 0.9929, 0.5173], grad_fn=<SubBackward0>), Loss = 90434408.0\n",
      "tensor([ 12052.0479, 421049.5000, 762960.0000])\n",
      "Iteration 258: b = tensor([0.9999, 0.9929, 0.5173], grad_fn=<SubBackward0>), Loss = 90434392.0\n",
      "tensor([ 12048.3799, 420773.1875, 740752.0000])\n",
      "Iteration 259: b = tensor([0.9999, 0.9929, 0.5173], grad_fn=<SubBackward0>), Loss = 90434384.0\n",
      "tensor([ 12044.8242, 420505.6875, 719272.0000])\n",
      "Iteration 260: b = tensor([0.9999, 0.9929, 0.5173], grad_fn=<SubBackward0>), Loss = 90434384.0\n",
      "tensor([ 12041.3535, 420244.2500, 698264.0000])\n",
      "Iteration 261: b = tensor([0.9999, 0.9929, 0.5173], grad_fn=<SubBackward0>), Loss = 90434376.0\n",
      "tensor([ 12037.9961, 419991.4375, 677968.0000])\n",
      "Iteration 262: b = tensor([0.9999, 0.9929, 0.5173], grad_fn=<SubBackward0>), Loss = 90434368.0\n",
      "tensor([ 12034.7285, 419745.1250, 658192.0000])\n",
      "Iteration 263: b = tensor([0.9999, 0.9929, 0.5173], grad_fn=<SubBackward0>), Loss = 90434368.0\n",
      "tensor([ 12031.5801, 419508.4375, 639144.0000])\n",
      "Iteration 264: b = tensor([0.9999, 0.9929, 0.5173], grad_fn=<SubBackward0>), Loss = 90434352.0\n",
      "tensor([ 12028.5010, 419276.7500, 620536.0000])\n",
      "Iteration 265: b = tensor([0.9999, 0.9929, 0.5173], grad_fn=<SubBackward0>), Loss = 90434352.0\n",
      "tensor([ 12025.5127, 419051.8125, 602480.0000])\n",
      "Iteration 266: b = tensor([0.9999, 0.9929, 0.5173], grad_fn=<SubBackward0>), Loss = 90434344.0\n",
      "tensor([ 12022.6152, 418833.3750, 584944.0000])\n",
      "Iteration 267: b = tensor([0.9999, 0.9929, 0.5173], grad_fn=<SubBackward0>), Loss = 90434336.0\n",
      "tensor([ 12019.7979, 418621.3125, 567904.0000])\n",
      "Iteration 268: b = tensor([0.9999, 0.9929, 0.5173], grad_fn=<SubBackward0>), Loss = 90434328.0\n",
      "tensor([ 12017.0703, 418415.7500, 551400.0000])\n",
      "Iteration 269: b = tensor([0.9999, 0.9929, 0.5173], grad_fn=<SubBackward0>), Loss = 90434328.0\n",
      "tensor([ 12014.4004, 418214.5000, 535264.0000])\n",
      "Iteration 270: b = tensor([0.9999, 0.9929, 0.5172], grad_fn=<SubBackward0>), Loss = 90434320.0\n",
      "tensor([ 12011.8115, 418020.0000, 519616.0000])\n",
      "Iteration 271: b = tensor([0.9999, 0.9929, 0.5172], grad_fn=<SubBackward0>), Loss = 90434320.0\n",
      "tensor([ 12009.3057, 417831.3125, 504488.0000])\n",
      "Iteration 272: b = tensor([0.9999, 0.9929, 0.5172], grad_fn=<SubBackward0>), Loss = 90434320.0\n",
      "tensor([ 12006.8594, 417647.2500, 489696.0000])\n",
      "Iteration 273: b = tensor([0.9999, 0.9929, 0.5172], grad_fn=<SubBackward0>), Loss = 90434320.0\n",
      "tensor([ 12004.5010, 417469.6250, 475440.0000])\n",
      "Iteration 274: b = tensor([0.9999, 0.9929, 0.5172], grad_fn=<SubBackward0>), Loss = 90434312.0\n",
      "tensor([ 12002.1943, 417296.2500, 461504.0000])\n",
      "Iteration 275: b = tensor([0.9999, 0.9929, 0.5172], grad_fn=<SubBackward0>), Loss = 90434312.0\n",
      "tensor([ 11999.9805, 417129.5625, 448096.0000])\n",
      "Iteration 276: b = tensor([0.9999, 0.9929, 0.5172], grad_fn=<SubBackward0>), Loss = 90434304.0\n",
      "tensor([ 11997.8203, 416966.9375, 435048.0000])\n",
      "Iteration 277: b = tensor([0.9999, 0.9929, 0.5172], grad_fn=<SubBackward0>), Loss = 90434296.0\n",
      "tensor([ 11995.7129, 416808.5000, 422320.0000])\n",
      "Iteration 278: b = tensor([0.9999, 0.9928, 0.5172], grad_fn=<SubBackward0>), Loss = 90434288.0\n",
      "tensor([ 11993.6641, 416654.1875, 409920.0000])\n",
      "Iteration 279: b = tensor([0.9999, 0.9928, 0.5172], grad_fn=<SubBackward0>), Loss = 90434288.0\n",
      "tensor([ 11991.6738, 416504.7500, 397920.0000])\n",
      "Iteration 280: b = tensor([0.9999, 0.9928, 0.5172], grad_fn=<SubBackward0>), Loss = 90434280.0\n",
      "tensor([ 11989.7432, 416359.0625, 386248.0000])\n",
      "Iteration 281: b = tensor([0.9999, 0.9928, 0.5172], grad_fn=<SubBackward0>), Loss = 90434280.0\n",
      "tensor([ 11987.8613, 416217.5625, 374888.0000])\n",
      "Iteration 282: b = tensor([0.9999, 0.9928, 0.5172], grad_fn=<SubBackward0>), Loss = 90434280.0\n",
      "tensor([ 11986.0410, 416080.6875, 363888.0000])\n",
      "Iteration 283: b = tensor([0.9999, 0.9928, 0.5172], grad_fn=<SubBackward0>), Loss = 90434272.0\n",
      "tensor([ 11984.2773, 415948.1250, 353256.0000])\n",
      "Iteration 284: b = tensor([0.9999, 0.9928, 0.5172], grad_fn=<SubBackward0>), Loss = 90434272.0\n",
      "tensor([ 11982.5732, 415819.7500, 342952.0000])\n",
      "Iteration 285: b = tensor([0.9999, 0.9928, 0.5172], grad_fn=<SubBackward0>), Loss = 90434272.0\n",
      "tensor([ 11980.8955, 415693.5625, 332816.0000])\n",
      "Iteration 286: b = tensor([0.9999, 0.9928, 0.5172], grad_fn=<SubBackward0>), Loss = 90434272.0\n",
      "tensor([ 11979.2754, 415571.7500, 323016.0000])\n",
      "Iteration 287: b = tensor([0.9999, 0.9928, 0.5172], grad_fn=<SubBackward0>), Loss = 90434256.0\n",
      "tensor([ 11977.7100, 415453.8750, 313552.0000])\n",
      "Iteration 288: b = tensor([0.9999, 0.9928, 0.5172], grad_fn=<SubBackward0>), Loss = 90434256.0\n",
      "tensor([ 11976.1729, 415338.1875, 304288.0000])\n",
      "Iteration 289: b = tensor([0.9999, 0.9928, 0.5172], grad_fn=<SubBackward0>), Loss = 90434256.0\n",
      "tensor([ 11974.6914, 415226.8750, 295360.0000])\n",
      "Iteration 290: b = tensor([0.9999, 0.9928, 0.5172], grad_fn=<SubBackward0>), Loss = 90434256.0\n",
      "tensor([ 11973.2432, 415117.8125, 286592.0000])\n",
      "Iteration 291: b = tensor([0.9999, 0.9928, 0.5172], grad_fn=<SubBackward0>), Loss = 90434264.0\n",
      "tensor([ 11971.8467, 415013.0000, 278192.0000])\n",
      "Iteration 292: b = tensor([0.9999, 0.9928, 0.5172], grad_fn=<SubBackward0>), Loss = 90434240.0\n",
      "tensor([ 11970.4854, 414910.6875, 269944.0000])\n",
      "Iteration 293: b = tensor([0.9999, 0.9928, 0.5172], grad_fn=<SubBackward0>), Loss = 90434248.0\n",
      "tensor([ 11969.1738, 414811.9375, 262048.0000])\n",
      "Iteration 294: b = tensor([0.9999, 0.9928, 0.5172], grad_fn=<SubBackward0>), Loss = 90434240.0\n",
      "tensor([ 11967.8926, 414715.7500, 254328.0000])\n",
      "Iteration 295: b = tensor([0.9999, 0.9928, 0.5172], grad_fn=<SubBackward0>), Loss = 90434248.0\n",
      "tensor([ 11966.6416, 414621.5000, 246752.0000])\n",
      "Iteration 296: b = tensor([0.9999, 0.9928, 0.5172], grad_fn=<SubBackward0>), Loss = 90434248.0\n",
      "tensor([ 11965.4453, 414531.9375, 239560.0000])\n",
      "Iteration 297: b = tensor([0.9999, 0.9928, 0.5172], grad_fn=<SubBackward0>), Loss = 90434240.0\n",
      "tensor([ 11964.2793, 414444.1250, 232528.0000])\n",
      "Iteration 298: b = tensor([0.9999, 0.9928, 0.5172], grad_fn=<SubBackward0>), Loss = 90434232.0\n",
      "tensor([ 11963.1387, 414358.6875, 225648.0000])\n",
      "Iteration 299: b = tensor([0.9999, 0.9928, 0.5172], grad_fn=<SubBackward0>), Loss = 90434240.0\n",
      "tensor([ 11962.0332, 414275.1875, 218944.0000])\n",
      "Iteration 300: b = tensor([0.9999, 0.9928, 0.5171], grad_fn=<SubBackward0>), Loss = 90434240.0\n",
      "tensor([ 11960.9482, 414193.8750, 212440.0000])\n",
      "Iteration 301: b = tensor([0.9999, 0.9928, 0.5171], grad_fn=<SubBackward0>), Loss = 90434240.0\n",
      "tensor([ 11959.8955, 414114.8750, 206088.0000])\n",
      "Iteration 302: b = tensor([0.9999, 0.9927, 0.5171], grad_fn=<SubBackward0>), Loss = 90434224.0\n",
      "tensor([ 11958.8730, 414037.9375, 199912.0000])\n",
      "Iteration 303: b = tensor([0.9999, 0.9927, 0.5171], grad_fn=<SubBackward0>), Loss = 90434224.0\n",
      "tensor([ 11957.8770, 413963.2500, 193928.0000])\n",
      "Iteration 304: b = tensor([0.9999, 0.9927, 0.5171], grad_fn=<SubBackward0>), Loss = 90434224.0\n",
      "tensor([ 11956.9121, 413890.7500, 188112.0000])\n",
      "Iteration 305: b = tensor([0.9999, 0.9927, 0.5171], grad_fn=<SubBackward0>), Loss = 90434224.0\n",
      "tensor([ 11955.9727, 413819.8125, 182440.0000])\n",
      "Iteration 306: b = tensor([0.9999, 0.9927, 0.5171], grad_fn=<SubBackward0>), Loss = 90434224.0\n",
      "tensor([ 11955.0625, 413751.3750, 176952.0000])\n",
      "Iteration 307: b = tensor([0.9999, 0.9927, 0.5171], grad_fn=<SubBackward0>), Loss = 90434216.0\n",
      "tensor([ 11954.1787, 413685.0000, 171624.0000])\n",
      "Iteration 308: b = tensor([0.9999, 0.9927, 0.5171], grad_fn=<SubBackward0>), Loss = 90434224.0\n",
      "tensor([ 11953.3252, 413621.2500, 166496.0000])\n",
      "Iteration 309: b = tensor([0.9999, 0.9927, 0.5171], grad_fn=<SubBackward0>), Loss = 90434216.0\n",
      "tensor([ 11952.5020, 413559.2500, 161552.0000])\n",
      "Iteration 310: b = tensor([0.9999, 0.9927, 0.5171], grad_fn=<SubBackward0>), Loss = 90434208.0\n",
      "tensor([ 11951.7051, 413499.3750, 156736.0000])\n",
      "Iteration 311: b = tensor([0.9999, 0.9927, 0.5171], grad_fn=<SubBackward0>), Loss = 90434208.0\n",
      "tensor([ 11950.9385, 413441.7500, 152096.0000])\n",
      "Iteration 312: b = tensor([0.9999, 0.9927, 0.5171], grad_fn=<SubBackward0>), Loss = 90434208.0\n",
      "tensor([ 11950.1680, 413384.1250, 147504.0000])\n",
      "Iteration 313: b = tensor([0.9999, 0.9927, 0.5171], grad_fn=<SubBackward0>), Loss = 90434208.0\n",
      "tensor([ 11949.4268, 413328.5000, 143032.0000])\n",
      "Iteration 314: b = tensor([0.9999, 0.9927, 0.5171], grad_fn=<SubBackward0>), Loss = 90434208.0\n",
      "tensor([ 11948.7158, 413275.2500, 138752.0000])\n",
      "Iteration 315: b = tensor([0.9999, 0.9927, 0.5171], grad_fn=<SubBackward0>), Loss = 90434200.0\n",
      "tensor([ 11948.0342, 413223.8750, 134632.0000])\n",
      "Iteration 316: b = tensor([0.9999, 0.9927, 0.5171], grad_fn=<SubBackward0>), Loss = 90434192.0\n",
      "tensor([ 11947.3516, 413173.0000, 130568.0000])\n",
      "Iteration 317: b = tensor([0.9999, 0.9927, 0.5171], grad_fn=<SubBackward0>), Loss = 90434200.0\n",
      "tensor([ 11946.7002, 413123.8750, 126616.0000])\n",
      "Iteration 318: b = tensor([0.9999, 0.9927, 0.5171], grad_fn=<SubBackward0>), Loss = 90434192.0\n",
      "tensor([ 11946.0713, 413076.5000, 122816.0000])\n",
      "Iteration 319: b = tensor([0.9999, 0.9927, 0.5171], grad_fn=<SubBackward0>), Loss = 90434192.0\n",
      "tensor([ 11945.4414, 413029.3750, 119024.0000])\n",
      "Iteration 320: b = tensor([0.9999, 0.9927, 0.5171], grad_fn=<SubBackward0>), Loss = 90434192.0\n",
      "tensor([ 11944.8535, 412985.2500, 115512.0000])\n",
      "Iteration 321: b = tensor([0.9999, 0.9927, 0.5171], grad_fn=<SubBackward0>), Loss = 90434192.0\n",
      "tensor([ 11944.2812, 412942.6250, 112088.0000])\n",
      "Iteration 322: b = tensor([0.9999, 0.9927, 0.5171], grad_fn=<SubBackward0>), Loss = 90434192.0\n",
      "tensor([ 11943.7129, 412899.7500, 108672.0000])\n",
      "Iteration 323: b = tensor([0.9999, 0.9927, 0.5171], grad_fn=<SubBackward0>), Loss = 90434192.0\n",
      "tensor([ 11943.1660, 412858.8750, 105400.0000])\n",
      "Iteration 324: b = tensor([0.9999, 0.9927, 0.5171], grad_fn=<SubBackward0>), Loss = 90434192.0\n",
      "tensor([ 11942.6240, 412818.1250, 102104.0000])\n",
      "Iteration 325: b = tensor([0.9999, 0.9927, 0.5171], grad_fn=<SubBackward0>), Loss = 90434192.0\n",
      "tensor([ 11942.1162, 412780.3750,  99096.0000])\n",
      "Iteration 326: b = tensor([0.9999, 0.9926, 0.5171], grad_fn=<SubBackward0>), Loss = 90434184.0\n",
      "tensor([ 11941.6025, 412742.1250,  95992.0000])\n",
      "Iteration 327: b = tensor([0.9999, 0.9926, 0.5171], grad_fn=<SubBackward0>), Loss = 90434184.0\n",
      "tensor([ 11941.1172, 412705.5000,  93096.0000])\n",
      "Iteration 328: b = tensor([0.9999, 0.9926, 0.5171], grad_fn=<SubBackward0>), Loss = 90434176.0\n",
      "tensor([ 11940.6338, 412669.0000,  90192.0000])\n",
      "Iteration 329: b = tensor([0.9999, 0.9926, 0.5171], grad_fn=<SubBackward0>), Loss = 90434184.0\n",
      "tensor([ 11940.1816, 412635.1250,  87464.0000])\n",
      "Iteration 330: b = tensor([0.9999, 0.9926, 0.5171], grad_fn=<SubBackward0>), Loss = 90434176.0\n",
      "tensor([ 11939.7246, 412601.3750,  84752.0000])\n",
      "Iteration 331: b = tensor([0.9999, 0.9926, 0.5171], grad_fn=<SubBackward0>), Loss = 90434176.0\n",
      "tensor([ 11939.2969, 412569.1250,  82176.0000])\n",
      "Iteration 332: b = tensor([0.9999, 0.9926, 0.5171], grad_fn=<SubBackward0>), Loss = 90434160.0\n",
      "tensor([ 11938.8711, 412537.5000,  79624.0000])\n",
      "Iteration 333: b = tensor([0.9999, 0.9926, 0.5171], grad_fn=<SubBackward0>), Loss = 90434176.0\n",
      "tensor([ 11938.4668, 412507.2500,  77216.0000])\n",
      "Iteration 334: b = tensor([0.9999, 0.9926, 0.5171], grad_fn=<SubBackward0>), Loss = 90434176.0\n",
      "tensor([ 11938.0742, 412477.8750,  74864.0000])\n",
      "Iteration 335: b = tensor([0.9999, 0.9926, 0.5171], grad_fn=<SubBackward0>), Loss = 90434160.0\n",
      "tensor([ 11937.6738, 412448.0000,  72448.0000])\n",
      "Iteration 336: b = tensor([0.9999, 0.9926, 0.5171], grad_fn=<SubBackward0>), Loss = 90434168.0\n",
      "tensor([ 11937.2998, 412419.8750,  70240.0000])\n",
      "Iteration 337: b = tensor([0.9999, 0.9926, 0.5171], grad_fn=<SubBackward0>), Loss = 90434160.0\n",
      "tensor([ 11936.9307, 412392.1250,  68016.0000])\n",
      "Iteration 338: b = tensor([0.9999, 0.9926, 0.5171], grad_fn=<SubBackward0>), Loss = 90434160.0\n",
      "tensor([ 11936.5879, 412366.7500,  65968.0000])\n",
      "Iteration 339: b = tensor([0.9999, 0.9926, 0.5171], grad_fn=<SubBackward0>), Loss = 90434160.0\n",
      "tensor([ 11936.2490, 412341.3750,  63960.0000])\n",
      "Iteration 340: b = tensor([0.9999, 0.9926, 0.5171], grad_fn=<SubBackward0>), Loss = 90434160.0\n",
      "tensor([ 11935.9141, 412316.0000,  61912.0000])\n",
      "Iteration 341: b = tensor([0.9999, 0.9926, 0.5171], grad_fn=<SubBackward0>), Loss = 90434160.0\n",
      "tensor([ 11935.5898, 412292.3750,  60024.0000])\n",
      "Iteration 342: b = tensor([0.9999, 0.9926, 0.5171], grad_fn=<SubBackward0>), Loss = 90434152.0\n",
      "tensor([ 11935.2793, 412269.0000,  58168.0000])\n",
      "Iteration 343: b = tensor([0.9999, 0.9926, 0.5171], grad_fn=<SubBackward0>), Loss = 90434160.0\n",
      "tensor([ 11934.9658, 412245.5000,  56280.0000])\n",
      "Iteration 344: b = tensor([0.9999, 0.9926, 0.5171], grad_fn=<SubBackward0>), Loss = 90434152.0\n",
      "tensor([ 11934.6816, 412224.6250,  54616.0000])\n",
      "Iteration 345: b = tensor([0.9999, 0.9926, 0.5171], grad_fn=<SubBackward0>), Loss = 90434152.0\n",
      "tensor([ 11934.3965, 412203.2500,  52904.0000])\n",
      "Iteration 346: b = tensor([0.9999, 0.9926, 0.5171], grad_fn=<SubBackward0>), Loss = 90434144.0\n",
      "tensor([ 11934.1113, 412182.0000,  51224.0000])\n",
      "Iteration 347: b = tensor([0.9999, 0.9926, 0.5171], grad_fn=<SubBackward0>), Loss = 90434144.0\n",
      "tensor([ 11933.8262, 412160.7500,  49504.0000])\n",
      "Iteration 348: b = tensor([0.9999, 0.9926, 0.5171], grad_fn=<SubBackward0>), Loss = 90434144.0\n",
      "tensor([ 11933.5693, 412141.5000,  47984.0000])\n",
      "Iteration 349: b = tensor([0.9999, 0.9926, 0.5171], grad_fn=<SubBackward0>), Loss = 90434144.0\n",
      "tensor([ 11933.3154, 412122.7500,  46480.0000])\n",
      "Iteration 350: b = tensor([0.9999, 0.9926, 0.5171], grad_fn=<SubBackward0>), Loss = 90434144.0\n",
      "tensor([ 11933.0605, 412103.6250,  44944.0000])\n",
      "Iteration 351: b = tensor([0.9999, 0.9925, 0.5171], grad_fn=<SubBackward0>), Loss = 90434136.0\n",
      "tensor([ 11932.8037, 412084.6250,  43440.0000])\n",
      "Iteration 352: b = tensor([0.9999, 0.9925, 0.5171], grad_fn=<SubBackward0>), Loss = 90434136.0\n",
      "tensor([ 11932.5723, 412067.3750,  42072.0000])\n",
      "Iteration 353: b = tensor([0.9999, 0.9925, 0.5171], grad_fn=<SubBackward0>), Loss = 90434128.0\n",
      "tensor([ 11932.3438, 412050.3750,  40712.0000])\n",
      "Iteration 354: b = tensor([0.9999, 0.9925, 0.5171], grad_fn=<SubBackward0>), Loss = 90434128.0\n",
      "tensor([ 11932.1123, 412033.3750,  39336.0000])\n",
      "Iteration 355: b = tensor([0.9999, 0.9925, 0.5171], grad_fn=<SubBackward0>), Loss = 90434128.0\n",
      "tensor([ 11931.8896, 412016.8750,  38024.0000])\n",
      "Iteration 356: b = tensor([0.9999, 0.9925, 0.5171], grad_fn=<SubBackward0>), Loss = 90434128.0\n",
      "tensor([ 11931.6875, 412001.5000,  36808.0000])\n",
      "Iteration 357: b = tensor([0.9999, 0.9925, 0.5171], grad_fn=<SubBackward0>), Loss = 90434136.0\n",
      "tensor([ 11931.4922, 411986.8750,  35640.0000])\n",
      "Iteration 358: b = tensor([0.9999, 0.9925, 0.5171], grad_fn=<SubBackward0>), Loss = 90434128.0\n",
      "tensor([ 11931.2852, 411972.1250,  34432.0000])\n",
      "Iteration 359: b = tensor([0.9999, 0.9925, 0.5171], grad_fn=<SubBackward0>), Loss = 90434128.0\n",
      "tensor([ 11931.0889, 411957.1250,  33272.0000])\n",
      "Iteration 360: b = tensor([0.9999, 0.9925, 0.5171], grad_fn=<SubBackward0>), Loss = 90434128.0\n",
      "tensor([ 11930.8887, 411942.6250,  32104.0000])\n",
      "Iteration 361: b = tensor([0.9999, 0.9925, 0.5171], grad_fn=<SubBackward0>), Loss = 90434120.0\n",
      "tensor([ 11930.7168, 411929.8750,  31080.0000])\n",
      "Iteration 362: b = tensor([0.9999, 0.9925, 0.5171], grad_fn=<SubBackward0>), Loss = 90434120.0\n",
      "tensor([ 11930.5488, 411917.2500,  30080.0000])\n",
      "Iteration 363: b = tensor([0.9999, 0.9925, 0.5171], grad_fn=<SubBackward0>), Loss = 90434120.0\n",
      "tensor([ 11930.3643, 411903.7500,  28992.0000])\n",
      "Iteration 364: b = tensor([0.9999, 0.9925, 0.5171], grad_fn=<SubBackward0>), Loss = 90434128.0\n",
      "tensor([ 11930.1934, 411891.2500,  28008.0000])\n",
      "Iteration 365: b = tensor([0.9999, 0.9925, 0.5171], grad_fn=<SubBackward0>), Loss = 90434112.0\n",
      "tensor([ 11930.0225, 411878.1250,  26992.0000])\n",
      "Iteration 366: b = tensor([0.9999, 0.9925, 0.5171], grad_fn=<SubBackward0>), Loss = 90434128.0\n",
      "tensor([ 11929.8545, 411865.8750,  26000.0000])\n",
      "Iteration 367: b = tensor([0.9999, 0.9925, 0.5171], grad_fn=<SubBackward0>), Loss = 90434112.0\n",
      "tensor([ 11929.7051, 411855.0000,  25128.0000])\n",
      "Iteration 368: b = tensor([0.9999, 0.9925, 0.5171], grad_fn=<SubBackward0>), Loss = 90434112.0\n",
      "tensor([ 11929.5645, 411844.7500,  24312.0000])\n",
      "Iteration 369: b = tensor([0.9999, 0.9925, 0.5171], grad_fn=<SubBackward0>), Loss = 90434112.0\n",
      "tensor([ 11929.4258, 411834.2500,  23472.0000])\n",
      "Iteration 370: b = tensor([0.9999, 0.9925, 0.5171], grad_fn=<SubBackward0>), Loss = 90434104.0\n",
      "tensor([ 11929.2812, 411823.8750,  22648.0000])\n",
      "Iteration 371: b = tensor([0.9999, 0.9925, 0.5171], grad_fn=<SubBackward0>), Loss = 90434104.0\n",
      "tensor([ 11929.1377, 411813.0000,  21816.0000])\n",
      "Iteration 372: b = tensor([0.9999, 0.9925, 0.5171], grad_fn=<SubBackward0>), Loss = 90434104.0\n",
      "tensor([ 11928.9961, 411802.7500,  20976.0000])\n",
      "Iteration 373: b = tensor([0.9999, 0.9925, 0.5171], grad_fn=<SubBackward0>), Loss = 90434096.0\n",
      "tensor([ 11928.8555, 411792.3750,  20160.0000])\n",
      "Iteration 374: b = tensor([0.9999, 0.9925, 0.5171], grad_fn=<SubBackward0>), Loss = 90434112.0\n",
      "tensor([ 11928.7354, 411783.6250,  19456.0000])\n",
      "Iteration 375: b = tensor([0.9999, 0.9924, 0.5171], grad_fn=<SubBackward0>), Loss = 90434096.0\n",
      "tensor([ 11928.6191, 411775.3750,  18816.0000])\n",
      "Iteration 376: b = tensor([0.9999, 0.9924, 0.5171], grad_fn=<SubBackward0>), Loss = 90434088.0\n",
      "tensor([ 11928.5078, 411766.7500,  18152.0000])\n",
      "Iteration 377: b = tensor([0.9999, 0.9924, 0.5171], grad_fn=<SubBackward0>), Loss = 90434096.0\n",
      "tensor([ 11928.3926, 411758.5000,  17456.0000])\n",
      "Iteration 378: b = tensor([0.9999, 0.9924, 0.5171], grad_fn=<SubBackward0>), Loss = 90434104.0\n",
      "tensor([ 11928.2793, 411750.2500,  16832.0000])\n",
      "Iteration 379: b = tensor([0.9999, 0.9924, 0.5171], grad_fn=<SubBackward0>), Loss = 90434096.0\n",
      "tensor([ 11928.1641, 411741.6250,  16128.0000])\n",
      "Iteration 380: b = tensor([0.9999, 0.9924, 0.5171], grad_fn=<SubBackward0>), Loss = 90434080.0\n",
      "tensor([ 11928.0547, 411733.7500,  15496.0000])\n",
      "Iteration 381: b = tensor([0.9999, 0.9924, 0.5171], grad_fn=<SubBackward0>), Loss = 90434088.0\n",
      "tensor([ 11927.9395, 411725.2500,  14848.0000])\n",
      "Iteration 382: b = tensor([0.9999, 0.9924, 0.5171], grad_fn=<SubBackward0>), Loss = 90434088.0\n",
      "tensor([ 11927.8486, 411718.5000,  14312.0000])\n",
      "Iteration 383: b = tensor([0.9999, 0.9924, 0.5171], grad_fn=<SubBackward0>), Loss = 90434088.0\n",
      "tensor([ 11927.7656, 411712.3750,  13864.0000])\n",
      "Iteration 384: b = tensor([0.9999, 0.9924, 0.5171], grad_fn=<SubBackward0>), Loss = 90434096.0\n",
      "tensor([ 11927.6797, 411706.0000,  13344.0000])\n",
      "Iteration 385: b = tensor([0.9999, 0.9924, 0.5171], grad_fn=<SubBackward0>), Loss = 90434088.0\n",
      "tensor([ 11927.5928, 411699.6250,  12832.0000])\n",
      "Iteration 386: b = tensor([0.9999, 0.9924, 0.5171], grad_fn=<SubBackward0>), Loss = 90434072.0\n",
      "tensor([ 11927.5059, 411693.6250,  12368.0000])\n",
      "Iteration 387: b = tensor([0.9999, 0.9924, 0.5171], grad_fn=<SubBackward0>), Loss = 90434064.0\n",
      "tensor([ 11927.4229, 411687.5000,  11864.0000])\n",
      "Iteration 388: b = tensor([0.9999, 0.9924, 0.5171], grad_fn=<SubBackward0>), Loss = 90434080.0\n",
      "tensor([ 11927.3330, 411681.1250,  11344.0000])\n",
      "Iteration 389: b = tensor([0.9999, 0.9924, 0.5171], grad_fn=<SubBackward0>), Loss = 90434072.0\n",
      "tensor([ 11927.2480, 411674.8750,  10888.0000])\n",
      "Iteration 390: b = tensor([0.9999, 0.9924, 0.5171], grad_fn=<SubBackward0>), Loss = 90434072.0\n",
      "tensor([ 11927.1592, 411668.8750,  10392.0000])\n",
      "Iteration 391: b = tensor([0.9999, 0.9924, 0.5171], grad_fn=<SubBackward0>), Loss = 90434072.0\n",
      "tensor([ 11927.0762, 411662.3750,   9880.0000])\n",
      "Iteration 392: b = tensor([0.9999, 0.9924, 0.5171], grad_fn=<SubBackward0>), Loss = 90434072.0\n",
      "tensor([ 11926.9912, 411656.3750,   9424.0000])\n",
      "Iteration 393: b = tensor([0.9999, 0.9924, 0.5171], grad_fn=<SubBackward0>), Loss = 90434064.0\n",
      "tensor([ 11926.9043, 411649.8750,   8928.0000])\n",
      "Iteration 394: b = tensor([0.9999, 0.9924, 0.5171], grad_fn=<SubBackward0>), Loss = 90434072.0\n",
      "tensor([ 11926.8457, 411645.7500,   8576.0000])\n",
      "Iteration 395: b = tensor([0.9999, 0.9924, 0.5171], grad_fn=<SubBackward0>), Loss = 90434064.0\n",
      "tensor([ 11926.7891, 411641.8750,   8280.0000])\n",
      "Iteration 396: b = tensor([0.9999, 0.9924, 0.5171], grad_fn=<SubBackward0>), Loss = 90434064.0\n",
      "tensor([ 11926.7344, 411637.8750,   7968.0000])\n",
      "Iteration 397: b = tensor([0.9999, 0.9924, 0.5171], grad_fn=<SubBackward0>), Loss = 90434064.0\n",
      "tensor([ 11926.6738, 411633.6250,   7656.0000])\n",
      "Iteration 398: b = tensor([0.9999, 0.9924, 0.5171], grad_fn=<SubBackward0>), Loss = 90434064.0\n",
      "tensor([ 11926.6152, 411629.5000,   7328.0000])\n",
      "Iteration 399: b = tensor([0.9999, 0.9923, 0.5171], grad_fn=<SubBackward0>), Loss = 90434048.0\n",
      "tensor([ 11926.5566, 411625.2500,   7000.0000])\n",
      "Iteration 400: b = tensor([0.9999, 0.9923, 0.5171], grad_fn=<SubBackward0>), Loss = 90434056.0\n",
      "tensor([ 11926.5029, 411621.5000,   6672.0000])\n",
      "Iteration 401: b = tensor([0.9999, 0.9923, 0.5171], grad_fn=<SubBackward0>), Loss = 90434056.0\n",
      "tensor([ 11926.4434, 411617.0000,   6344.0000])\n",
      "Iteration 402: b = tensor([0.9999, 0.9923, 0.5171], grad_fn=<SubBackward0>), Loss = 90434064.0\n",
      "tensor([ 11926.3887, 411613.1250,   6048.0000])\n",
      "Iteration 403: b = tensor([0.9999, 0.9923, 0.5171], grad_fn=<SubBackward0>), Loss = 90434056.0\n",
      "tensor([ 11926.3271, 411609.0000,   5720.0000])\n",
      "Iteration 404: b = tensor([0.9999, 0.9923, 0.5171], grad_fn=<SubBackward0>), Loss = 90434048.0\n",
      "tensor([ 11926.2705, 411604.8750,   5400.0000])\n",
      "Iteration 405: b = tensor([0.9999, 0.9923, 0.5171], grad_fn=<SubBackward0>), Loss = 90434048.0\n",
      "tensor([ 11926.2119, 411600.7500,   5056.0000])\n",
      "Iteration 406: b = tensor([0.9999, 0.9923, 0.5171], grad_fn=<SubBackward0>), Loss = 90434056.0\n",
      "tensor([ 11926.1543, 411596.7500,   4760.0000])\n",
      "Iteration 407: b = tensor([0.9999, 0.9923, 0.5171], grad_fn=<SubBackward0>), Loss = 90434040.0\n",
      "tensor([ 11926.0967, 411592.3750,   4440.0000])\n",
      "Iteration 408: b = tensor([0.9999, 0.9923, 0.5171], grad_fn=<SubBackward0>), Loss = 90434048.0\n",
      "tensor([ 11926.0371, 411588.3750,   4096.0000])\n",
      "Iteration 409: b = tensor([0.9999, 0.9923, 0.5171], grad_fn=<SubBackward0>), Loss = 90434040.0\n",
      "tensor([ 11925.9795, 411584.1250,   3768.0000])\n",
      "Iteration 410: b = tensor([0.9999, 0.9923, 0.5171], grad_fn=<SubBackward0>), Loss = 90434040.0\n",
      "tensor([ 11925.9238, 411580.2500,   3488.0000])\n",
      "Iteration 411: b = tensor([0.9999, 0.9923, 0.5171], grad_fn=<SubBackward0>), Loss = 90434032.0\n",
      "tensor([ 11925.8672, 411576.2500,   3136.0000])\n",
      "Iteration 412: b = tensor([0.9999, 0.9923, 0.5171], grad_fn=<SubBackward0>), Loss = 90434032.0\n",
      "tensor([ 11925.8066, 411571.7500,   2848.0000])\n",
      "Iteration 413: b = tensor([0.9999, 0.9923, 0.5171], grad_fn=<SubBackward0>), Loss = 90434024.0\n",
      "tensor([ 11925.7793, 411570.1250,   2680.0000])\n",
      "Iteration 414: b = tensor([0.9999, 0.9923, 0.5171], grad_fn=<SubBackward0>), Loss = 90434032.0\n",
      "tensor([ 11925.7480, 411568.1250,   2544.0000])\n",
      "Iteration 415: b = tensor([0.9999, 0.9923, 0.5171], grad_fn=<SubBackward0>), Loss = 90434016.0\n",
      "tensor([ 11925.7207, 411565.8750,   2376.0000])\n",
      "Iteration 416: b = tensor([0.9999, 0.9923, 0.5171], grad_fn=<SubBackward0>), Loss = 90434024.0\n",
      "tensor([ 11925.6924, 411563.8750,   2224.0000])\n",
      "Iteration 417: b = tensor([0.9999, 0.9923, 0.5171], grad_fn=<SubBackward0>), Loss = 90434016.0\n",
      "tensor([ 11925.6602, 411562.0000,   2088.0000])\n",
      "Iteration 418: b = tensor([0.9999, 0.9923, 0.5171], grad_fn=<SubBackward0>), Loss = 90434024.0\n",
      "tensor([ 11925.6338, 411560.1250,   1952.0000])\n",
      "Iteration 419: b = tensor([0.9999, 0.9923, 0.5171], grad_fn=<SubBackward0>), Loss = 90434016.0\n",
      "tensor([ 11925.6064, 411558.2500,   1808.0000])\n",
      "Iteration 420: b = tensor([0.9999, 0.9923, 0.5171], grad_fn=<SubBackward0>), Loss = 90434024.0\n",
      "tensor([ 11925.5713, 411556.0000,   1648.0000])\n",
      "Iteration 421: b = tensor([0.9999, 0.9923, 0.5171], grad_fn=<SubBackward0>), Loss = 90434016.0\n",
      "tensor([ 11925.5449, 411554.2500,   1488.0000])\n",
      "Iteration 422: b = tensor([0.9999, 0.9923, 0.5171], grad_fn=<SubBackward0>), Loss = 90434016.0\n",
      "tensor([ 11925.5176, 411552.2500,   1352.0000])\n",
      "Iteration 423: b = tensor([0.9999, 0.9923, 0.5171], grad_fn=<SubBackward0>), Loss = 90434016.0\n",
      "tensor([ 11925.4893, 411550.3750,   1232.0000])\n",
      "Iteration 424: b = tensor([0.9999, 0.9922, 0.5171], grad_fn=<SubBackward0>), Loss = 90434008.0\n",
      "tensor([ 11925.4580, 411548.2500,   1064.0000])\n",
      "Iteration 425: b = tensor([0.9999, 0.9922, 0.5171], grad_fn=<SubBackward0>), Loss = 90434016.0\n",
      "tensor([ 11925.4268, 411546.3750,    888.0000])\n",
      "Iteration 426: b = tensor([0.9999, 0.9922, 0.5171], grad_fn=<SubBackward0>), Loss = 90434016.0\n",
      "tensor([ 11925.4004, 411544.5000,    760.0000])\n",
      "Iteration 427: b = tensor([0.9999, 0.9922, 0.5171], grad_fn=<SubBackward0>), Loss = 90434008.0\n",
      "tensor([ 11925.3682, 411542.5000,    608.0000])\n",
      "Iteration 428: b = tensor([0.9999, 0.9922, 0.5171], grad_fn=<SubBackward0>), Loss = 90434000.0\n",
      "tensor([ 11925.3398, 411540.5000,    464.0000])\n",
      "Iteration 429: b = tensor([0.9999, 0.9922, 0.5171], grad_fn=<SubBackward0>), Loss = 90434000.0\n",
      "tensor([1.1925e+04, 4.1154e+05, 3.2000e+02])\n",
      "Iteration 430: b = tensor([0.9999, 0.9922, 0.5171], grad_fn=<SubBackward0>), Loss = 90434000.0\n",
      "tensor([1.1925e+04, 4.1154e+05, 1.6000e+02])\n",
      "Iteration 431: b = tensor([0.9999, 0.9922, 0.5171], grad_fn=<SubBackward0>), Loss = 90433992.0\n",
      "tensor([1.1925e+04, 4.1153e+05, 1.6000e+01])\n",
      "Iteration 432: b = tensor([0.9999, 0.9922, 0.5171], grad_fn=<SubBackward0>), Loss = 90434000.0\n",
      "tensor([ 1.1925e+04,  4.1153e+05, -1.1200e+02])\n",
      "Iteration 433: b = tensor([0.9999, 0.9922, 0.5171], grad_fn=<SubBackward0>), Loss = 90433992.0\n",
      "tensor([ 1.1925e+04,  4.1153e+05, -2.8000e+02])\n",
      "Iteration 434: b = tensor([0.9999, 0.9922, 0.5171], grad_fn=<SubBackward0>), Loss = 90433992.0\n",
      "tensor([ 11925.1641, 411528.6250,   -432.0000])\n",
      "Iteration 435: b = tensor([0.9999, 0.9922, 0.5171], grad_fn=<SubBackward0>), Loss = 90433992.0\n",
      "tensor([ 11925.1377, 411526.7500,   -560.0000])\n",
      "Iteration 436: b = tensor([0.9999, 0.9922, 0.5171], grad_fn=<SubBackward0>), Loss = 90434000.0\n",
      "tensor([ 11925.1094, 411525.1250,   -704.0000])\n",
      "Iteration 437: b = tensor([0.9999, 0.9922, 0.5171], grad_fn=<SubBackward0>), Loss = 90433984.0\n",
      "tensor([ 11925.0791, 411522.8750,   -856.0000])\n",
      "Iteration 438: b = tensor([0.9999, 0.9922, 0.5171], grad_fn=<SubBackward0>), Loss = 90433992.0\n",
      "tensor([ 11925.0508, 411521.0000,  -1008.0000])\n",
      "Iteration 439: b = tensor([0.9999, 0.9922, 0.5171], grad_fn=<SubBackward0>), Loss = 90433984.0\n",
      "tensor([ 11925.0195, 411518.7500,  -1160.0000])\n",
      "Iteration 440: b = tensor([0.9999, 0.9922, 0.5171], grad_fn=<SubBackward0>), Loss = 90433984.0\n",
      "tensor([ 11924.9893, 411516.8750,  -1312.0000])\n",
      "Iteration 441: b = tensor([0.9999, 0.9922, 0.5171], grad_fn=<SubBackward0>), Loss = 90433984.0\n",
      "tensor([ 11924.9648, 411515.2500,  -1416.0000])\n",
      "Iteration 442: b = tensor([0.9999, 0.9922, 0.5171], grad_fn=<SubBackward0>), Loss = 90433984.0\n",
      "tensor([ 11924.9346, 411513.2500,  -1592.0000])\n",
      "Iteration 443: b = tensor([0.9999, 0.9922, 0.5171], grad_fn=<SubBackward0>), Loss = 90433984.0\n",
      "tensor([ 11924.9062, 411511.1250,  -1760.0000])\n",
      "Iteration 444: b = tensor([0.9999, 0.9922, 0.5171], grad_fn=<SubBackward0>), Loss = 90433976.0\n",
      "tensor([ 11924.8721, 411509.0000,  -1912.0000])\n",
      "Iteration 445: b = tensor([0.9999, 0.9922, 0.5171], grad_fn=<SubBackward0>), Loss = 90433976.0\n",
      "tensor([ 11924.8457, 411507.3750,  -2056.0000])\n",
      "Iteration 446: b = tensor([0.9999, 0.9922, 0.5171], grad_fn=<SubBackward0>), Loss = 90433976.0\n",
      "tensor([ 11924.8193, 411505.3750,  -2192.0000])\n",
      "Iteration 447: b = tensor([0.9999, 0.9922, 0.5171], grad_fn=<SubBackward0>), Loss = 90433968.0\n",
      "tensor([ 11924.7891, 411503.5000,  -2336.0000])\n",
      "Iteration 448: b = tensor([0.9999, 0.9921, 0.5171], grad_fn=<SubBackward0>), Loss = 90433968.0\n",
      "tensor([ 11924.7549, 411501.1250,  -2488.0000])\n",
      "Iteration 449: b = tensor([0.9999, 0.9921, 0.5171], grad_fn=<SubBackward0>), Loss = 90433976.0\n",
      "tensor([ 11924.7285, 411499.5000,  -2648.0000])\n",
      "Iteration 450: b = tensor([0.9999, 0.9921, 0.5171], grad_fn=<SubBackward0>), Loss = 90433968.0\n",
      "tensor([ 11924.6982, 411497.3750,  -2792.0000])\n",
      "Iteration 451: b = tensor([0.9999, 0.9921, 0.5171], grad_fn=<SubBackward0>), Loss = 90433968.0\n",
      "tensor([ 11924.6699, 411495.5000,  -2936.0000])\n",
      "Iteration 452: b = tensor([0.9999, 0.9921, 0.5171], grad_fn=<SubBackward0>), Loss = 90433968.0\n",
      "tensor([ 11924.6396, 411493.3750,  -3088.0000])\n",
      "Iteration 453: b = tensor([0.9999, 0.9921, 0.5171], grad_fn=<SubBackward0>), Loss = 90433960.0\n",
      "tensor([ 11924.6416, 411493.6250,  -3056.0000])\n",
      "Iteration 454: b = tensor([0.9999, 0.9921, 0.5171], grad_fn=<SubBackward0>), Loss = 90433968.0\n",
      "tensor([ 11924.6406, 411494.0000,  -3024.0000])\n",
      "Iteration 455: b = tensor([0.9999, 0.9921, 0.5171], grad_fn=<SubBackward0>), Loss = 90433968.0\n",
      "tensor([ 11924.6387, 411494.1250,  -3008.0000])\n",
      "Iteration 456: b = tensor([0.9999, 0.9921, 0.5171], grad_fn=<SubBackward0>), Loss = 90433960.0\n",
      "tensor([ 11924.6387, 411494.0000,  -2984.0000])\n",
      "Iteration 457: b = tensor([0.9999, 0.9921, 0.5171], grad_fn=<SubBackward0>), Loss = 90433952.0\n",
      "tensor([ 11924.6357, 411494.3750,  -2968.0000])\n",
      "Iteration 458: b = tensor([0.9999, 0.9921, 0.5171], grad_fn=<SubBackward0>), Loss = 90433960.0\n",
      "tensor([ 11924.6113, 411492.7500,  -3080.0000])\n",
      "Iteration 459: b = tensor([0.9999, 0.9921, 0.5171], grad_fn=<SubBackward0>), Loss = 90433952.0\n",
      "tensor([ 11924.6104, 411492.7500,  -3064.0000])\n",
      "Iteration 460: b = tensor([0.9999, 0.9921, 0.5171], grad_fn=<SubBackward0>), Loss = 90433952.0\n",
      "tensor([ 11924.6074, 411492.7500,  -3048.0000])\n",
      "Iteration 461: b = tensor([0.9999, 0.9921, 0.5171], grad_fn=<SubBackward0>), Loss = 90433952.0\n",
      "tensor([ 11924.6055, 411492.8750,  -3032.0000])\n",
      "Iteration 462: b = tensor([0.9999, 0.9921, 0.5171], grad_fn=<SubBackward0>), Loss = 90433960.0\n",
      "tensor([ 11924.6055, 411493.3750,  -3008.0000])\n",
      "Iteration 463: b = tensor([0.9999, 0.9921, 0.5171], grad_fn=<SubBackward0>), Loss = 90433952.0\n",
      "tensor([ 11924.6055, 411493.3750,  -2984.0000])\n",
      "Iteration 464: b = tensor([0.9999, 0.9921, 0.5171], grad_fn=<SubBackward0>), Loss = 90433952.0\n",
      "tensor([ 11924.6025, 411493.5000,  -2976.0000])\n",
      "Iteration 465: b = tensor([0.9999, 0.9921, 0.5171], grad_fn=<SubBackward0>), Loss = 90433944.0\n",
      "tensor([ 11924.5732, 411491.5000,  -3120.0000])\n",
      "Iteration 466: b = tensor([0.9999, 0.9921, 0.5171], grad_fn=<SubBackward0>), Loss = 90433936.0\n",
      "tensor([ 11924.5752, 411491.8750,  -3088.0000])\n",
      "Iteration 467: b = tensor([0.9999, 0.9921, 0.5171], grad_fn=<SubBackward0>), Loss = 90433952.0\n",
      "tensor([ 11924.5713, 411491.8750,  -3072.0000])\n",
      "Iteration 468: b = tensor([0.9999, 0.9921, 0.5171], grad_fn=<SubBackward0>), Loss = 90433944.0\n",
      "tensor([ 11924.5732, 411492.2500,  -3016.0000])\n",
      "Iteration 469: b = tensor([0.9999, 0.9921, 0.5171], grad_fn=<SubBackward0>), Loss = 90433936.0\n",
      "tensor([ 11924.5723, 411492.6250,  -2984.0000])\n",
      "Iteration 470: b = tensor([0.9999, 0.9921, 0.5171], grad_fn=<SubBackward0>), Loss = 90433936.0\n",
      "tensor([ 11924.5684, 411492.7500,  -2984.0000])\n",
      "Iteration 471: b = tensor([0.9999, 0.9921, 0.5171], grad_fn=<SubBackward0>), Loss = 90433936.0\n",
      "tensor([ 11924.5723, 411492.7500,  -2952.0000])\n",
      "Iteration 472: b = tensor([0.9999, 0.9920, 0.5171], grad_fn=<SubBackward0>), Loss = 90433936.0\n",
      "tensor([ 11924.5410, 411490.8750,  -3104.0000])\n",
      "Iteration 473: b = tensor([0.9999, 0.9920, 0.5171], grad_fn=<SubBackward0>), Loss = 90433936.0\n",
      "tensor([ 11924.5400, 411491.0000,  -3072.0000])\n",
      "Iteration 474: b = tensor([0.9999, 0.9920, 0.5171], grad_fn=<SubBackward0>), Loss = 90433928.0\n",
      "tensor([ 11924.5381, 411490.8750,  -3080.0000])\n",
      "Iteration 475: b = tensor([0.9999, 0.9920, 0.5171], grad_fn=<SubBackward0>), Loss = 90433928.0\n",
      "tensor([ 11924.5391, 411491.2500,  -3032.0000])\n",
      "Iteration 476: b = tensor([0.9999, 0.9920, 0.5171], grad_fn=<SubBackward0>), Loss = 90433928.0\n",
      "tensor([ 11924.5400, 411491.5000,  -3008.0000])\n",
      "Iteration 477: b = tensor([0.9999, 0.9920, 0.5171], grad_fn=<SubBackward0>), Loss = 90433920.0\n",
      "tensor([ 11924.5391, 411491.6250,  -2984.0000])\n",
      "Iteration 478: b = tensor([0.9999, 0.9920, 0.5171], grad_fn=<SubBackward0>), Loss = 90433920.0\n",
      "tensor([ 11924.5342, 411491.6250,  -2968.0000])\n",
      "Iteration 479: b = tensor([0.9999, 0.9920, 0.5171], grad_fn=<SubBackward0>), Loss = 90433920.0\n",
      "tensor([ 11924.5059, 411489.8750,  -3128.0000])\n",
      "Iteration 480: b = tensor([0.9999, 0.9920, 0.5171], grad_fn=<SubBackward0>), Loss = 90433928.0\n",
      "tensor([ 11924.5068, 411490.0000,  -3112.0000])\n",
      "Iteration 481: b = tensor([0.9999, 0.9920, 0.5171], grad_fn=<SubBackward0>), Loss = 90433920.0\n",
      "tensor([ 11924.5049, 411490.1250,  -3056.0000])\n",
      "Iteration 482: b = tensor([0.9999, 0.9920, 0.5171], grad_fn=<SubBackward0>), Loss = 90433920.0\n",
      "tensor([ 11924.5059, 411490.5000,  -3032.0000])\n",
      "Iteration 483: b = tensor([0.9999, 0.9920, 0.5171], grad_fn=<SubBackward0>), Loss = 90433920.0\n",
      "tensor([ 11924.5029, 411490.6250,  -3008.0000])\n",
      "Iteration 484: b = tensor([0.9999, 0.9920, 0.5171], grad_fn=<SubBackward0>), Loss = 90433912.0\n",
      "tensor([ 11924.5020, 411490.8750,  -2992.0000])\n",
      "Iteration 485: b = tensor([0.9999, 0.9920, 0.5171], grad_fn=<SubBackward0>), Loss = 90433912.0\n",
      "tensor([ 11924.5020, 411491.0000,  -2976.0000])\n",
      "Iteration 486: b = tensor([0.9999, 0.9920, 0.5171], grad_fn=<SubBackward0>), Loss = 90433920.0\n",
      "tensor([ 11924.4717, 411489.0000,  -3104.0000])\n",
      "Iteration 487: b = tensor([0.9999, 0.9920, 0.5171], grad_fn=<SubBackward0>), Loss = 90433912.0\n",
      "tensor([ 11924.4707, 411489.1250,  -3104.0000])\n",
      "Iteration 488: b = tensor([0.9999, 0.9920, 0.5171], grad_fn=<SubBackward0>), Loss = 90433920.0\n",
      "tensor([ 11924.4727, 411489.5000,  -3064.0000])\n",
      "Iteration 489: b = tensor([0.9999, 0.9920, 0.5171], grad_fn=<SubBackward0>), Loss = 90433896.0\n",
      "tensor([ 11924.4736, 411489.5000,  -3040.0000])\n",
      "Iteration 490: b = tensor([0.9999, 0.9920, 0.5171], grad_fn=<SubBackward0>), Loss = 90433904.0\n",
      "tensor([ 11924.4668, 411489.6250,  -3008.0000])\n",
      "Iteration 491: b = tensor([0.9999, 0.9920, 0.5171], grad_fn=<SubBackward0>), Loss = 90433904.0\n",
      "tensor([ 11924.4697, 411490.0000,  -2984.0000])\n",
      "Iteration 492: b = tensor([0.9999, 0.9920, 0.5171], grad_fn=<SubBackward0>), Loss = 90433904.0\n",
      "tensor([ 11924.4668, 411490.0000,  -2976.0000])\n",
      "Iteration 493: b = tensor([0.9999, 0.9920, 0.5171], grad_fn=<SubBackward0>), Loss = 90433896.0\n",
      "tensor([ 11924.4365, 411487.7500,  -3128.0000])\n",
      "Iteration 494: b = tensor([0.9999, 0.9920, 0.5171], grad_fn=<SubBackward0>), Loss = 90433888.0\n",
      "tensor([ 11924.4365, 411488.3750,  -3088.0000])\n",
      "Iteration 495: b = tensor([0.9999, 0.9920, 0.5171], grad_fn=<SubBackward0>), Loss = 90433896.0\n",
      "tensor([ 11924.4365, 411488.5000,  -3056.0000])\n",
      "Iteration 496: b = tensor([0.9999, 0.9920, 0.5171], grad_fn=<SubBackward0>), Loss = 90433896.0\n",
      "tensor([ 11924.4375, 411488.7500,  -3040.0000])\n",
      "Iteration 497: b = tensor([0.9999, 0.9919, 0.5171], grad_fn=<SubBackward0>), Loss = 90433896.0\n",
      "tensor([ 11924.4346, 411488.6250,  -3032.0000])\n",
      "Iteration 498: b = tensor([0.9999, 0.9919, 0.5171], grad_fn=<SubBackward0>), Loss = 90433904.0\n",
      "tensor([ 11924.4385, 411489.2500,  -2984.0000])\n",
      "Iteration 499: b = tensor([0.9999, 0.9919, 0.5171], grad_fn=<SubBackward0>), Loss = 90433880.0\n",
      "tensor([ 11924.4355, 411489.3750,  -2968.0000])\n",
      "Iteration 500: b = tensor([0.9999, 0.9919, 0.5171], grad_fn=<SubBackward0>), Loss = 90433896.0\n",
      "tensor([ 11924.4043, 411487.0000,  -3136.0000])\n",
      "Iteration 501: b = tensor([0.9999, 0.9919, 0.5171], grad_fn=<SubBackward0>), Loss = 90433880.0\n",
      "tensor([ 11924.4062, 411487.5000,  -3088.0000])\n",
      "Iteration 502: b = tensor([0.9999, 0.9919, 0.5171], grad_fn=<SubBackward0>), Loss = 90433880.0\n",
      "tensor([ 11924.4033, 411487.5000,  -3088.0000])\n",
      "Iteration 503: b = tensor([0.9999, 0.9919, 0.5171], grad_fn=<SubBackward0>), Loss = 90433880.0\n",
      "tensor([ 11924.4014, 411487.6250,  -3072.0000])\n",
      "Iteration 504: b = tensor([0.9999, 0.9919, 0.5171], grad_fn=<SubBackward0>), Loss = 90433880.0\n",
      "tensor([ 11924.3965, 411487.6250,  -3056.0000])\n",
      "Iteration 505: b = tensor([0.9999, 0.9919, 0.5171], grad_fn=<SubBackward0>), Loss = 90433872.0\n",
      "tensor([ 11924.4014, 411488.0000,  -2976.0000])\n",
      "Iteration 506: b = tensor([0.9999, 0.9919, 0.5171], grad_fn=<SubBackward0>), Loss = 90433880.0\n",
      "tensor([ 11924.3701, 411486.1250,  -3144.0000])\n",
      "Iteration 507: b = tensor([0.9999, 0.9919, 0.5171], grad_fn=<SubBackward0>), Loss = 90433872.0\n",
      "tensor([ 11924.3750, 411486.5000,  -3112.0000])\n",
      "Iteration 508: b = tensor([0.9999, 0.9919, 0.5171], grad_fn=<SubBackward0>), Loss = 90433864.0\n",
      "tensor([ 11924.3701, 411486.3750,  -3088.0000])\n",
      "Iteration 509: b = tensor([0.9999, 0.9919, 0.5171], grad_fn=<SubBackward0>), Loss = 90433872.0\n",
      "tensor([ 11924.3711, 411486.7500,  -3064.0000])\n",
      "Iteration 510: b = tensor([0.9999, 0.9919, 0.5171], grad_fn=<SubBackward0>), Loss = 90433872.0\n",
      "tensor([ 11924.3672, 411486.7500,  -3072.0000])\n",
      "Iteration 511: b = tensor([0.9999, 0.9919, 0.5171], grad_fn=<SubBackward0>), Loss = 90433864.0\n",
      "tensor([ 11924.3662, 411486.8750,  -3040.0000])\n",
      "Iteration 512: b = tensor([0.9999, 0.9919, 0.5171], grad_fn=<SubBackward0>), Loss = 90433864.0\n",
      "tensor([ 11924.3711, 411487.5000,  -2984.0000])\n",
      "Iteration 513: b = tensor([0.9999, 0.9919, 0.5171], grad_fn=<SubBackward0>), Loss = 90433864.0\n",
      "tensor([ 11924.3682, 411487.3750,  -2960.0000])\n",
      "Iteration 514: b = tensor([0.9999, 0.9919, 0.5171], grad_fn=<SubBackward0>), Loss = 90433856.0\n",
      "tensor([ 11924.3379, 411485.7500,  -3104.0000])\n",
      "Iteration 515: b = tensor([0.9999, 0.9919, 0.5171], grad_fn=<SubBackward0>), Loss = 90433864.0\n",
      "tensor([ 11924.3359, 411485.5000,  -3104.0000])\n",
      "Iteration 516: b = tensor([0.9999, 0.9919, 0.5171], grad_fn=<SubBackward0>), Loss = 90433856.0\n",
      "tensor([ 11924.3379, 411485.7500,  -3072.0000])\n",
      "Iteration 517: b = tensor([0.9999, 0.9919, 0.5171], grad_fn=<SubBackward0>), Loss = 90433856.0\n",
      "tensor([ 11924.3340, 411486.1250,  -3032.0000])\n",
      "Iteration 518: b = tensor([0.9999, 0.9919, 0.5171], grad_fn=<SubBackward0>), Loss = 90433856.0\n",
      "tensor([ 11924.3330, 411486.2500,  -3024.0000])\n",
      "Iteration 519: b = tensor([0.9999, 0.9919, 0.5171], grad_fn=<SubBackward0>), Loss = 90433848.0\n",
      "tensor([ 11924.3311, 411486.1250,  -3024.0000])\n",
      "Iteration 520: b = tensor([0.9999, 0.9919, 0.5171], grad_fn=<SubBackward0>), Loss = 90433848.0\n",
      "tensor([ 11924.3340, 411486.6250,  -2984.0000])\n",
      "Iteration 521: b = tensor([0.9999, 0.9918, 0.5171], grad_fn=<SubBackward0>), Loss = 90433856.0\n",
      "tensor([ 11924.3301, 411486.6250,  -2960.0000])\n",
      "Iteration 522: b = tensor([0.9999, 0.9918, 0.5171], grad_fn=<SubBackward0>), Loss = 90433848.0\n",
      "tensor([ 11924.3057, 411484.8750,  -3088.0000])\n",
      "Iteration 523: b = tensor([0.9999, 0.9918, 0.5171], grad_fn=<SubBackward0>), Loss = 90433848.0\n",
      "tensor([ 11924.3008, 411484.7500,  -3096.0000])\n",
      "Iteration 524: b = tensor([0.9999, 0.9918, 0.5171], grad_fn=<SubBackward0>), Loss = 90433848.0\n",
      "tensor([ 11924.2988, 411484.7500,  -3080.0000])\n",
      "Iteration 525: b = tensor([0.9999, 0.9918, 0.5171], grad_fn=<SubBackward0>), Loss = 90433840.0\n",
      "tensor([ 11924.2998, 411485.1250,  -3040.0000])\n",
      "Iteration 526: b = tensor([0.9999, 0.9918, 0.5171], grad_fn=<SubBackward0>), Loss = 90433840.0\n",
      "tensor([ 11924.3037, 411485.5000,  -3000.0000])\n",
      "Iteration 527: b = tensor([0.9999, 0.9918, 0.5171], grad_fn=<SubBackward0>), Loss = 90433840.0\n",
      "tensor([ 11924.2988, 411485.7500,  -2968.0000])\n",
      "Iteration 528: b = tensor([0.9999, 0.9918, 0.5171], grad_fn=<SubBackward0>), Loss = 90433840.0\n",
      "tensor([ 11924.2725, 411484.0000,  -3104.0000])\n",
      "Iteration 529: b = tensor([0.9999, 0.9918, 0.5171], grad_fn=<SubBackward0>), Loss = 90433840.0\n",
      "tensor([ 11924.2734, 411484.1250,  -3056.0000])\n",
      "Iteration 530: b = tensor([0.9999, 0.9918, 0.5171], grad_fn=<SubBackward0>), Loss = 90433840.0\n",
      "tensor([ 11924.2744, 411484.3750,  -3048.0000])\n",
      "Iteration 531: b = tensor([0.9999, 0.9918, 0.5171], grad_fn=<SubBackward0>), Loss = 90433840.0\n",
      "tensor([ 11924.2695, 411484.5000,  -3016.0000])\n",
      "Iteration 532: b = tensor([0.9999, 0.9918, 0.5171], grad_fn=<SubBackward0>), Loss = 90433832.0\n",
      "tensor([ 11924.2676, 411484.3750,  -3008.0000])\n",
      "Iteration 533: b = tensor([0.9999, 0.9918, 0.5171], grad_fn=<SubBackward0>), Loss = 90433840.0\n",
      "tensor([ 11924.2666, 411484.8750,  -2984.0000])\n",
      "Iteration 534: b = tensor([0.9999, 0.9918, 0.5171], grad_fn=<SubBackward0>), Loss = 90433832.0\n",
      "tensor([ 11924.2666, 411484.7500,  -2960.0000])\n",
      "Iteration 535: b = tensor([0.9999, 0.9918, 0.5171], grad_fn=<SubBackward0>), Loss = 90433824.0\n",
      "tensor([ 11924.2383, 411482.8750,  -3112.0000])\n",
      "Iteration 536: b = tensor([0.9999, 0.9918, 0.5171], grad_fn=<SubBackward0>), Loss = 90433824.0\n",
      "tensor([ 11924.2393, 411483.1250,  -3088.0000])\n",
      "Iteration 537: b = tensor([0.9999, 0.9918, 0.5171], grad_fn=<SubBackward0>), Loss = 90433824.0\n",
      "tensor([ 11924.2383, 411483.1250,  -3064.0000])\n",
      "Iteration 538: b = tensor([0.9999, 0.9918, 0.5171], grad_fn=<SubBackward0>), Loss = 90433824.0\n",
      "tensor([ 11924.2324, 411483.6250,  -3056.0000])\n",
      "Iteration 539: b = tensor([0.9999, 0.9918, 0.5171], grad_fn=<SubBackward0>), Loss = 90433824.0\n",
      "tensor([ 11924.2324, 411483.6250,  -3032.0000])\n",
      "Iteration 540: b = tensor([0.9999, 0.9918, 0.5171], grad_fn=<SubBackward0>), Loss = 90433816.0\n",
      "tensor([ 11924.2334, 411483.7500,  -3000.0000])\n",
      "Iteration 541: b = tensor([0.9999, 0.9918, 0.5171], grad_fn=<SubBackward0>), Loss = 90433816.0\n",
      "tensor([ 11924.2314, 411483.8750,  -2984.0000])\n",
      "Iteration 542: b = tensor([0.9999, 0.9918, 0.5171], grad_fn=<SubBackward0>), Loss = 90433816.0\n",
      "tensor([ 11924.2324, 411484.1250,  -2960.0000])\n",
      "Iteration 543: b = tensor([0.9999, 0.9918, 0.5171], grad_fn=<SubBackward0>), Loss = 90433816.0\n",
      "tensor([ 11924.2031, 411482.3750,  -3096.0000])\n",
      "Iteration 544: b = tensor([0.9999, 0.9918, 0.5171], grad_fn=<SubBackward0>), Loss = 90433816.0\n",
      "tensor([ 11924.2002, 411482.5000,  -3064.0000])\n",
      "Iteration 545: b = tensor([0.9999, 0.9917, 0.5171], grad_fn=<SubBackward0>), Loss = 90433816.0\n",
      "tensor([ 11924.2002, 411482.6250,  -3048.0000])\n",
      "Iteration 546: b = tensor([0.9999, 0.9917, 0.5171], grad_fn=<SubBackward0>), Loss = 90433808.0\n",
      "tensor([ 11924.1973, 411482.6250,  -3016.0000])\n",
      "Iteration 547: b = tensor([0.9999, 0.9917, 0.5171], grad_fn=<SubBackward0>), Loss = 90433808.0\n",
      "tensor([ 11924.2031, 411482.7500,  -3000.0000])\n",
      "Iteration 548: b = tensor([0.9999, 0.9917, 0.5171], grad_fn=<SubBackward0>), Loss = 90433800.0\n",
      "tensor([ 11924.1982, 411483.2500,  -2968.0000])\n",
      "Iteration 549: b = tensor([0.9999, 0.9917, 0.5171], grad_fn=<SubBackward0>), Loss = 90433808.0\n",
      "tensor([ 11924.1689, 411481.2500,  -3136.0000])\n",
      "Iteration 550: b = tensor([0.9999, 0.9917, 0.5171], grad_fn=<SubBackward0>), Loss = 90433800.0\n",
      "tensor([ 11924.1689, 411481.1250,  -3096.0000])\n",
      "Iteration 551: b = tensor([0.9999, 0.9917, 0.5171], grad_fn=<SubBackward0>), Loss = 90433800.0\n",
      "tensor([ 11924.1680, 411481.3750,  -3064.0000])\n",
      "Iteration 552: b = tensor([0.9999, 0.9917, 0.5171], grad_fn=<SubBackward0>), Loss = 90433808.0\n",
      "tensor([ 11924.1709, 411481.7500,  -3056.0000])\n",
      "Iteration 553: b = tensor([0.9999, 0.9917, 0.5171], grad_fn=<SubBackward0>), Loss = 90433792.0\n",
      "tensor([ 11924.1680, 411481.6250,  -3056.0000])\n",
      "Iteration 554: b = tensor([0.9999, 0.9917, 0.5171], grad_fn=<SubBackward0>), Loss = 90433792.0\n",
      "tensor([ 11924.1631, 411481.7500,  -3024.0000])\n",
      "Iteration 555: b = tensor([0.9999, 0.9917, 0.5171], grad_fn=<SubBackward0>), Loss = 90433792.0\n",
      "tensor([ 11924.1621, 411482.0000,  -3000.0000])\n",
      "Iteration 556: b = tensor([0.9999, 0.9917, 0.5171], grad_fn=<SubBackward0>), Loss = 90433792.0\n",
      "tensor([ 11924.1680, 411482.7500,  -2936.0000])\n",
      "Iteration 557: b = tensor([0.9999, 0.9917, 0.5171], grad_fn=<SubBackward0>), Loss = 90433784.0\n",
      "tensor([ 11924.1348, 411480.5000,  -3088.0000])\n",
      "Iteration 558: b = tensor([0.9999, 0.9917, 0.5171], grad_fn=<SubBackward0>), Loss = 90433792.0\n",
      "tensor([ 11924.1289, 411480.1250,  -3088.0000])\n",
      "Iteration 559: b = tensor([0.9999, 0.9917, 0.5171], grad_fn=<SubBackward0>), Loss = 90433784.0\n",
      "tensor([ 11924.1309, 411480.6250,  -3064.0000])\n",
      "Iteration 560: b = tensor([0.9999, 0.9917, 0.5171], grad_fn=<SubBackward0>), Loss = 90433776.0\n",
      "tensor([ 11924.1270, 411480.8750,  -3048.0000])\n",
      "Iteration 561: b = tensor([0.9999, 0.9917, 0.5171], grad_fn=<SubBackward0>), Loss = 90433792.0\n",
      "tensor([ 11924.1348, 411481.2500,  -2984.0000])\n",
      "Iteration 562: b = tensor([0.9999, 0.9917, 0.5171], grad_fn=<SubBackward0>), Loss = 90433776.0\n",
      "tensor([ 11924.1338, 411481.6250,  -2952.0000])\n",
      "Iteration 563: b = tensor([0.9999, 0.9917, 0.5171], grad_fn=<SubBackward0>), Loss = 90433784.0\n",
      "tensor([ 11924.1035, 411479.5000,  -3120.0000])\n",
      "Iteration 564: b = tensor([0.9999, 0.9917, 0.5171], grad_fn=<SubBackward0>), Loss = 90433776.0\n",
      "tensor([ 11924.1016, 411479.5000,  -3112.0000])\n",
      "Iteration 565: b = tensor([0.9999, 0.9917, 0.5171], grad_fn=<SubBackward0>), Loss = 90433776.0\n",
      "tensor([ 11924.0977, 411479.6250,  -3088.0000])\n",
      "Iteration 566: b = tensor([0.9999, 0.9917, 0.5171], grad_fn=<SubBackward0>), Loss = 90433776.0\n",
      "tensor([ 11924.0977, 411480.0000,  -3040.0000])\n",
      "Iteration 567: b = tensor([0.9999, 0.9917, 0.5171], grad_fn=<SubBackward0>), Loss = 90433768.0\n",
      "tensor([ 11924.0986, 411480.2500,  -3016.0000])\n",
      "Iteration 568: b = tensor([0.9999, 0.9917, 0.5171], grad_fn=<SubBackward0>), Loss = 90433768.0\n",
      "tensor([ 11924.0967, 411480.1250,  -3024.0000])\n",
      "Iteration 569: b = tensor([0.9999, 0.9917, 0.5171], grad_fn=<SubBackward0>), Loss = 90433776.0\n",
      "tensor([ 11924.0977, 411480.2500,  -2992.0000])\n",
      "Iteration 570: b = tensor([0.9999, 0.9916, 0.5171], grad_fn=<SubBackward0>), Loss = 90433776.0\n",
      "tensor([ 11924.0957, 411480.7500,  -2944.0000])\n",
      "Iteration 571: b = tensor([0.9999, 0.9916, 0.5171], grad_fn=<SubBackward0>), Loss = 90433776.0\n",
      "tensor([ 11924.0674, 411478.8750,  -3112.0000])\n",
      "Iteration 572: b = tensor([0.9999, 0.9916, 0.5171], grad_fn=<SubBackward0>), Loss = 90433768.0\n",
      "tensor([ 11924.0645, 411478.8750,  -3072.0000])\n",
      "Iteration 573: b = tensor([0.9999, 0.9916, 0.5171], grad_fn=<SubBackward0>), Loss = 90433768.0\n",
      "tensor([ 11924.0645, 411478.8750,  -3064.0000])\n",
      "Iteration 574: b = tensor([0.9999, 0.9916, 0.5171], grad_fn=<SubBackward0>), Loss = 90433768.0\n",
      "tensor([ 11924.0664, 411479.2500,  -3032.0000])\n",
      "Iteration 575: b = tensor([0.9999, 0.9916, 0.5171], grad_fn=<SubBackward0>), Loss = 90433760.0\n",
      "tensor([ 11924.0635, 411479.5000,  -3000.0000])\n",
      "Iteration 576: b = tensor([0.9999, 0.9916, 0.5171], grad_fn=<SubBackward0>), Loss = 90433760.0\n",
      "tensor([ 11924.0664, 411479.7500,  -2976.0000])\n",
      "Iteration 577: b = tensor([0.9999, 0.9916, 0.5171], grad_fn=<SubBackward0>), Loss = 90433744.0\n",
      "tensor([ 11924.0342, 411477.7500,  -3112.0000])\n",
      "Iteration 578: b = tensor([0.9999, 0.9916, 0.5171], grad_fn=<SubBackward0>), Loss = 90433760.0\n",
      "tensor([ 11924.0332, 411477.8750,  -3112.0000])\n",
      "Iteration 579: b = tensor([0.9999, 0.9916, 0.5171], grad_fn=<SubBackward0>), Loss = 90433752.0\n",
      "tensor([ 11924.0303, 411477.8750,  -3096.0000])\n",
      "Iteration 580: b = tensor([0.9999, 0.9916, 0.5171], grad_fn=<SubBackward0>), Loss = 90433760.0\n",
      "tensor([ 11924.0303, 411478.1250,  -3064.0000])\n",
      "Iteration 581: b = tensor([0.9999, 0.9916, 0.5171], grad_fn=<SubBackward0>), Loss = 90433744.0\n",
      "tensor([ 11924.0303, 411478.2500,  -3048.0000])\n",
      "Iteration 582: b = tensor([0.9999, 0.9916, 0.5171], grad_fn=<SubBackward0>), Loss = 90433744.0\n",
      "tensor([ 11924.0273, 411478.2500,  -3024.0000])\n",
      "Iteration 583: b = tensor([0.9999, 0.9916, 0.5171], grad_fn=<SubBackward0>), Loss = 90433752.0\n",
      "tensor([ 11924.0283, 411478.3750,  -2992.0000])\n",
      "Iteration 584: b = tensor([0.9999, 0.9916, 0.5171], grad_fn=<SubBackward0>), Loss = 90433752.0\n",
      "tensor([ 11924.0283, 411478.7500,  -2968.0000])\n",
      "Iteration 585: b = tensor([0.9999, 0.9916, 0.5171], grad_fn=<SubBackward0>), Loss = 90433736.0\n",
      "tensor([ 11923.9990, 411476.8750,  -3096.0000])\n",
      "Iteration 586: b = tensor([0.9999, 0.9916, 0.5171], grad_fn=<SubBackward0>), Loss = 90433736.0\n",
      "tensor([ 11923.9980, 411477.0000,  -3080.0000])\n",
      "Iteration 587: b = tensor([0.9999, 0.9916, 0.5171], grad_fn=<SubBackward0>), Loss = 90433744.0\n",
      "tensor([ 11923.9961, 411477.2500,  -3072.0000])\n",
      "Iteration 588: b = tensor([0.9999, 0.9916, 0.5171], grad_fn=<SubBackward0>), Loss = 90433728.0\n",
      "tensor([ 11923.9951, 411477.6250,  -3048.0000])\n",
      "Iteration 589: b = tensor([0.9999, 0.9916, 0.5171], grad_fn=<SubBackward0>), Loss = 90433736.0\n",
      "tensor([ 11923.9990, 411477.7500,  -3008.0000])\n",
      "Iteration 590: b = tensor([0.9999, 0.9916, 0.5171], grad_fn=<SubBackward0>), Loss = 90433736.0\n",
      "tensor([ 11923.9941, 411477.8750,  -2992.0000])\n",
      "Iteration 591: b = tensor([0.9999, 0.9916, 0.5171], grad_fn=<SubBackward0>), Loss = 90433744.0\n",
      "tensor([ 11923.9971, 411478.0000,  -2960.0000])\n",
      "Iteration 592: b = tensor([0.9999, 0.9916, 0.5171], grad_fn=<SubBackward0>), Loss = 90433728.0\n",
      "tensor([ 11923.9629, 411475.8750,  -3120.0000])\n",
      "Iteration 593: b = tensor([0.9999, 0.9916, 0.5171], grad_fn=<SubBackward0>), Loss = 90433720.0\n",
      "tensor([ 11923.9629, 411476.2500,  -3112.0000])\n",
      "Iteration 594: b = tensor([0.9999, 0.9915, 0.5171], grad_fn=<SubBackward0>), Loss = 90433728.0\n",
      "tensor([ 11923.9639, 411476.2500,  -3072.0000])\n",
      "Iteration 595: b = tensor([0.9999, 0.9915, 0.5171], grad_fn=<SubBackward0>), Loss = 90433728.0\n",
      "tensor([ 11923.9639, 411476.5000,  -3056.0000])\n",
      "Iteration 596: b = tensor([0.9998, 0.9915, 0.5171], grad_fn=<SubBackward0>), Loss = 90433728.0\n",
      "tensor([ 11923.9639, 411476.7500,  -3008.0000])\n",
      "Iteration 597: b = tensor([0.9998, 0.9915, 0.5171], grad_fn=<SubBackward0>), Loss = 90433720.0\n",
      "tensor([ 11923.9629, 411477.1250,  -2992.0000])\n",
      "Iteration 598: b = tensor([0.9998, 0.9915, 0.5171], grad_fn=<SubBackward0>), Loss = 90433720.0\n",
      "tensor([ 11923.9590, 411477.1250,  -2952.0000])\n",
      "Iteration 599: b = tensor([0.9998, 0.9915, 0.5171], grad_fn=<SubBackward0>), Loss = 90433712.0\n",
      "tensor([ 11923.9346, 411475.2500,  -3120.0000])\n",
      "Iteration 600: b = tensor([0.9998, 0.9915, 0.5171], grad_fn=<SubBackward0>), Loss = 90433720.0\n",
      "tensor([ 11923.9346, 411475.3750,  -3088.0000])\n",
      "Iteration 601: b = tensor([0.9998, 0.9915, 0.5171], grad_fn=<SubBackward0>), Loss = 90433720.0\n",
      "tensor([ 11923.9297, 411475.5000,  -3072.0000])\n",
      "Iteration 602: b = tensor([0.9998, 0.9915, 0.5171], grad_fn=<SubBackward0>), Loss = 90433720.0\n",
      "tensor([ 11923.9277, 411475.7500,  -3056.0000])\n",
      "Iteration 603: b = tensor([0.9998, 0.9915, 0.5171], grad_fn=<SubBackward0>), Loss = 90433720.0\n",
      "tensor([ 11923.9307, 411475.7500,  -3024.0000])\n",
      "Iteration 604: b = tensor([0.9998, 0.9915, 0.5171], grad_fn=<SubBackward0>), Loss = 90433712.0\n",
      "tensor([ 11923.9297, 411476.0000,  -3000.0000])\n",
      "Iteration 605: b = tensor([0.9998, 0.9915, 0.5171], grad_fn=<SubBackward0>), Loss = 90433712.0\n",
      "tensor([ 11923.9297, 411476.0000,  -2960.0000])\n",
      "Iteration 606: b = tensor([0.9998, 0.9915, 0.5171], grad_fn=<SubBackward0>), Loss = 90433704.0\n",
      "tensor([ 11923.9004, 411474.0000,  -3128.0000])\n",
      "Iteration 607: b = tensor([0.9998, 0.9915, 0.5171], grad_fn=<SubBackward0>), Loss = 90433712.0\n",
      "tensor([ 11923.8945, 411474.3750,  -3112.0000])\n",
      "Iteration 608: b = tensor([0.9998, 0.9915, 0.5171], grad_fn=<SubBackward0>), Loss = 90433712.0\n",
      "tensor([ 11923.8965, 411474.6250,  -3072.0000])\n",
      "Iteration 609: b = tensor([0.9998, 0.9915, 0.5171], grad_fn=<SubBackward0>), Loss = 90433704.0\n",
      "tensor([ 11923.8945, 411474.8750,  -3032.0000])\n",
      "Iteration 610: b = tensor([0.9998, 0.9915, 0.5171], grad_fn=<SubBackward0>), Loss = 90433696.0\n",
      "tensor([ 11923.8945, 411475.0000,  -3016.0000])\n",
      "Iteration 611: b = tensor([0.9998, 0.9915, 0.5171], grad_fn=<SubBackward0>), Loss = 90433696.0\n",
      "tensor([ 11923.8926, 411475.1250,  -3008.0000])\n",
      "Iteration 612: b = tensor([0.9998, 0.9915, 0.5171], grad_fn=<SubBackward0>), Loss = 90433696.0\n",
      "tensor([ 11923.8955, 411475.5000,  -2952.0000])\n",
      "Iteration 613: b = tensor([0.9998, 0.9915, 0.5171], grad_fn=<SubBackward0>), Loss = 90433688.0\n",
      "tensor([ 11923.8633, 411473.2500,  -3120.0000])\n",
      "Iteration 614: b = tensor([0.9998, 0.9915, 0.5171], grad_fn=<SubBackward0>), Loss = 90433688.0\n",
      "tensor([ 11923.8633, 411473.5000,  -3096.0000])\n",
      "Iteration 615: b = tensor([0.9998, 0.9915, 0.5171], grad_fn=<SubBackward0>), Loss = 90433680.0\n",
      "tensor([ 11923.8623, 411473.7500,  -3104.0000])\n",
      "Iteration 616: b = tensor([0.9998, 0.9915, 0.5171], grad_fn=<SubBackward0>), Loss = 90433696.0\n",
      "tensor([ 11923.8701, 411474.2500,  -3016.0000])\n",
      "Iteration 617: b = tensor([0.9998, 0.9915, 0.5171], grad_fn=<SubBackward0>), Loss = 90433696.0\n",
      "tensor([ 11923.8633, 411474.1250,  -3016.0000])\n",
      "Iteration 618: b = tensor([0.9998, 0.9914, 0.5171], grad_fn=<SubBackward0>), Loss = 90433688.0\n",
      "tensor([ 11923.8555, 411474.1250,  -3008.0000])\n",
      "Iteration 619: b = tensor([0.9998, 0.9914, 0.5171], grad_fn=<SubBackward0>), Loss = 90433696.0\n",
      "tensor([ 11923.8564, 411474.3750,  -2992.0000])\n",
      "Iteration 620: b = tensor([0.9998, 0.9914, 0.5171], grad_fn=<SubBackward0>), Loss = 90433680.0\n",
      "tensor([ 11923.8594, 411474.5000,  -2960.0000])\n",
      "Iteration 621: b = tensor([0.9998, 0.9914, 0.5171], grad_fn=<SubBackward0>), Loss = 90433680.0\n",
      "tensor([ 11923.8281, 411472.7500,  -3088.0000])\n",
      "Iteration 622: b = tensor([0.9998, 0.9914, 0.5171], grad_fn=<SubBackward0>), Loss = 90433680.0\n",
      "tensor([ 11923.8271, 411472.6250,  -3088.0000])\n",
      "Iteration 623: b = tensor([0.9998, 0.9914, 0.5171], grad_fn=<SubBackward0>), Loss = 90433680.0\n",
      "tensor([ 11923.8262, 411472.8750,  -3064.0000])\n",
      "Iteration 624: b = tensor([0.9998, 0.9914, 0.5171], grad_fn=<SubBackward0>), Loss = 90433680.0\n",
      "tensor([ 11923.8242, 411473.1250,  -3040.0000])\n",
      "Iteration 625: b = tensor([0.9998, 0.9914, 0.5171], grad_fn=<SubBackward0>), Loss = 90433672.0\n",
      "tensor([ 11923.8213, 411473.2500,  -3008.0000])\n",
      "Iteration 626: b = tensor([0.9998, 0.9914, 0.5171], grad_fn=<SubBackward0>), Loss = 90433672.0\n",
      "tensor([ 11923.8223, 411473.6250,  -2984.0000])\n",
      "Iteration 627: b = tensor([0.9998, 0.9914, 0.5171], grad_fn=<SubBackward0>), Loss = 90433672.0\n",
      "tensor([ 11923.8213, 411473.6250,  -2968.0000])\n",
      "Iteration 628: b = tensor([0.9998, 0.9914, 0.5171], grad_fn=<SubBackward0>), Loss = 90433672.0\n",
      "tensor([ 11923.7930, 411471.5000,  -3136.0000])\n",
      "Iteration 629: b = tensor([0.9998, 0.9914, 0.5171], grad_fn=<SubBackward0>), Loss = 90433672.0\n",
      "tensor([ 11923.7871, 411471.5000,  -3120.0000])\n",
      "Iteration 630: b = tensor([0.9998, 0.9914, 0.5171], grad_fn=<SubBackward0>), Loss = 90433664.0\n",
      "tensor([ 11923.7939, 411471.8750,  -3064.0000])\n",
      "Iteration 631: b = tensor([0.9998, 0.9914, 0.5171], grad_fn=<SubBackward0>), Loss = 90433664.0\n",
      "tensor([ 11923.7949, 411472.6250,  -3008.0000])\n",
      "Iteration 632: b = tensor([0.9998, 0.9914, 0.5171], grad_fn=<SubBackward0>), Loss = 90433664.0\n",
      "tensor([ 11923.7930, 411472.7500,  -2984.0000])\n",
      "Iteration 633: b = tensor([0.9998, 0.9914, 0.5171], grad_fn=<SubBackward0>), Loss = 90433656.0\n",
      "tensor([ 11923.7920, 411473.1250,  -2944.0000])\n",
      "Iteration 634: b = tensor([0.9998, 0.9914, 0.5171], grad_fn=<SubBackward0>), Loss = 90433664.0\n",
      "tensor([ 11923.7646, 411471.0000,  -3112.0000])\n",
      "Iteration 635: b = tensor([0.9998, 0.9914, 0.5171], grad_fn=<SubBackward0>), Loss = 90433664.0\n",
      "tensor([ 11923.7637, 411470.8750,  -3088.0000])\n",
      "Iteration 636: b = tensor([0.9998, 0.9914, 0.5171], grad_fn=<SubBackward0>), Loss = 90433664.0\n",
      "tensor([ 11923.7617, 411471.1250,  -3072.0000])\n",
      "Iteration 637: b = tensor([0.9998, 0.9914, 0.5171], grad_fn=<SubBackward0>), Loss = 90433664.0\n",
      "tensor([ 11923.7627, 411471.5000,  -3048.0000])\n",
      "Iteration 638: b = tensor([0.9998, 0.9914, 0.5171], grad_fn=<SubBackward0>), Loss = 90433656.0\n",
      "tensor([ 11923.7627, 411471.7500,  -3016.0000])\n",
      "Iteration 639: b = tensor([0.9998, 0.9914, 0.5171], grad_fn=<SubBackward0>), Loss = 90433656.0\n",
      "tensor([ 11923.7588, 411471.6250,  -3008.0000])\n",
      "Iteration 640: b = tensor([0.9998, 0.9914, 0.5171], grad_fn=<SubBackward0>), Loss = 90433648.0\n",
      "tensor([ 11923.7578, 411471.7500,  -3000.0000])\n",
      "Iteration 641: b = tensor([0.9998, 0.9914, 0.5171], grad_fn=<SubBackward0>), Loss = 90433648.0\n",
      "tensor([ 11923.7568, 411471.8750,  -2976.0000])\n",
      "Iteration 642: b = tensor([0.9998, 0.9914, 0.5171], grad_fn=<SubBackward0>), Loss = 90433648.0\n",
      "tensor([ 11923.7275, 411470.1250,  -3104.0000])\n",
      "Iteration 643: b = tensor([0.9998, 0.9913, 0.5171], grad_fn=<SubBackward0>), Loss = 90433632.0\n",
      "tensor([ 11923.7266, 411470.1250,  -3072.0000])\n",
      "Iteration 644: b = tensor([0.9998, 0.9913, 0.5171], grad_fn=<SubBackward0>), Loss = 90433648.0\n",
      "tensor([ 11923.7246, 411470.3750,  -3064.0000])\n",
      "Iteration 645: b = tensor([0.9998, 0.9913, 0.5171], grad_fn=<SubBackward0>), Loss = 90433640.0\n",
      "tensor([ 11923.7246, 411470.6250,  -3032.0000])\n",
      "Iteration 646: b = tensor([0.9998, 0.9913, 0.5171], grad_fn=<SubBackward0>), Loss = 90433632.0\n",
      "tensor([ 11923.7246, 411470.8750,  -2992.0000])\n",
      "Iteration 647: b = tensor([0.9998, 0.9913, 0.5171], grad_fn=<SubBackward0>), Loss = 90433632.0\n",
      "tensor([ 11923.7266, 411471.3750,  -2960.0000])\n",
      "Iteration 648: b = tensor([0.9998, 0.9913, 0.5171], grad_fn=<SubBackward0>), Loss = 90433632.0\n",
      "tensor([ 11923.6992, 411469.3750,  -3096.0000])\n",
      "Iteration 649: b = tensor([0.9998, 0.9913, 0.5171], grad_fn=<SubBackward0>), Loss = 90433632.0\n",
      "tensor([ 11923.6924, 411469.1250,  -3120.0000])\n",
      "Iteration 650: b = tensor([0.9998, 0.9913, 0.5171], grad_fn=<SubBackward0>), Loss = 90433632.0\n",
      "tensor([ 11923.6943, 411469.5000,  -3088.0000])\n",
      "Iteration 651: b = tensor([0.9998, 0.9913, 0.5171], grad_fn=<SubBackward0>), Loss = 90433624.0\n",
      "tensor([ 11923.6934, 411469.5000,  -3056.0000])\n",
      "Iteration 652: b = tensor([0.9998, 0.9913, 0.5171], grad_fn=<SubBackward0>), Loss = 90433624.0\n",
      "tensor([ 11923.6914, 411469.6250,  -3032.0000])\n",
      "Iteration 653: b = tensor([0.9998, 0.9913, 0.5171], grad_fn=<SubBackward0>), Loss = 90433616.0\n",
      "tensor([ 11923.6934, 411470.1250,  -2992.0000])\n",
      "Iteration 654: b = tensor([0.9998, 0.9913, 0.5171], grad_fn=<SubBackward0>), Loss = 90433616.0\n",
      "tensor([ 11923.6914, 411470.2500,  -2984.0000])\n",
      "Iteration 655: b = tensor([0.9998, 0.9913, 0.5171], grad_fn=<SubBackward0>), Loss = 90433616.0\n",
      "tensor([ 11923.6895, 411470.2500,  -2968.0000])\n",
      "Iteration 656: b = tensor([0.9998, 0.9913, 0.5171], grad_fn=<SubBackward0>), Loss = 90433624.0\n",
      "tensor([ 11923.6602, 411468.2500,  -3120.0000])\n",
      "Iteration 657: b = tensor([0.9998, 0.9913, 0.5171], grad_fn=<SubBackward0>), Loss = 90433616.0\n",
      "tensor([ 11923.6611, 411468.5000,  -3096.0000])\n",
      "Iteration 658: b = tensor([0.9998, 0.9913, 0.5171], grad_fn=<SubBackward0>), Loss = 90433632.0\n",
      "tensor([ 11923.6602, 411468.8750,  -3032.0000])\n",
      "Iteration 659: b = tensor([0.9998, 0.9913, 0.5171], grad_fn=<SubBackward0>), Loss = 90433616.0\n",
      "tensor([ 11923.6611, 411469.1250,  -3024.0000])\n",
      "Iteration 660: b = tensor([0.9998, 0.9913, 0.5171], grad_fn=<SubBackward0>), Loss = 90433616.0\n",
      "tensor([ 11923.6562, 411469.0000,  -3008.0000])\n",
      "Iteration 661: b = tensor([0.9998, 0.9913, 0.5171], grad_fn=<SubBackward0>), Loss = 90433616.0\n",
      "tensor([ 11923.6582, 411469.3750,  -2984.0000])\n",
      "Iteration 662: b = tensor([0.9998, 0.9913, 0.5171], grad_fn=<SubBackward0>), Loss = 90433616.0\n",
      "tensor([ 11923.6523, 411469.2500,  -2992.0000])\n",
      "Iteration 663: b = tensor([0.9998, 0.9913, 0.5171], grad_fn=<SubBackward0>), Loss = 90433616.0\n",
      "tensor([ 11923.6582, 411469.6250,  -2936.0000])\n",
      "Iteration 664: b = tensor([0.9998, 0.9913, 0.5171], grad_fn=<SubBackward0>), Loss = 90433616.0\n",
      "tensor([ 11923.6270, 411467.5000,  -3088.0000])\n",
      "Iteration 665: b = tensor([0.9998, 0.9913, 0.5171], grad_fn=<SubBackward0>), Loss = 90433608.0\n",
      "tensor([ 11923.6260, 411467.8750,  -3056.0000])\n",
      "Iteration 666: b = tensor([0.9998, 0.9913, 0.5171], grad_fn=<SubBackward0>), Loss = 90433616.0\n",
      "tensor([ 11923.6240, 411467.8750,  -3040.0000])\n",
      "Iteration 667: b = tensor([0.9998, 0.9912, 0.5171], grad_fn=<SubBackward0>), Loss = 90433608.0\n",
      "tensor([ 11923.6260, 411468.3750,  -3008.0000])\n",
      "Iteration 668: b = tensor([0.9998, 0.9912, 0.5171], grad_fn=<SubBackward0>), Loss = 90433608.0\n",
      "tensor([ 11923.6250, 411468.3750,  -2992.0000])\n",
      "Iteration 669: b = tensor([0.9998, 0.9912, 0.5171], grad_fn=<SubBackward0>), Loss = 90433600.0\n",
      "tensor([ 11923.6191, 411468.3750,  -2984.0000])\n",
      "Iteration 670: b = tensor([0.9998, 0.9912, 0.5171], grad_fn=<SubBackward0>), Loss = 90433600.0\n",
      "tensor([ 11923.6201, 411468.6250,  -2928.0000])\n",
      "Iteration 671: b = tensor([0.9998, 0.9912, 0.5171], grad_fn=<SubBackward0>), Loss = 90433600.0\n",
      "tensor([ 11923.5928, 411467.0000,  -3064.0000])\n",
      "Iteration 672: b = tensor([0.9998, 0.9912, 0.5171], grad_fn=<SubBackward0>), Loss = 90433592.0\n",
      "tensor([ 11923.5928, 411467.1250,  -3040.0000])\n",
      "Iteration 673: b = tensor([0.9998, 0.9912, 0.5171], grad_fn=<SubBackward0>), Loss = 90433592.0\n",
      "tensor([ 11923.5908, 411467.0000,  -3048.0000])\n",
      "Iteration 674: b = tensor([0.9998, 0.9912, 0.5171], grad_fn=<SubBackward0>), Loss = 90433600.0\n",
      "tensor([ 11923.5879, 411467.2500,  -3032.0000])\n",
      "Iteration 675: b = tensor([0.9998, 0.9912, 0.5171], grad_fn=<SubBackward0>), Loss = 90433584.0\n",
      "tensor([ 11923.5889, 411467.1250,  -3008.0000])\n",
      "Iteration 676: b = tensor([0.9998, 0.9912, 0.5171], grad_fn=<SubBackward0>), Loss = 90433584.0\n",
      "tensor([ 11923.5928, 411468.0000,  -2936.0000])\n",
      "Iteration 677: b = tensor([0.9998, 0.9912, 0.5171], grad_fn=<SubBackward0>), Loss = 90433592.0\n",
      "tensor([ 11923.5625, 411466.0000,  -3064.0000])\n",
      "Iteration 678: b = tensor([0.9998, 0.9912, 0.5171], grad_fn=<SubBackward0>), Loss = 90433584.0\n",
      "tensor([ 11923.5586, 411466.0000,  -3056.0000])\n",
      "Iteration 679: b = tensor([0.9998, 0.9912, 0.5171], grad_fn=<SubBackward0>), Loss = 90433584.0\n",
      "tensor([ 11923.5527, 411466.0000,  -3056.0000])\n",
      "Iteration 680: b = tensor([0.9998, 0.9912, 0.5171], grad_fn=<SubBackward0>), Loss = 90433584.0\n",
      "tensor([ 11923.5566, 411466.1250,  -3040.0000])\n",
      "Iteration 681: b = tensor([0.9998, 0.9912, 0.5171], grad_fn=<SubBackward0>), Loss = 90433584.0\n",
      "tensor([ 11923.5596, 411466.6250,  -2992.0000])\n",
      "Iteration 682: b = tensor([0.9998, 0.9912, 0.5171], grad_fn=<SubBackward0>), Loss = 90433584.0\n",
      "tensor([ 11923.5576, 411466.6250,  -2992.0000])\n",
      "Iteration 683: b = tensor([0.9998, 0.9912, 0.5171], grad_fn=<SubBackward0>), Loss = 90433568.0\n",
      "tensor([ 11923.5537, 411466.6250,  -2960.0000])\n",
      "Iteration 684: b = tensor([0.9998, 0.9912, 0.5171], grad_fn=<SubBackward0>), Loss = 90433576.0\n",
      "tensor([ 11923.5264, 411464.7500,  -3112.0000])\n",
      "Iteration 685: b = tensor([0.9998, 0.9912, 0.5171], grad_fn=<SubBackward0>), Loss = 90433568.0\n",
      "tensor([ 11923.5273, 411465.1250,  -3072.0000])\n",
      "Iteration 686: b = tensor([0.9998, 0.9912, 0.5171], grad_fn=<SubBackward0>), Loss = 90433568.0\n",
      "tensor([ 11923.5234, 411465.3750,  -3056.0000])\n",
      "Iteration 687: b = tensor([0.9998, 0.9912, 0.5171], grad_fn=<SubBackward0>), Loss = 90433568.0\n",
      "tensor([ 11923.5225, 411465.5000,  -3040.0000])\n",
      "Iteration 688: b = tensor([0.9998, 0.9912, 0.5171], grad_fn=<SubBackward0>), Loss = 90433576.0\n",
      "tensor([ 11923.5234, 411465.5000,  -3016.0000])\n",
      "Iteration 689: b = tensor([0.9998, 0.9912, 0.5171], grad_fn=<SubBackward0>), Loss = 90433568.0\n",
      "tensor([ 11923.5205, 411465.6250,  -2992.0000])\n",
      "Iteration 690: b = tensor([0.9998, 0.9912, 0.5171], grad_fn=<SubBackward0>), Loss = 90433568.0\n",
      "tensor([ 11923.5195, 411465.8750,  -2960.0000])\n",
      "Iteration 691: b = tensor([0.9998, 0.9911, 0.5171], grad_fn=<SubBackward0>), Loss = 90433568.0\n",
      "tensor([ 11923.4893, 411463.7500,  -3136.0000])\n",
      "Iteration 692: b = tensor([0.9998, 0.9911, 0.5171], grad_fn=<SubBackward0>), Loss = 90433560.0\n",
      "tensor([ 11923.4912, 411463.8750,  -3112.0000])\n",
      "Iteration 693: b = tensor([0.9998, 0.9911, 0.5171], grad_fn=<SubBackward0>), Loss = 90433560.0\n",
      "tensor([ 11923.4922, 411464.5000,  -3056.0000])\n",
      "Iteration 694: b = tensor([0.9998, 0.9911, 0.5171], grad_fn=<SubBackward0>), Loss = 90433552.0\n",
      "tensor([ 11923.4902, 411464.6250,  -3032.0000])\n",
      "Iteration 695: b = tensor([0.9998, 0.9911, 0.5171], grad_fn=<SubBackward0>), Loss = 90433552.0\n",
      "tensor([ 11923.4863, 411464.7500,  -3008.0000])\n",
      "Iteration 696: b = tensor([0.9998, 0.9911, 0.5171], grad_fn=<SubBackward0>), Loss = 90433552.0\n",
      "tensor([ 11923.4863, 411464.6250,  -3000.0000])\n",
      "Iteration 697: b = tensor([0.9998, 0.9911, 0.5171], grad_fn=<SubBackward0>), Loss = 90433552.0\n",
      "tensor([ 11923.4824, 411464.7500,  -2976.0000])\n",
      "Iteration 698: b = tensor([0.9998, 0.9911, 0.5171], grad_fn=<SubBackward0>), Loss = 90433552.0\n",
      "tensor([ 11923.4570, 411463.0000,  -3120.0000])\n",
      "Iteration 699: b = tensor([0.9998, 0.9911, 0.5171], grad_fn=<SubBackward0>), Loss = 90433552.0\n",
      "tensor([ 11923.4551, 411463.2500,  -3096.0000])\n",
      "Iteration 700: b = tensor([0.9998, 0.9911, 0.5171], grad_fn=<SubBackward0>), Loss = 90433552.0\n",
      "tensor([ 11923.4512, 411463.1250,  -3072.0000])\n",
      "Iteration 701: b = tensor([0.9998, 0.9911, 0.5171], grad_fn=<SubBackward0>), Loss = 90433536.0\n",
      "tensor([ 11923.4531, 411463.6250,  -3040.0000])\n",
      "Iteration 702: b = tensor([0.9998, 0.9911, 0.5171], grad_fn=<SubBackward0>), Loss = 90433552.0\n",
      "tensor([ 11923.4521, 411463.7500,  -3024.0000])\n",
      "Iteration 703: b = tensor([0.9998, 0.9911, 0.5171], grad_fn=<SubBackward0>), Loss = 90433544.0\n",
      "tensor([ 11923.4521, 411464.0000,  -2984.0000])\n",
      "Iteration 704: b = tensor([0.9998, 0.9911, 0.5171], grad_fn=<SubBackward0>), Loss = 90433536.0\n",
      "tensor([ 11923.4512, 411464.2500,  -2952.0000])\n",
      "Iteration 705: b = tensor([0.9998, 0.9911, 0.5171], grad_fn=<SubBackward0>), Loss = 90433544.0\n",
      "tensor([ 11923.4238, 411462.2500,  -3128.0000])\n",
      "Iteration 706: b = tensor([0.9998, 0.9911, 0.5171], grad_fn=<SubBackward0>), Loss = 90433552.0\n",
      "tensor([ 11923.4258, 411462.6250,  -3080.0000])\n",
      "Iteration 707: b = tensor([0.9998, 0.9911, 0.5171], grad_fn=<SubBackward0>), Loss = 90433536.0\n",
      "tensor([ 11923.4219, 411462.6250,  -3072.0000])\n",
      "Iteration 708: b = tensor([0.9998, 0.9911, 0.5171], grad_fn=<SubBackward0>), Loss = 90433528.0\n",
      "tensor([ 11923.4180, 411462.6250,  -3040.0000])\n",
      "Iteration 709: b = tensor([0.9998, 0.9911, 0.5171], grad_fn=<SubBackward0>), Loss = 90433536.0\n",
      "tensor([ 11923.4209, 411462.7500,  -3024.0000])\n",
      "Iteration 710: b = tensor([0.9998, 0.9911, 0.5171], grad_fn=<SubBackward0>), Loss = 90433544.0\n",
      "tensor([ 11923.4199, 411463.0000,  -3008.0000])\n",
      "Iteration 711: b = tensor([0.9998, 0.9911, 0.5171], grad_fn=<SubBackward0>), Loss = 90433536.0\n",
      "tensor([ 11923.4219, 411463.2500,  -2968.0000])\n",
      "Iteration 712: b = tensor([0.9998, 0.9911, 0.5171], grad_fn=<SubBackward0>), Loss = 90433528.0\n",
      "tensor([ 11923.3896, 411461.1250,  -3112.0000])\n",
      "Iteration 713: b = tensor([0.9998, 0.9911, 0.5171], grad_fn=<SubBackward0>), Loss = 90433520.0\n",
      "tensor([ 11923.3926, 411461.7500,  -3072.0000])\n",
      "Iteration 714: b = tensor([0.9998, 0.9911, 0.5171], grad_fn=<SubBackward0>), Loss = 90433528.0\n",
      "tensor([ 11923.3887, 411461.8750,  -3064.0000])\n",
      "Iteration 715: b = tensor([0.9998, 0.9910, 0.5171], grad_fn=<SubBackward0>), Loss = 90433520.0\n",
      "tensor([ 11923.3848, 411461.6250,  -3064.0000])\n",
      "Iteration 716: b = tensor([0.9998, 0.9910, 0.5171], grad_fn=<SubBackward0>), Loss = 90433520.0\n",
      "tensor([ 11923.3848, 411462.0000,  -3016.0000])\n",
      "Iteration 717: b = tensor([0.9998, 0.9910, 0.5171], grad_fn=<SubBackward0>), Loss = 90433520.0\n",
      "tensor([ 11923.3867, 411462.2500,  -2992.0000])\n",
      "Iteration 718: b = tensor([0.9998, 0.9910, 0.5171], grad_fn=<SubBackward0>), Loss = 90433520.0\n",
      "tensor([ 11923.3857, 411462.2500,  -2976.0000])\n",
      "Iteration 719: b = tensor([0.9998, 0.9910, 0.5171], grad_fn=<SubBackward0>), Loss = 90433504.0\n",
      "tensor([ 11923.3594, 411460.6250,  -3112.0000])\n",
      "Iteration 720: b = tensor([0.9998, 0.9910, 0.5171], grad_fn=<SubBackward0>), Loss = 90433520.0\n",
      "tensor([ 11923.3555, 411460.7500,  -3096.0000])\n",
      "Iteration 721: b = tensor([0.9998, 0.9910, 0.5171], grad_fn=<SubBackward0>), Loss = 90433520.0\n",
      "tensor([ 11923.3535, 411460.7500,  -3080.0000])\n",
      "Iteration 722: b = tensor([0.9998, 0.9910, 0.5171], grad_fn=<SubBackward0>), Loss = 90433512.0\n",
      "tensor([ 11923.3555, 411460.8750,  -3064.0000])\n",
      "Iteration 723: b = tensor([0.9998, 0.9910, 0.5171], grad_fn=<SubBackward0>), Loss = 90433512.0\n",
      "tensor([ 11923.3525, 411461.0000,  -3032.0000])\n",
      "Iteration 724: b = tensor([0.9998, 0.9910, 0.5171], grad_fn=<SubBackward0>), Loss = 90433504.0\n",
      "tensor([ 11923.3545, 411461.2500,  -2976.0000])\n",
      "Iteration 725: b = tensor([0.9998, 0.9910, 0.5171], grad_fn=<SubBackward0>), Loss = 90433512.0\n",
      "tensor([ 11923.3223, 411459.2500,  -3136.0000])\n",
      "Iteration 726: b = tensor([0.9998, 0.9910, 0.5171], grad_fn=<SubBackward0>), Loss = 90433504.0\n",
      "tensor([ 11923.3232, 411459.5000,  -3120.0000])\n",
      "Iteration 727: b = tensor([0.9998, 0.9910, 0.5171], grad_fn=<SubBackward0>), Loss = 90433504.0\n",
      "tensor([ 11923.3232, 411459.5000,  -3096.0000])\n",
      "Iteration 728: b = tensor([0.9998, 0.9910, 0.5171], grad_fn=<SubBackward0>), Loss = 90433504.0\n",
      "tensor([ 11923.3184, 411459.7500,  -3088.0000])\n",
      "Iteration 729: b = tensor([0.9998, 0.9910, 0.5171], grad_fn=<SubBackward0>), Loss = 90433496.0\n",
      "tensor([ 11923.3203, 411460.0000,  -3056.0000])\n",
      "Iteration 730: b = tensor([0.9998, 0.9910, 0.5171], grad_fn=<SubBackward0>), Loss = 90433488.0\n",
      "tensor([ 11923.3164, 411459.8750,  -3040.0000])\n",
      "Iteration 731: b = tensor([0.9998, 0.9910, 0.5171], grad_fn=<SubBackward0>), Loss = 90433488.0\n",
      "tensor([ 11923.3184, 411460.3750,  -3016.0000])\n",
      "Iteration 732: b = tensor([0.9998, 0.9910, 0.5171], grad_fn=<SubBackward0>), Loss = 90433488.0\n",
      "tensor([ 11923.3174, 411460.5000,  -3016.0000])\n",
      "Iteration 733: b = tensor([0.9998, 0.9910, 0.5171], grad_fn=<SubBackward0>), Loss = 90433488.0\n",
      "tensor([ 11923.3145, 411460.7500,  -2960.0000])\n",
      "Iteration 734: b = tensor([0.9998, 0.9910, 0.5171], grad_fn=<SubBackward0>), Loss = 90433488.0\n",
      "tensor([ 11923.2871, 411458.6250,  -3112.0000])\n",
      "Iteration 735: b = tensor([0.9998, 0.9910, 0.5171], grad_fn=<SubBackward0>), Loss = 90433488.0\n",
      "tensor([ 11923.2852, 411458.8750,  -3088.0000])\n",
      "Iteration 736: b = tensor([0.9998, 0.9910, 0.5171], grad_fn=<SubBackward0>), Loss = 90433472.0\n",
      "tensor([ 11923.2832, 411459.1250,  -3056.0000])\n",
      "Iteration 737: b = tensor([0.9998, 0.9910, 0.5171], grad_fn=<SubBackward0>), Loss = 90433472.0\n",
      "tensor([ 11923.2871, 411459.5000,  -3024.0000])\n",
      "Iteration 738: b = tensor([0.9998, 0.9910, 0.5171], grad_fn=<SubBackward0>), Loss = 90433488.0\n",
      "tensor([ 11923.2900, 411459.6250,  -3000.0000])\n",
      "Iteration 739: b = tensor([0.9998, 0.9910, 0.5171], grad_fn=<SubBackward0>), Loss = 90433488.0\n",
      "tensor([ 11923.2871, 411459.7500,  -3000.0000])\n",
      "Iteration 740: b = tensor([0.9998, 0.9909, 0.5171], grad_fn=<SubBackward0>), Loss = 90433488.0\n",
      "tensor([ 11923.2842, 411460.0000,  -2952.0000])\n",
      "Iteration 741: b = tensor([0.9998, 0.9909, 0.5171], grad_fn=<SubBackward0>), Loss = 90433472.0\n",
      "tensor([ 11923.2559, 411458.0000,  -3104.0000])\n",
      "Iteration 742: b = tensor([0.9998, 0.9909, 0.5171], grad_fn=<SubBackward0>), Loss = 90433480.0\n",
      "tensor([ 11923.2529, 411458.1250,  -3080.0000])\n",
      "Iteration 743: b = tensor([0.9998, 0.9909, 0.5171], grad_fn=<SubBackward0>), Loss = 90433472.0\n",
      "tensor([ 11923.2568, 411458.3750,  -3048.0000])\n",
      "Iteration 744: b = tensor([0.9998, 0.9909, 0.5171], grad_fn=<SubBackward0>), Loss = 90433472.0\n",
      "tensor([ 11923.2490, 411458.5000,  -3032.0000])\n",
      "Iteration 745: b = tensor([0.9998, 0.9909, 0.5171], grad_fn=<SubBackward0>), Loss = 90433472.0\n",
      "tensor([ 11923.2510, 411458.6250,  -3000.0000])\n",
      "Iteration 746: b = tensor([0.9998, 0.9909, 0.5171], grad_fn=<SubBackward0>), Loss = 90433472.0\n",
      "tensor([ 11923.2510, 411458.8750,  -2976.0000])\n",
      "Iteration 747: b = tensor([0.9998, 0.9909, 0.5171], grad_fn=<SubBackward0>), Loss = 90433472.0\n",
      "tensor([ 11923.2188, 411457.1250,  -3128.0000])\n",
      "Iteration 748: b = tensor([0.9998, 0.9909, 0.5171], grad_fn=<SubBackward0>), Loss = 90433464.0\n",
      "tensor([ 11923.2188, 411457.0000,  -3104.0000])\n",
      "Iteration 749: b = tensor([0.9998, 0.9909, 0.5171], grad_fn=<SubBackward0>), Loss = 90433464.0\n",
      "tensor([ 11923.2207, 411457.3750,  -3080.0000])\n",
      "Iteration 750: b = tensor([0.9998, 0.9909, 0.5171], grad_fn=<SubBackward0>), Loss = 90433472.0\n",
      "tensor([ 11923.2197, 411457.5000,  -3064.0000])\n",
      "Iteration 751: b = tensor([0.9998, 0.9909, 0.5171], grad_fn=<SubBackward0>), Loss = 90433464.0\n",
      "tensor([ 11923.2207, 411457.8750,  -3024.0000])\n",
      "Iteration 752: b = tensor([0.9998, 0.9909, 0.5171], grad_fn=<SubBackward0>), Loss = 90433456.0\n",
      "tensor([ 11923.2158, 411458.0000,  -3024.0000])\n",
      "Iteration 753: b = tensor([0.9998, 0.9909, 0.5171], grad_fn=<SubBackward0>), Loss = 90433472.0\n",
      "tensor([ 11923.2158, 411458.1250,  -2992.0000])\n",
      "Iteration 754: b = tensor([0.9998, 0.9909, 0.5171], grad_fn=<SubBackward0>), Loss = 90433464.0\n",
      "tensor([ 11923.2148, 411458.2500,  -2960.0000])\n",
      "Iteration 755: b = tensor([0.9998, 0.9909, 0.5171], grad_fn=<SubBackward0>), Loss = 90433456.0\n",
      "tensor([ 11923.1816, 411456.0000,  -3136.0000])\n",
      "Iteration 756: b = tensor([0.9998, 0.9909, 0.5171], grad_fn=<SubBackward0>), Loss = 90433456.0\n",
      "tensor([ 11923.1875, 411456.5000,  -3064.0000])\n",
      "Iteration 757: b = tensor([0.9998, 0.9909, 0.5171], grad_fn=<SubBackward0>), Loss = 90433456.0\n",
      "tensor([ 11923.1875, 411457.0000,  -3024.0000])\n",
      "Iteration 758: b = tensor([0.9998, 0.9909, 0.5171], grad_fn=<SubBackward0>), Loss = 90433456.0\n",
      "tensor([ 11923.1855, 411456.8750,  -3024.0000])\n",
      "Iteration 759: b = tensor([0.9998, 0.9909, 0.5171], grad_fn=<SubBackward0>), Loss = 90433448.0\n",
      "tensor([ 11923.1826, 411457.0000,  -3008.0000])\n",
      "Iteration 760: b = tensor([0.9998, 0.9909, 0.5171], grad_fn=<SubBackward0>), Loss = 90433440.0\n",
      "tensor([ 11923.1826, 411457.1250,  -2960.0000])\n",
      "Iteration 761: b = tensor([0.9998, 0.9909, 0.5171], grad_fn=<SubBackward0>), Loss = 90433440.0\n",
      "tensor([ 11923.1533, 411455.1250,  -3128.0000])\n",
      "Iteration 762: b = tensor([0.9998, 0.9909, 0.5171], grad_fn=<SubBackward0>), Loss = 90433456.0\n",
      "tensor([ 11923.1514, 411455.2500,  -3112.0000])\n",
      "Iteration 763: b = tensor([0.9998, 0.9909, 0.5171], grad_fn=<SubBackward0>), Loss = 90433448.0\n",
      "tensor([ 11923.1494, 411455.3750,  -3096.0000])\n",
      "Iteration 764: b = tensor([0.9998, 0.9908, 0.5171], grad_fn=<SubBackward0>), Loss = 90433440.0\n",
      "tensor([ 11923.1465, 411455.3750,  -3072.0000])\n",
      "Iteration 765: b = tensor([0.9998, 0.9908, 0.5171], grad_fn=<SubBackward0>), Loss = 90433440.0\n",
      "tensor([ 11923.1494, 411455.6250,  -3040.0000])\n",
      "Iteration 766: b = tensor([0.9998, 0.9908, 0.5171], grad_fn=<SubBackward0>), Loss = 90433432.0\n",
      "tensor([ 11923.1514, 411456.1250,  -3000.0000])\n",
      "Iteration 767: b = tensor([0.9998, 0.9908, 0.5171], grad_fn=<SubBackward0>), Loss = 90433432.0\n",
      "tensor([ 11923.1504, 411456.2500,  -2976.0000])\n",
      "Iteration 768: b = tensor([0.9998, 0.9908, 0.5171], grad_fn=<SubBackward0>), Loss = 90433440.0\n",
      "tensor([ 11923.1191, 411454.5000,  -3136.0000])\n",
      "Iteration 769: b = tensor([0.9998, 0.9908, 0.5171], grad_fn=<SubBackward0>), Loss = 90433432.0\n",
      "tensor([ 11923.1201, 411454.6250,  -3112.0000])\n",
      "Iteration 770: b = tensor([0.9998, 0.9908, 0.5171], grad_fn=<SubBackward0>), Loss = 90433440.0\n",
      "tensor([ 11923.1162, 411454.3750,  -3096.0000])\n",
      "Iteration 771: b = tensor([0.9998, 0.9908, 0.5171], grad_fn=<SubBackward0>), Loss = 90433424.0\n",
      "tensor([ 11923.1182, 411454.7500,  -3072.0000])\n",
      "Iteration 772: b = tensor([0.9998, 0.9908, 0.5171], grad_fn=<SubBackward0>), Loss = 90433424.0\n",
      "tensor([ 11923.1182, 411455.2500,  -3032.0000])\n",
      "Iteration 773: b = tensor([0.9998, 0.9908, 0.5171], grad_fn=<SubBackward0>), Loss = 90433416.0\n",
      "tensor([ 11923.1191, 411455.2500,  -3000.0000])\n",
      "Iteration 774: b = tensor([0.9998, 0.9908, 0.5171], grad_fn=<SubBackward0>), Loss = 90433424.0\n",
      "tensor([ 11923.1182, 411455.6250,  -2976.0000])\n",
      "Iteration 775: b = tensor([0.9998, 0.9908, 0.5171], grad_fn=<SubBackward0>), Loss = 90433424.0\n",
      "tensor([ 11923.0859, 411453.5000,  -3136.0000])\n",
      "Iteration 776: b = tensor([0.9998, 0.9908, 0.5171], grad_fn=<SubBackward0>), Loss = 90433424.0\n",
      "tensor([ 11923.0850, 411453.2500,  -3128.0000])\n",
      "Iteration 777: b = tensor([0.9998, 0.9908, 0.5171], grad_fn=<SubBackward0>), Loss = 90433424.0\n",
      "tensor([ 11923.0811, 411453.6250,  -3104.0000])\n",
      "Iteration 778: b = tensor([0.9998, 0.9908, 0.5171], grad_fn=<SubBackward0>), Loss = 90433416.0\n",
      "tensor([ 11923.0840, 411454.0000,  -3048.0000])\n",
      "Iteration 779: b = tensor([0.9998, 0.9908, 0.5171], grad_fn=<SubBackward0>), Loss = 90433408.0\n",
      "tensor([ 11923.0879, 411454.3750,  -3016.0000])\n",
      "Iteration 780: b = tensor([0.9998, 0.9908, 0.5171], grad_fn=<SubBackward0>), Loss = 90433408.0\n",
      "tensor([ 11923.0850, 411454.3750,  -3000.0000])\n",
      "Iteration 781: b = tensor([0.9998, 0.9908, 0.5171], grad_fn=<SubBackward0>), Loss = 90433408.0\n",
      "tensor([ 11923.0811, 411454.5000,  -2984.0000])\n",
      "Iteration 782: b = tensor([0.9998, 0.9908, 0.5171], grad_fn=<SubBackward0>), Loss = 90433408.0\n",
      "tensor([ 11923.0811, 411454.7500,  -2968.0000])\n",
      "Iteration 783: b = tensor([0.9998, 0.9908, 0.5171], grad_fn=<SubBackward0>), Loss = 90433416.0\n",
      "tensor([ 11923.0469, 411452.6250,  -3120.0000])\n",
      "Iteration 784: b = tensor([0.9998, 0.9908, 0.5171], grad_fn=<SubBackward0>), Loss = 90433408.0\n",
      "tensor([ 11923.0527, 411452.8750,  -3072.0000])\n",
      "Iteration 785: b = tensor([0.9998, 0.9908, 0.5171], grad_fn=<SubBackward0>), Loss = 90433408.0\n",
      "tensor([ 11923.0527, 411453.3750,  -3040.0000])\n",
      "Iteration 786: b = tensor([0.9998, 0.9908, 0.5171], grad_fn=<SubBackward0>), Loss = 90433408.0\n",
      "tensor([ 11923.0488, 411453.2500,  -3048.0000])\n",
      "Iteration 787: b = tensor([0.9998, 0.9908, 0.5171], grad_fn=<SubBackward0>), Loss = 90433408.0\n",
      "tensor([ 11923.0498, 411453.7500,  -3008.0000])\n",
      "Iteration 788: b = tensor([0.9998, 0.9907, 0.5171], grad_fn=<SubBackward0>), Loss = 90433408.0\n",
      "tensor([ 11923.0469, 411453.5000,  -2992.0000])\n",
      "Iteration 789: b = tensor([0.9998, 0.9907, 0.5171], grad_fn=<SubBackward0>), Loss = 90433392.0\n",
      "tensor([ 11923.0430, 411453.3750,  -2984.0000])\n",
      "Iteration 790: b = tensor([0.9998, 0.9907, 0.5171], grad_fn=<SubBackward0>), Loss = 90433392.0\n",
      "tensor([ 11923.0439, 411453.8750,  -2952.0000])\n",
      "Iteration 791: b = tensor([0.9998, 0.9907, 0.5171], grad_fn=<SubBackward0>), Loss = 90433392.0\n",
      "tensor([ 11923.0176, 411452.1250,  -3088.0000])\n",
      "Iteration 792: b = tensor([0.9998, 0.9907, 0.5171], grad_fn=<SubBackward0>), Loss = 90433392.0\n",
      "tensor([ 11923.0176, 411452.3750,  -3048.0000])\n",
      "Iteration 793: b = tensor([0.9998, 0.9907, 0.5171], grad_fn=<SubBackward0>), Loss = 90433392.0\n",
      "tensor([ 11923.0127, 411452.2500,  -3048.0000])\n",
      "Iteration 794: b = tensor([0.9998, 0.9907, 0.5171], grad_fn=<SubBackward0>), Loss = 90433392.0\n",
      "tensor([ 11923.0127, 411452.3750,  -3048.0000])\n",
      "Iteration 795: b = tensor([0.9998, 0.9907, 0.5171], grad_fn=<SubBackward0>), Loss = 90433384.0\n",
      "tensor([ 11923.0127, 411452.6250,  -3000.0000])\n",
      "Iteration 796: b = tensor([0.9998, 0.9907, 0.5171], grad_fn=<SubBackward0>), Loss = 90433392.0\n",
      "tensor([ 11923.0127, 411453.1250,  -2936.0000])\n",
      "Iteration 797: b = tensor([0.9998, 0.9907, 0.5171], grad_fn=<SubBackward0>), Loss = 90433376.0\n",
      "tensor([ 11922.9844, 411451.1250,  -3120.0000])\n",
      "Iteration 798: b = tensor([0.9998, 0.9907, 0.5171], grad_fn=<SubBackward0>), Loss = 90433384.0\n",
      "tensor([ 11922.9805, 411451.1250,  -3128.0000])\n",
      "Iteration 799: b = tensor([0.9998, 0.9907, 0.5171], grad_fn=<SubBackward0>), Loss = 90433384.0\n",
      "tensor([ 11922.9814, 411451.1250,  -3088.0000])\n",
      "Iteration 800: b = tensor([0.9998, 0.9907, 0.5171], grad_fn=<SubBackward0>), Loss = 90433384.0\n",
      "tensor([ 11922.9795, 411451.3750,  -3056.0000])\n",
      "Iteration 801: b = tensor([0.9998, 0.9907, 0.5171], grad_fn=<SubBackward0>), Loss = 90433376.0\n",
      "tensor([ 11922.9814, 411451.7500,  -3008.0000])\n",
      "Iteration 802: b = tensor([0.9998, 0.9907, 0.5171], grad_fn=<SubBackward0>), Loss = 90433376.0\n",
      "tensor([ 11922.9844, 411452.2500,  -2960.0000])\n",
      "Iteration 803: b = tensor([0.9998, 0.9907, 0.5171], grad_fn=<SubBackward0>), Loss = 90433376.0\n",
      "tensor([ 11922.9512, 411450.0000,  -3136.0000])\n",
      "Iteration 804: b = tensor([0.9998, 0.9907, 0.5171], grad_fn=<SubBackward0>), Loss = 90433360.0\n",
      "tensor([ 11922.9502, 411449.7500,  -3136.0000])\n",
      "Iteration 805: b = tensor([0.9998, 0.9907, 0.5171], grad_fn=<SubBackward0>), Loss = 90433368.0\n",
      "tensor([ 11922.9492, 411450.0000,  -3120.0000])\n",
      "Iteration 806: b = tensor([0.9998, 0.9907, 0.5171], grad_fn=<SubBackward0>), Loss = 90433368.0\n",
      "tensor([ 11922.9492, 411450.5000,  -3064.0000])\n",
      "Iteration 807: b = tensor([0.9998, 0.9907, 0.5171], grad_fn=<SubBackward0>), Loss = 90433368.0\n",
      "tensor([ 11922.9473, 411450.7500,  -3048.0000])\n",
      "Iteration 808: b = tensor([0.9998, 0.9907, 0.5171], grad_fn=<SubBackward0>), Loss = 90433360.0\n",
      "tensor([ 11922.9463, 411450.7500,  -3032.0000])\n",
      "Iteration 809: b = tensor([0.9998, 0.9907, 0.5171], grad_fn=<SubBackward0>), Loss = 90433360.0\n",
      "tensor([ 11922.9463, 411451.1250,  -2992.0000])\n",
      "Iteration 810: b = tensor([0.9998, 0.9907, 0.5171], grad_fn=<SubBackward0>), Loss = 90433368.0\n",
      "tensor([ 11922.9492, 411451.5000,  -2952.0000])\n",
      "Iteration 811: b = tensor([0.9998, 0.9907, 0.5171], grad_fn=<SubBackward0>), Loss = 90433360.0\n",
      "tensor([ 11922.9189, 411449.5000,  -3112.0000])\n",
      "Iteration 812: b = tensor([0.9998, 0.9907, 0.5171], grad_fn=<SubBackward0>), Loss = 90433352.0\n",
      "tensor([ 11922.9121, 411449.1250,  -3104.0000])\n",
      "Iteration 813: b = tensor([0.9998, 0.9906, 0.5171], grad_fn=<SubBackward0>), Loss = 90433360.0\n",
      "tensor([ 11922.9160, 411449.5000,  -3080.0000])\n",
      "Iteration 814: b = tensor([0.9998, 0.9906, 0.5171], grad_fn=<SubBackward0>), Loss = 90433352.0\n",
      "tensor([ 11922.9160, 411449.8750,  -3032.0000])\n",
      "Iteration 815: b = tensor([0.9998, 0.9906, 0.5171], grad_fn=<SubBackward0>), Loss = 90433352.0\n",
      "tensor([ 11922.9111, 411449.8750,  -3032.0000])\n",
      "Iteration 816: b = tensor([0.9998, 0.9906, 0.5171], grad_fn=<SubBackward0>), Loss = 90433352.0\n",
      "tensor([ 11922.9131, 411450.1250,  -3008.0000])\n",
      "Iteration 817: b = tensor([0.9998, 0.9906, 0.5171], grad_fn=<SubBackward0>), Loss = 90433352.0\n",
      "tensor([ 11922.9062, 411450.2500,  -2992.0000])\n",
      "Iteration 818: b = tensor([0.9998, 0.9906, 0.5171], grad_fn=<SubBackward0>), Loss = 90433344.0\n",
      "tensor([ 11922.9072, 411450.3750,  -2960.0000])\n",
      "Iteration 819: b = tensor([0.9998, 0.9906, 0.5171], grad_fn=<SubBackward0>), Loss = 90433344.0\n",
      "tensor([ 11922.8799, 411448.2500,  -3112.0000])\n",
      "Iteration 820: b = tensor([0.9998, 0.9906, 0.5171], grad_fn=<SubBackward0>), Loss = 90433344.0\n",
      "tensor([ 11922.8838, 411448.8750,  -3048.0000])\n",
      "Iteration 821: b = tensor([0.9998, 0.9906, 0.5171], grad_fn=<SubBackward0>), Loss = 90433344.0\n",
      "tensor([ 11922.8789, 411449.0000,  -3032.0000])\n",
      "Iteration 822: b = tensor([0.9998, 0.9906, 0.5171], grad_fn=<SubBackward0>), Loss = 90433344.0\n",
      "tensor([ 11922.8750, 411448.8750,  -3040.0000])\n",
      "Iteration 823: b = tensor([0.9998, 0.9906, 0.5171], grad_fn=<SubBackward0>), Loss = 90433344.0\n",
      "tensor([ 11922.8740, 411449.0000,  -3024.0000])\n",
      "Iteration 824: b = tensor([0.9998, 0.9906, 0.5171], grad_fn=<SubBackward0>), Loss = 90433336.0\n",
      "tensor([ 11922.8779, 411449.2500,  -3008.0000])\n",
      "Iteration 825: b = tensor([0.9998, 0.9906, 0.5171], grad_fn=<SubBackward0>), Loss = 90433336.0\n",
      "tensor([ 11922.8740, 411449.6250,  -2960.0000])\n",
      "Iteration 826: b = tensor([0.9998, 0.9906, 0.5171], grad_fn=<SubBackward0>), Loss = 90433336.0\n",
      "tensor([ 11922.8506, 411447.8750,  -3088.0000])\n",
      "Iteration 827: b = tensor([0.9998, 0.9906, 0.5171], grad_fn=<SubBackward0>), Loss = 90433344.0\n",
      "tensor([ 11922.8486, 411448.0000,  -3056.0000])\n",
      "Iteration 828: b = tensor([0.9998, 0.9906, 0.5171], grad_fn=<SubBackward0>), Loss = 90433336.0\n",
      "tensor([ 11922.8477, 411448.0000,  -3056.0000])\n",
      "Iteration 829: b = tensor([0.9998, 0.9906, 0.5171], grad_fn=<SubBackward0>), Loss = 90433328.0\n",
      "tensor([ 11922.8438, 411448.2500,  -3048.0000])\n",
      "Iteration 830: b = tensor([0.9998, 0.9906, 0.5171], grad_fn=<SubBackward0>), Loss = 90433328.0\n",
      "tensor([ 11922.8438, 411448.3750,  -3024.0000])\n",
      "Iteration 831: b = tensor([0.9998, 0.9906, 0.5171], grad_fn=<SubBackward0>), Loss = 90433328.0\n",
      "tensor([ 11922.8428, 411448.5000,  -2992.0000])\n",
      "Iteration 832: b = tensor([0.9998, 0.9906, 0.5171], grad_fn=<SubBackward0>), Loss = 90433328.0\n",
      "tensor([ 11922.8447, 411448.6250,  -2968.0000])\n",
      "Iteration 833: b = tensor([0.9998, 0.9906, 0.5171], grad_fn=<SubBackward0>), Loss = 90433328.0\n",
      "tensor([ 11922.8125, 411446.6250,  -3096.0000])\n",
      "Iteration 834: b = tensor([0.9998, 0.9906, 0.5171], grad_fn=<SubBackward0>), Loss = 90433328.0\n",
      "tensor([ 11922.8115, 411446.8750,  -3096.0000])\n",
      "Iteration 835: b = tensor([0.9998, 0.9906, 0.5171], grad_fn=<SubBackward0>), Loss = 90433312.0\n",
      "tensor([ 11922.8115, 411446.7500,  -3096.0000])\n",
      "Iteration 836: b = tensor([0.9998, 0.9906, 0.5171], grad_fn=<SubBackward0>), Loss = 90433328.0\n",
      "tensor([ 11922.8105, 411447.2500,  -3056.0000])\n",
      "Iteration 837: b = tensor([0.9998, 0.9905, 0.5171], grad_fn=<SubBackward0>), Loss = 90433312.0\n",
      "tensor([ 11922.8105, 411447.2500,  -3024.0000])\n",
      "Iteration 838: b = tensor([0.9998, 0.9905, 0.5171], grad_fn=<SubBackward0>), Loss = 90433312.0\n",
      "tensor([ 11922.8066, 411447.3750,  -3016.0000])\n",
      "Iteration 839: b = tensor([0.9998, 0.9905, 0.5171], grad_fn=<SubBackward0>), Loss = 90433312.0\n",
      "tensor([ 11922.8105, 411448.0000,  -2960.0000])\n",
      "Iteration 840: b = tensor([0.9998, 0.9905, 0.5171], grad_fn=<SubBackward0>), Loss = 90433312.0\n",
      "tensor([ 11922.7803, 411445.8750,  -3096.0000])\n",
      "Iteration 841: b = tensor([0.9998, 0.9905, 0.5171], grad_fn=<SubBackward0>), Loss = 90433312.0\n",
      "tensor([ 11922.7793, 411446.1250,  -3088.0000])\n",
      "Iteration 842: b = tensor([0.9998, 0.9905, 0.5171], grad_fn=<SubBackward0>), Loss = 90433312.0\n",
      "tensor([ 11922.7783, 411446.2500,  -3048.0000])\n",
      "Iteration 843: b = tensor([0.9998, 0.9905, 0.5171], grad_fn=<SubBackward0>), Loss = 90433304.0\n",
      "tensor([ 11922.7773, 411446.3750,  -3024.0000])\n",
      "Iteration 844: b = tensor([0.9998, 0.9905, 0.5171], grad_fn=<SubBackward0>), Loss = 90433304.0\n",
      "tensor([ 11922.7754, 411446.5000,  -3000.0000])\n",
      "Iteration 845: b = tensor([0.9998, 0.9905, 0.5171], grad_fn=<SubBackward0>), Loss = 90433296.0\n",
      "tensor([ 11922.7754, 411446.7500,  -3000.0000])\n",
      "Iteration 846: b = tensor([0.9998, 0.9905, 0.5171], grad_fn=<SubBackward0>), Loss = 90433296.0\n",
      "tensor([ 11922.7754, 411446.8750,  -2968.0000])\n",
      "Iteration 847: b = tensor([0.9998, 0.9905, 0.5171], grad_fn=<SubBackward0>), Loss = 90433304.0\n",
      "tensor([ 11922.7461, 411445.1250,  -3112.0000])\n",
      "Iteration 848: b = tensor([0.9998, 0.9905, 0.5171], grad_fn=<SubBackward0>), Loss = 90433304.0\n",
      "tensor([ 11922.7461, 411445.3750,  -3096.0000])\n",
      "Iteration 849: b = tensor([0.9998, 0.9905, 0.5171], grad_fn=<SubBackward0>), Loss = 90433296.0\n",
      "tensor([ 11922.7461, 411445.5000,  -3048.0000])\n",
      "Iteration 850: b = tensor([0.9998, 0.9905, 0.5171], grad_fn=<SubBackward0>), Loss = 90433296.0\n",
      "tensor([ 11922.7461, 411445.7500,  -3024.0000])\n",
      "Iteration 851: b = tensor([0.9998, 0.9905, 0.5171], grad_fn=<SubBackward0>), Loss = 90433280.0\n",
      "tensor([ 11922.7461, 411445.7500,  -3000.0000])\n",
      "Iteration 852: b = tensor([0.9998, 0.9905, 0.5171], grad_fn=<SubBackward0>), Loss = 90433296.0\n",
      "tensor([ 11922.7441, 411445.7500,  -2976.0000])\n",
      "Iteration 853: b = tensor([0.9998, 0.9905, 0.5171], grad_fn=<SubBackward0>), Loss = 90433288.0\n",
      "tensor([ 11922.7158, 411444.2500,  -3136.0000])\n",
      "Iteration 854: b = tensor([0.9998, 0.9905, 0.5171], grad_fn=<SubBackward0>), Loss = 90433288.0\n",
      "tensor([ 11922.7139, 411444.2500,  -3120.0000])\n",
      "Iteration 855: b = tensor([0.9998, 0.9905, 0.5171], grad_fn=<SubBackward0>), Loss = 90433288.0\n",
      "tensor([ 11922.7129, 411444.3750,  -3096.0000])\n",
      "Iteration 856: b = tensor([0.9998, 0.9905, 0.5171], grad_fn=<SubBackward0>), Loss = 90433280.0\n",
      "tensor([ 11922.7158, 411445.0000,  -3032.0000])\n",
      "Iteration 857: b = tensor([0.9998, 0.9905, 0.5171], grad_fn=<SubBackward0>), Loss = 90433288.0\n",
      "tensor([ 11922.7129, 411444.7500,  -3040.0000])\n",
      "Iteration 858: b = tensor([0.9998, 0.9905, 0.5171], grad_fn=<SubBackward0>), Loss = 90433288.0\n",
      "tensor([ 11922.7080, 411445.0000,  -3016.0000])\n",
      "Iteration 859: b = tensor([0.9998, 0.9905, 0.5171], grad_fn=<SubBackward0>), Loss = 90433280.0\n",
      "tensor([ 11922.7061, 411445.0000,  -3008.0000])\n",
      "Iteration 860: b = tensor([0.9998, 0.9905, 0.5171], grad_fn=<SubBackward0>), Loss = 90433280.0\n",
      "tensor([ 11922.7109, 411445.6250,  -2944.0000])\n",
      "Iteration 861: b = tensor([0.9998, 0.9904, 0.5171], grad_fn=<SubBackward0>), Loss = 90433272.0\n",
      "tensor([ 11922.6797, 411443.5000,  -3088.0000])\n",
      "Iteration 862: b = tensor([0.9998, 0.9904, 0.5171], grad_fn=<SubBackward0>), Loss = 90433264.0\n",
      "tensor([ 11922.6787, 411443.6250,  -3064.0000])\n",
      "Iteration 863: b = tensor([0.9998, 0.9904, 0.5171], grad_fn=<SubBackward0>), Loss = 90433264.0\n",
      "tensor([ 11922.6768, 411443.5000,  -3072.0000])\n",
      "Iteration 864: b = tensor([0.9998, 0.9904, 0.5171], grad_fn=<SubBackward0>), Loss = 90433272.0\n",
      "tensor([ 11922.6768, 411443.8750,  -3048.0000])\n",
      "Iteration 865: b = tensor([0.9998, 0.9904, 0.5171], grad_fn=<SubBackward0>), Loss = 90433264.0\n",
      "tensor([ 11922.6729, 411444.0000,  -3008.0000])\n",
      "Iteration 866: b = tensor([0.9998, 0.9904, 0.5171], grad_fn=<SubBackward0>), Loss = 90433264.0\n",
      "tensor([ 11922.6768, 411444.3750,  -2984.0000])\n",
      "Iteration 867: b = tensor([0.9998, 0.9904, 0.5171], grad_fn=<SubBackward0>), Loss = 90433264.0\n",
      "tensor([ 11922.6748, 411444.3750,  -2952.0000])\n",
      "Iteration 868: b = tensor([0.9998, 0.9904, 0.5171], grad_fn=<SubBackward0>), Loss = 90433264.0\n",
      "tensor([ 11922.6445, 411442.3750,  -3120.0000])\n",
      "Iteration 869: b = tensor([0.9998, 0.9904, 0.5171], grad_fn=<SubBackward0>), Loss = 90433264.0\n",
      "tensor([ 11922.6445, 411442.6250,  -3088.0000])\n",
      "Iteration 870: b = tensor([0.9998, 0.9904, 0.5171], grad_fn=<SubBackward0>), Loss = 90433264.0\n",
      "tensor([ 11922.6416, 411442.6250,  -3072.0000])\n",
      "Iteration 871: b = tensor([0.9998, 0.9904, 0.5171], grad_fn=<SubBackward0>), Loss = 90433256.0\n",
      "tensor([ 11922.6445, 411443.0000,  -3048.0000])\n",
      "Iteration 872: b = tensor([0.9998, 0.9904, 0.5171], grad_fn=<SubBackward0>), Loss = 90433264.0\n",
      "tensor([ 11922.6416, 411443.1250,  -3040.0000])\n",
      "Iteration 873: b = tensor([0.9998, 0.9904, 0.5171], grad_fn=<SubBackward0>), Loss = 90433248.0\n",
      "tensor([ 11922.6396, 411443.2500,  -2992.0000])\n",
      "Iteration 874: b = tensor([0.9998, 0.9904, 0.5171], grad_fn=<SubBackward0>), Loss = 90433248.0\n",
      "tensor([ 11922.6426, 411443.6250,  -2960.0000])\n",
      "Iteration 875: b = tensor([0.9998, 0.9904, 0.5171], grad_fn=<SubBackward0>), Loss = 90433256.0\n",
      "tensor([ 11922.6104, 411441.5000,  -3128.0000])\n",
      "Iteration 876: b = tensor([0.9998, 0.9904, 0.5171], grad_fn=<SubBackward0>), Loss = 90433248.0\n",
      "tensor([ 11922.6055, 411441.2500,  -3104.0000])\n",
      "Iteration 877: b = tensor([0.9998, 0.9904, 0.5171], grad_fn=<SubBackward0>), Loss = 90433248.0\n",
      "tensor([ 11922.6084, 411441.5000,  -3072.0000])\n",
      "Iteration 878: b = tensor([0.9998, 0.9904, 0.5171], grad_fn=<SubBackward0>), Loss = 90433248.0\n",
      "tensor([ 11922.6084, 411442.0000,  -3056.0000])\n",
      "Iteration 879: b = tensor([0.9998, 0.9904, 0.5171], grad_fn=<SubBackward0>), Loss = 90433248.0\n",
      "tensor([ 11922.6035, 411441.8750,  -3032.0000])\n",
      "Iteration 880: b = tensor([0.9998, 0.9904, 0.5171], grad_fn=<SubBackward0>), Loss = 90433248.0\n",
      "tensor([ 11922.6035, 411442.2500,  -3008.0000])\n",
      "Iteration 881: b = tensor([0.9998, 0.9904, 0.5171], grad_fn=<SubBackward0>), Loss = 90433240.0\n",
      "tensor([ 11922.6055, 411442.3750,  -2984.0000])\n",
      "Iteration 882: b = tensor([0.9998, 0.9904, 0.5171], grad_fn=<SubBackward0>), Loss = 90433240.0\n",
      "tensor([ 11922.6055, 411442.7500,  -2928.0000])\n",
      "Iteration 883: b = tensor([0.9998, 0.9904, 0.5171], grad_fn=<SubBackward0>), Loss = 90433232.0\n",
      "tensor([ 11922.5781, 411441.0000,  -3080.0000])\n",
      "Iteration 884: b = tensor([0.9998, 0.9904, 0.5171], grad_fn=<SubBackward0>), Loss = 90433232.0\n",
      "tensor([ 11922.5723, 411440.7500,  -3064.0000])\n",
      "Iteration 885: b = tensor([0.9998, 0.9904, 0.5171], grad_fn=<SubBackward0>), Loss = 90433232.0\n",
      "tensor([ 11922.5732, 411441.1250,  -3056.0000])\n",
      "Iteration 886: b = tensor([0.9998, 0.9903, 0.5171], grad_fn=<SubBackward0>), Loss = 90433232.0\n",
      "tensor([ 11922.5752, 411441.5000,  -3008.0000])\n",
      "Iteration 887: b = tensor([0.9998, 0.9903, 0.5171], grad_fn=<SubBackward0>), Loss = 90433232.0\n",
      "tensor([ 11922.5674, 411441.5000,  -2992.0000])\n",
      "Iteration 888: b = tensor([0.9998, 0.9903, 0.5171], grad_fn=<SubBackward0>), Loss = 90433232.0\n",
      "tensor([ 11922.5723, 411441.6250,  -2984.0000])\n",
      "Iteration 889: b = tensor([0.9998, 0.9903, 0.5171], grad_fn=<SubBackward0>), Loss = 90433232.0\n",
      "tensor([ 11922.5723, 411441.8750,  -2968.0000])\n",
      "Iteration 890: b = tensor([0.9998, 0.9903, 0.5171], grad_fn=<SubBackward0>), Loss = 90433224.0\n",
      "tensor([ 11922.5439, 411439.6250,  -3112.0000])\n",
      "Iteration 891: b = tensor([0.9998, 0.9903, 0.5171], grad_fn=<SubBackward0>), Loss = 90433216.0\n",
      "tensor([ 11922.5391, 411439.8750,  -3088.0000])\n",
      "Iteration 892: b = tensor([0.9998, 0.9903, 0.5171], grad_fn=<SubBackward0>), Loss = 90433216.0\n",
      "tensor([ 11922.5381, 411440.1250,  -3040.0000])\n",
      "Iteration 893: b = tensor([0.9998, 0.9903, 0.5171], grad_fn=<SubBackward0>), Loss = 90433224.0\n",
      "tensor([ 11922.5381, 411440.2500,  -3040.0000])\n",
      "Iteration 894: b = tensor([0.9998, 0.9903, 0.5171], grad_fn=<SubBackward0>), Loss = 90433216.0\n",
      "tensor([ 11922.5371, 411440.6250,  -2984.0000])\n",
      "Iteration 895: b = tensor([0.9998, 0.9903, 0.5171], grad_fn=<SubBackward0>), Loss = 90433216.0\n",
      "tensor([ 11922.5361, 411440.6250,  -2984.0000])\n",
      "Iteration 896: b = tensor([0.9998, 0.9903, 0.5171], grad_fn=<SubBackward0>), Loss = 90433216.0\n",
      "tensor([ 11922.5352, 411440.8750,  -2952.0000])\n",
      "Iteration 897: b = tensor([0.9998, 0.9903, 0.5171], grad_fn=<SubBackward0>), Loss = 90433216.0\n",
      "tensor([ 11922.5098, 411439.0000,  -3120.0000])\n",
      "Iteration 898: b = tensor([0.9998, 0.9903, 0.5171], grad_fn=<SubBackward0>), Loss = 90433208.0\n",
      "tensor([ 11922.5039, 411439.0000,  -3088.0000])\n",
      "Iteration 899: b = tensor([0.9998, 0.9903, 0.5171], grad_fn=<SubBackward0>), Loss = 90433208.0\n",
      "tensor([ 11922.5020, 411439.1250,  -3072.0000])\n",
      "Iteration 900: b = tensor([0.9998, 0.9903, 0.5171], grad_fn=<SubBackward0>), Loss = 90433216.0\n",
      "tensor([ 11922.5049, 411439.3750,  -3024.0000])\n",
      "Iteration 901: b = tensor([0.9998, 0.9903, 0.5171], grad_fn=<SubBackward0>), Loss = 90433216.0\n",
      "tensor([ 11922.5029, 411439.7500,  -3008.0000])\n",
      "Iteration 902: b = tensor([0.9998, 0.9903, 0.5171], grad_fn=<SubBackward0>), Loss = 90433200.0\n",
      "tensor([ 11922.5039, 411439.8750,  -2976.0000])\n",
      "Iteration 903: b = tensor([0.9998, 0.9903, 0.5171], grad_fn=<SubBackward0>), Loss = 90433200.0\n",
      "tensor([ 11922.4746, 411437.7500,  -3128.0000])\n",
      "Iteration 904: b = tensor([0.9998, 0.9903, 0.5171], grad_fn=<SubBackward0>), Loss = 90433200.0\n",
      "tensor([ 11922.4697, 411438.1250,  -3104.0000])\n",
      "Iteration 905: b = tensor([0.9998, 0.9903, 0.5171], grad_fn=<SubBackward0>), Loss = 90433192.0\n",
      "tensor([ 11922.4717, 411438.3750,  -3064.0000])\n",
      "Iteration 906: b = tensor([0.9998, 0.9903, 0.5171], grad_fn=<SubBackward0>), Loss = 90433192.0\n",
      "tensor([ 11922.4707, 411438.2500,  -3056.0000])\n",
      "Iteration 907: b = tensor([0.9998, 0.9903, 0.5171], grad_fn=<SubBackward0>), Loss = 90433200.0\n",
      "tensor([ 11922.4727, 411438.7500,  -3016.0000])\n",
      "Iteration 908: b = tensor([0.9998, 0.9903, 0.5171], grad_fn=<SubBackward0>), Loss = 90433192.0\n",
      "tensor([ 11922.4697, 411439.0000,  -3000.0000])\n",
      "Iteration 909: b = tensor([0.9998, 0.9903, 0.5171], grad_fn=<SubBackward0>), Loss = 90433200.0\n",
      "tensor([ 11922.4688, 411438.8750,  -2992.0000])\n",
      "Iteration 910: b = tensor([0.9998, 0.9902, 0.5171], grad_fn=<SubBackward0>), Loss = 90433192.0\n",
      "tensor([ 11922.4678, 411439.0000,  -2992.0000])\n",
      "Iteration 911: b = tensor([0.9998, 0.9902, 0.5171], grad_fn=<SubBackward0>), Loss = 90433192.0\n",
      "tensor([ 11922.4658, 411439.3750,  -2928.0000])\n",
      "Iteration 912: b = tensor([0.9998, 0.9902, 0.5171], grad_fn=<SubBackward0>), Loss = 90433184.0\n",
      "tensor([ 11922.4375, 411437.5000,  -3088.0000])\n",
      "Iteration 913: b = tensor([0.9998, 0.9902, 0.5171], grad_fn=<SubBackward0>), Loss = 90433184.0\n",
      "tensor([ 11922.4385, 411437.5000,  -3048.0000])\n",
      "Iteration 914: b = tensor([0.9998, 0.9902, 0.5171], grad_fn=<SubBackward0>), Loss = 90433184.0\n",
      "tensor([ 11922.4346, 411437.3750,  -3064.0000])\n",
      "Iteration 915: b = tensor([0.9998, 0.9902, 0.5171], grad_fn=<SubBackward0>), Loss = 90433184.0\n",
      "tensor([ 11922.4326, 411437.6250,  -3056.0000])\n",
      "Iteration 916: b = tensor([0.9998, 0.9902, 0.5171], grad_fn=<SubBackward0>), Loss = 90433184.0\n",
      "tensor([ 11922.4434, 411438.5000,  -2968.0000])\n",
      "Iteration 917: b = tensor([0.9998, 0.9902, 0.5171], grad_fn=<SubBackward0>), Loss = 90433176.0\n",
      "tensor([ 11922.4072, 411436.1250,  -3120.0000])\n",
      "Iteration 918: b = tensor([0.9998, 0.9902, 0.5171], grad_fn=<SubBackward0>), Loss = 90433168.0\n",
      "tensor([ 11922.4023, 411436.2500,  -3120.0000])\n",
      "Iteration 919: b = tensor([0.9998, 0.9902, 0.5171], grad_fn=<SubBackward0>), Loss = 90433176.0\n",
      "tensor([ 11922.4004, 411436.1250,  -3120.0000])\n",
      "Iteration 920: b = tensor([0.9998, 0.9902, 0.5171], grad_fn=<SubBackward0>), Loss = 90433184.0\n",
      "tensor([ 11922.4053, 411436.8750,  -3048.0000])\n",
      "Iteration 921: b = tensor([0.9998, 0.9902, 0.5171], grad_fn=<SubBackward0>), Loss = 90433184.0\n",
      "tensor([ 11922.4053, 411437.0000,  -3008.0000])\n",
      "Iteration 922: b = tensor([0.9998, 0.9902, 0.5171], grad_fn=<SubBackward0>), Loss = 90433168.0\n",
      "tensor([ 11922.4004, 411437.1250,  -3016.0000])\n",
      "Iteration 923: b = tensor([0.9998, 0.9902, 0.5171], grad_fn=<SubBackward0>), Loss = 90433168.0\n",
      "tensor([ 11922.3994, 411437.0000,  -2992.0000])\n",
      "Iteration 924: b = tensor([0.9998, 0.9902, 0.5171], grad_fn=<SubBackward0>), Loss = 90433168.0\n",
      "tensor([ 11922.4004, 411437.2500,  -2976.0000])\n",
      "Iteration 925: b = tensor([0.9998, 0.9902, 0.5171], grad_fn=<SubBackward0>), Loss = 90433168.0\n",
      "tensor([ 11922.3701, 411435.5000,  -3120.0000])\n",
      "Iteration 926: b = tensor([0.9998, 0.9902, 0.5171], grad_fn=<SubBackward0>), Loss = 90433160.0\n",
      "tensor([ 11922.3662, 411435.3750,  -3104.0000])\n",
      "Iteration 927: b = tensor([0.9998, 0.9902, 0.5171], grad_fn=<SubBackward0>), Loss = 90433160.0\n",
      "tensor([ 11922.3691, 411435.7500,  -3064.0000])\n",
      "Iteration 928: b = tensor([0.9998, 0.9902, 0.5171], grad_fn=<SubBackward0>), Loss = 90433152.0\n",
      "tensor([ 11922.3691, 411436.1250,  -3016.0000])\n",
      "Iteration 929: b = tensor([0.9998, 0.9902, 0.5171], grad_fn=<SubBackward0>), Loss = 90433152.0\n",
      "tensor([ 11922.3672, 411436.3750,  -3016.0000])\n",
      "Iteration 930: b = tensor([0.9998, 0.9902, 0.5171], grad_fn=<SubBackward0>), Loss = 90433152.0\n",
      "tensor([ 11922.3711, 411436.3750,  -2984.0000])\n",
      "Iteration 931: b = tensor([0.9998, 0.9902, 0.5171], grad_fn=<SubBackward0>), Loss = 90433152.0\n",
      "tensor([ 11922.3652, 411436.6250,  -2952.0000])\n",
      "Iteration 932: b = tensor([0.9998, 0.9902, 0.5171], grad_fn=<SubBackward0>), Loss = 90433152.0\n",
      "tensor([ 11922.3369, 411434.6250,  -3104.0000])\n",
      "Iteration 933: b = tensor([0.9998, 0.9902, 0.5171], grad_fn=<SubBackward0>), Loss = 90433152.0\n",
      "tensor([ 11922.3359, 411434.7500,  -3096.0000])\n",
      "Iteration 934: b = tensor([0.9998, 0.9901, 0.5171], grad_fn=<SubBackward0>), Loss = 90433152.0\n",
      "tensor([ 11922.3359, 411434.7500,  -3056.0000])\n",
      "Iteration 935: b = tensor([0.9998, 0.9901, 0.5171], grad_fn=<SubBackward0>), Loss = 90433152.0\n",
      "tensor([ 11922.3340, 411435.0000,  -3048.0000])\n",
      "Iteration 936: b = tensor([0.9998, 0.9901, 0.5171], grad_fn=<SubBackward0>), Loss = 90433152.0\n",
      "tensor([ 11922.3320, 411435.2500,  -3024.0000])\n",
      "Iteration 937: b = tensor([0.9998, 0.9901, 0.5171], grad_fn=<SubBackward0>), Loss = 90433152.0\n",
      "tensor([ 11922.3311, 411435.2500,  -3008.0000])\n",
      "Iteration 938: b = tensor([0.9998, 0.9901, 0.5171], grad_fn=<SubBackward0>), Loss = 90433144.0\n",
      "tensor([ 11922.3301, 411435.2500,  -2984.0000])\n",
      "Iteration 939: b = tensor([0.9998, 0.9901, 0.5171], grad_fn=<SubBackward0>), Loss = 90433152.0\n",
      "tensor([ 11922.3340, 411435.5000,  -2952.0000])\n",
      "Iteration 940: b = tensor([0.9998, 0.9901, 0.5171], grad_fn=<SubBackward0>), Loss = 90433136.0\n",
      "tensor([ 11922.3027, 411433.8750,  -3096.0000])\n",
      "Iteration 941: b = tensor([0.9998, 0.9901, 0.5171], grad_fn=<SubBackward0>), Loss = 90433144.0\n",
      "tensor([ 11922.3057, 411434.2500,  -3040.0000])\n",
      "Iteration 942: b = tensor([0.9998, 0.9901, 0.5171], grad_fn=<SubBackward0>), Loss = 90433144.0\n",
      "tensor([ 11922.2988, 411434.0000,  -3056.0000])\n",
      "Iteration 943: b = tensor([0.9998, 0.9901, 0.5171], grad_fn=<SubBackward0>), Loss = 90433136.0\n",
      "tensor([ 11922.2988, 411434.0000,  -3048.0000])\n",
      "Iteration 944: b = tensor([0.9998, 0.9901, 0.5171], grad_fn=<SubBackward0>), Loss = 90433128.0\n",
      "tensor([ 11922.2998, 411434.2500,  -3008.0000])\n",
      "Iteration 945: b = tensor([0.9998, 0.9901, 0.5171], grad_fn=<SubBackward0>), Loss = 90433136.0\n",
      "tensor([ 11922.2998, 411434.6250,  -2968.0000])\n",
      "Iteration 946: b = tensor([0.9998, 0.9901, 0.5171], grad_fn=<SubBackward0>), Loss = 90433128.0\n",
      "tensor([ 11922.2705, 411433.1250,  -3104.0000])\n",
      "Iteration 947: b = tensor([0.9998, 0.9901, 0.5171], grad_fn=<SubBackward0>), Loss = 90433128.0\n",
      "tensor([ 11922.2646, 411433.0000,  -3112.0000])\n",
      "Iteration 948: b = tensor([0.9998, 0.9901, 0.5171], grad_fn=<SubBackward0>), Loss = 90433136.0\n",
      "tensor([ 11922.2715, 411433.0000,  -3064.0000])\n",
      "Iteration 949: b = tensor([0.9998, 0.9901, 0.5171], grad_fn=<SubBackward0>), Loss = 90433128.0\n",
      "tensor([ 11922.2646, 411433.1250,  -3056.0000])\n",
      "Iteration 950: b = tensor([0.9998, 0.9901, 0.5171], grad_fn=<SubBackward0>), Loss = 90433136.0\n",
      "tensor([ 11922.2637, 411433.3750,  -3024.0000])\n",
      "Iteration 951: b = tensor([0.9998, 0.9901, 0.5171], grad_fn=<SubBackward0>), Loss = 90433120.0\n",
      "tensor([ 11922.2686, 411433.7500,  -2984.0000])\n",
      "Iteration 952: b = tensor([0.9998, 0.9901, 0.5171], grad_fn=<SubBackward0>), Loss = 90433120.0\n",
      "tensor([ 11922.2646, 411433.8750,  -2960.0000])\n",
      "Iteration 953: b = tensor([0.9998, 0.9901, 0.5171], grad_fn=<SubBackward0>), Loss = 90433112.0\n",
      "tensor([ 11922.2363, 411431.8750,  -3128.0000])\n",
      "Iteration 954: b = tensor([0.9998, 0.9901, 0.5171], grad_fn=<SubBackward0>), Loss = 90433120.0\n",
      "tensor([ 11922.2334, 411432.1250,  -3096.0000])\n",
      "Iteration 955: b = tensor([0.9998, 0.9901, 0.5171], grad_fn=<SubBackward0>), Loss = 90433120.0\n",
      "tensor([ 11922.2314, 411432.1250,  -3072.0000])\n",
      "Iteration 956: b = tensor([0.9998, 0.9901, 0.5171], grad_fn=<SubBackward0>), Loss = 90433120.0\n",
      "tensor([ 11922.2363, 411432.5000,  -3040.0000])\n",
      "Iteration 957: b = tensor([0.9998, 0.9901, 0.5171], grad_fn=<SubBackward0>), Loss = 90433120.0\n",
      "tensor([ 11922.2354, 411432.7500,  -3000.0000])\n",
      "Iteration 958: b = tensor([0.9998, 0.9901, 0.5171], grad_fn=<SubBackward0>), Loss = 90433104.0\n",
      "tensor([ 11922.2314, 411432.8750,  -3000.0000])\n",
      "Iteration 959: b = tensor([0.9998, 0.9900, 0.5171], grad_fn=<SubBackward0>), Loss = 90433112.0\n",
      "tensor([ 11922.2324, 411432.7500,  -2976.0000])\n",
      "Iteration 960: b = tensor([0.9998, 0.9900, 0.5171], grad_fn=<SubBackward0>), Loss = 90433112.0\n",
      "tensor([ 11922.2012, 411431.0000,  -3128.0000])\n",
      "Iteration 961: b = tensor([0.9998, 0.9900, 0.5171], grad_fn=<SubBackward0>), Loss = 90433112.0\n",
      "tensor([ 11922.1992, 411431.2500,  -3104.0000])\n",
      "Iteration 962: b = tensor([0.9998, 0.9900, 0.5171], grad_fn=<SubBackward0>), Loss = 90433112.0\n",
      "tensor([ 11922.2021, 411431.3750,  -3072.0000])\n",
      "Iteration 963: b = tensor([0.9998, 0.9900, 0.5171], grad_fn=<SubBackward0>), Loss = 90433104.0\n",
      "tensor([ 11922.2002, 411431.5000,  -3056.0000])\n",
      "Iteration 964: b = tensor([0.9998, 0.9900, 0.5171], grad_fn=<SubBackward0>), Loss = 90433096.0\n",
      "tensor([ 11922.1992, 411431.7500,  -3024.0000])\n",
      "Iteration 965: b = tensor([0.9998, 0.9900, 0.5171], grad_fn=<SubBackward0>), Loss = 90433096.0\n",
      "tensor([ 11922.1963, 411432.1250,  -2984.0000])\n",
      "Iteration 966: b = tensor([0.9998, 0.9900, 0.5171], grad_fn=<SubBackward0>), Loss = 90433088.0\n",
      "tensor([ 11922.1973, 411432.1250,  -2976.0000])\n",
      "Iteration 967: b = tensor([0.9998, 0.9900, 0.5171], grad_fn=<SubBackward0>), Loss = 90433088.0\n",
      "tensor([ 11922.1650, 411430.0000,  -3136.0000])\n",
      "Iteration 968: b = tensor([0.9998, 0.9900, 0.5171], grad_fn=<SubBackward0>), Loss = 90433096.0\n",
      "tensor([ 11922.1680, 411430.5000,  -3072.0000])\n",
      "Iteration 969: b = tensor([0.9998, 0.9900, 0.5171], grad_fn=<SubBackward0>), Loss = 90433088.0\n",
      "tensor([ 11922.1650, 411430.5000,  -3064.0000])\n",
      "Iteration 970: b = tensor([0.9998, 0.9900, 0.5171], grad_fn=<SubBackward0>), Loss = 90433088.0\n",
      "tensor([ 11922.1631, 411430.6250,  -3064.0000])\n",
      "Iteration 971: b = tensor([0.9998, 0.9900, 0.5171], grad_fn=<SubBackward0>), Loss = 90433080.0\n",
      "tensor([ 11922.1680, 411431.0000,  -3016.0000])\n",
      "Iteration 972: b = tensor([0.9998, 0.9900, 0.5171], grad_fn=<SubBackward0>), Loss = 90433080.0\n",
      "tensor([ 11922.1631, 411431.1250,  -3024.0000])\n",
      "Iteration 973: b = tensor([0.9998, 0.9900, 0.5171], grad_fn=<SubBackward0>), Loss = 90433088.0\n",
      "tensor([ 11922.1660, 411431.3750,  -2968.0000])\n",
      "Iteration 974: b = tensor([0.9998, 0.9900, 0.5171], grad_fn=<SubBackward0>), Loss = 90433088.0\n",
      "tensor([ 11922.1367, 411429.2500,  -3120.0000])\n",
      "Iteration 975: b = tensor([0.9998, 0.9900, 0.5171], grad_fn=<SubBackward0>), Loss = 90433088.0\n",
      "tensor([ 11922.1328, 411429.5000,  -3112.0000])\n",
      "Iteration 976: b = tensor([0.9998, 0.9900, 0.5171], grad_fn=<SubBackward0>), Loss = 90433072.0\n",
      "tensor([ 11922.1338, 411430.1250,  -3040.0000])\n",
      "Iteration 977: b = tensor([0.9998, 0.9900, 0.5171], grad_fn=<SubBackward0>), Loss = 90433072.0\n",
      "tensor([ 11922.1328, 411429.8750,  -3064.0000])\n",
      "Iteration 978: b = tensor([0.9998, 0.9900, 0.5171], grad_fn=<SubBackward0>), Loss = 90433072.0\n",
      "tensor([ 11922.1309, 411429.8750,  -3048.0000])\n",
      "Iteration 979: b = tensor([0.9998, 0.9900, 0.5171], grad_fn=<SubBackward0>), Loss = 90433080.0\n",
      "tensor([ 11922.1299, 411430.1250,  -3024.0000])\n",
      "Iteration 980: b = tensor([0.9998, 0.9900, 0.5171], grad_fn=<SubBackward0>), Loss = 90433072.0\n",
      "tensor([ 11922.1289, 411430.3750,  -2984.0000])\n",
      "Iteration 981: b = tensor([0.9998, 0.9900, 0.5171], grad_fn=<SubBackward0>), Loss = 90433080.0\n",
      "tensor([ 11922.1270, 411430.5000,  -2976.0000])\n",
      "Iteration 982: b = tensor([0.9998, 0.9900, 0.5171], grad_fn=<SubBackward0>), Loss = 90433072.0\n",
      "tensor([ 11922.1016, 411428.6250,  -3112.0000])\n",
      "Iteration 983: b = tensor([0.9998, 0.9899, 0.5171], grad_fn=<SubBackward0>), Loss = 90433072.0\n",
      "tensor([ 11922.0996, 411429.0000,  -3072.0000])\n",
      "Iteration 984: b = tensor([0.9998, 0.9899, 0.5171], grad_fn=<SubBackward0>), Loss = 90433072.0\n",
      "tensor([ 11922.0938, 411428.8750,  -3080.0000])\n",
      "Iteration 985: b = tensor([0.9998, 0.9899, 0.5171], grad_fn=<SubBackward0>), Loss = 90433064.0\n",
      "tensor([ 11922.0957, 411429.1250,  -3040.0000])\n",
      "Iteration 986: b = tensor([0.9998, 0.9899, 0.5171], grad_fn=<SubBackward0>), Loss = 90433056.0\n",
      "tensor([ 11922.1016, 411429.6250,  -3000.0000])\n",
      "Iteration 987: b = tensor([0.9998, 0.9899, 0.5171], grad_fn=<SubBackward0>), Loss = 90433056.0\n",
      "tensor([ 11922.0967, 411429.5000,  -2976.0000])\n",
      "Iteration 988: b = tensor([0.9998, 0.9899, 0.5171], grad_fn=<SubBackward0>), Loss = 90433056.0\n",
      "tensor([ 11922.0703, 411427.5000,  -3120.0000])\n",
      "Iteration 989: b = tensor([0.9998, 0.9899, 0.5171], grad_fn=<SubBackward0>), Loss = 90433048.0\n",
      "tensor([ 11922.0674, 411427.7500,  -3104.0000])\n",
      "Iteration 990: b = tensor([0.9998, 0.9899, 0.5171], grad_fn=<SubBackward0>), Loss = 90433056.0\n",
      "tensor([ 11922.0654, 411427.8750,  -3088.0000])\n",
      "Iteration 991: b = tensor([0.9998, 0.9899, 0.5171], grad_fn=<SubBackward0>), Loss = 90433048.0\n",
      "tensor([ 11922.0645, 411428.2500,  -3040.0000])\n",
      "Iteration 992: b = tensor([0.9998, 0.9899, 0.5171], grad_fn=<SubBackward0>), Loss = 90433048.0\n",
      "tensor([ 11922.0645, 411428.6250,  -3008.0000])\n",
      "Iteration 993: b = tensor([0.9998, 0.9899, 0.5171], grad_fn=<SubBackward0>), Loss = 90433048.0\n",
      "tensor([ 11922.0625, 411428.3750,  -2992.0000])\n",
      "Iteration 994: b = tensor([0.9998, 0.9899, 0.5171], grad_fn=<SubBackward0>), Loss = 90433040.0\n",
      "tensor([ 11922.0654, 411428.6250,  -2960.0000])\n",
      "Iteration 995: b = tensor([0.9998, 0.9899, 0.5171], grad_fn=<SubBackward0>), Loss = 90433040.0\n",
      "tensor([ 11922.0332, 411426.6250,  -3152.0000])\n",
      "Iteration 996: b = tensor([0.9998, 0.9899, 0.5171], grad_fn=<SubBackward0>), Loss = 90433048.0\n",
      "tensor([ 11922.0352, 411427.1250,  -3088.0000])\n",
      "Iteration 997: b = tensor([0.9998, 0.9899, 0.5171], grad_fn=<SubBackward0>), Loss = 90433048.0\n",
      "tensor([ 11922.0312, 411427.2500,  -3080.0000])\n",
      "Iteration 998: b = tensor([0.9998, 0.9899, 0.5171], grad_fn=<SubBackward0>), Loss = 90433040.0\n",
      "tensor([ 11922.0312, 411427.2500,  -3056.0000])\n",
      "Iteration 999: b = tensor([0.9998, 0.9899, 0.5171], grad_fn=<SubBackward0>), Loss = 90433040.0\n",
      "tensor([ 11922.0312, 411427.3750,  -3008.0000])\n",
      "Iteration 1000: b = tensor([0.9998, 0.9899, 0.5171], grad_fn=<SubBackward0>), Loss = 90433040.0\n",
      "tensor([ 11922.0303, 411427.5000,  -3008.0000])\n",
      "Optimal b: tensor([0.9998, 0.9899, 0.5171], grad_fn=<SubBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# Helper function for finding the optimal b\n",
    "def grad_descent(y, b, lr=0.00000000001):  # Reduced learning rate\n",
    "    grad = torch.autograd.grad(y, b)[0]\n",
    "\n",
    "    return b - lr * grad, grad\n",
    "\n",
    "# Helper function for representing the objective function\n",
    "def obj_fun(b):\n",
    "    # Mean squared error\n",
    "    y_pred = X_train[:, 0] * b[1] + X_train[:, 1] * X_train[:, 0] * b[2] + b[0]  # Linear regression equation with intercept and slope\n",
    "\n",
    "    return torch.sum((y_pred - y_train) ** 2)\n",
    "\n",
    "# Initial guess for b\n",
    "b = torch.tensor([1.0, 1.0, 1.0], requires_grad=True)\n",
    "\n",
    "# Optimization loop\n",
    "for i in range(1000):  # Increased the number of iterations for better convergence\n",
    "    loss = obj_fun(b)\n",
    "    b, grad = grad_descent(loss, b)\n",
    "    print(f\"Iteration {i+1}: b = {b}, Loss = {loss.item()}\")\n",
    "    print(grad)\n",
    "\n",
    "# Final result\n",
    "print(\"Optimal b:\", b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the regression function\n",
    "\n",
    "def regression_line(b, x_values):\n",
    "\n",
    "    y_pred = []\n",
    "\n",
    "    for x in x_values:\n",
    "        y_pred.append(float(b[0]) + float(b[1])*x[0] + float(b[1])*x[0]*x[1])\n",
    "\n",
    "    return y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Call the function and get y_pred\n",
    "\n",
    "y_pred  = regression_line(b, X_train)\n",
    "\n",
    "plt.plot(X_train, y_train, \"o\", color=\"blue\")\n",
    "plt.plot(X_train, y_pred, \"o\", color=\"red\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYQAAAD4CAYAAADsKpHdAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjYuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8o6BhiAAAACXBIWXMAAAsTAAALEwEAmpwYAAAXWElEQVR4nO3dbYxkV3ng8f/jahsHosU2jCwy492ZiJGjcaQE03KMiLLI9oJxoow/oJV3ozDKWmptt0UgipTY2Q9odxNpkaIYCK4xvXZYgyIMcdDaYlGQ15is8gFDD86yzBjHAyx4RjbuxC+JslJw9zz7oU57yu2q7lvV9Xrv/yeVqu6pW1X39J25z73nOefcyEwkSbpg2hsgSZoNBgRJEmBAkCQVBgRJEmBAkCQVC9PegJ28+c1vzoMHD057MyRprpw4ceJvM3PfoJ+b6YBw8OBB1tbWpr0ZkjRXIuIHw3zOJiNJEmBAkCQVBgRJEmBAkCQVBgRJEmBAkKTZsrICCwsQ0XleWZnYT890t1NJapSVFTh+/Pzy5ub55XZ77D/vFYIkzYrV1cHKR8yAIEmzYnNzsPIRMyBI0qxotQYrHzEDgiTNiqWlwcpHzKSyJM2KrcTx6mqnmajV6gSDCSSUwYAgSbOl3Z5YANjOJiNJEmBAkCQVBgRJEmBAkCQVBgRJEmBAkCQVBgRJEmBAkCQVBgRJEmBAkCQVBgRJEmBAkCQVBgRJEmBAkCQVBgRJEmBAkCQVBgRJ2quVFVhYgIjO88rKtLdoKN4xTZL2YmUFjh8/v7y5eX55Snc+G1alK4SI+K2IOBkR346Iz0bExRFxKCIei4jTEfG5iLiorPu6sny6vH+w63vuKOVPRsR7xlQnSZqc1dXBymfYrgEhIvYDvwksZubPAi3gFuAjwJ2Z+VbgBeDW8pFbgRdK+Z1lPSLiSPncVcCNQDsiWqOtjiRN2ObmYOUzrGoOYQH4iYhYAF4PPANcBzxQ3r8PuLm8PlqWKe9fHxFRyu/PzH/KzO8Dp4Fr9lwDSZqmVp/z2n7lM2zXgJCZZ4E/BH5IJxC8BJwAXszMjbLaGWB/eb0feLp8dqOs/6bu8h6feUVELEXEWkSsra+vD1MnSZqcpaXBymdYlSajS+mc3R8Cfgp4A50mn7HIzNXMXMzMxX379o3rZyRpNNptWF4+f0XQanWW5yyhDNWajG4Avp+Z65n5MvAF4J3AJaUJCeAAcLa8PgtcAVDefyPwd93lPT4jSfOr3YaNDcjsPM9hMIBqAeGHwLUR8fqSC7geOAU8CryvrHMMeLC8fqgsU97/SmZmKb+l9EI6BBwGvj6aakjSBNRkvEE/u45DyMzHIuIB4JvABvA4sAr8D+D+iPj9UnZv+ci9wGci4jTwPJ2eRWTmyYj4PJ1gsgHclpnzl4aX1Ew1Gm/QT3RO3mfT4uJirq2tTXszJKlzRdCrK2mr1WkmmiERcSIzFwf9nFNXSFIVNRpv0I8BQZKqqNF4g34MCJJURY3GG/Tj5HaSVMVW4nh1tdNM1Gp1gkFNEspgQJCk6trtWgWA7WwykiQBBgRJUmFAkCQBBgRJUmFAkCQBBgRJUmFAkCQBBgRJUmFAkCQBBgRJUmFAkNQcNb/j2V45l5GkZmjAHc/2yisESc2wujpYeQMZECQ1QwPueLZXBgRJzdCAO57tlQFBUjM04I5ne2VSWVIzNOCOZ3tlQJDUHDW/49le2WQkSQIMCJKkwoAgaf45AnkkzCFImm+OQB4ZrxAkzTdHII+MAUHSfHME8sgYECTNN0cgj4wBQdJ8cwTyyJhUljTfHIE8MgYESfPPEcgjYZORJAmoGBAi4pKIeCAivhMRT0TEOyLisoh4OCKeKs+XlnUjIj4eEacj4lsRcXXX9xwr6z8VEcfGVSlJ0uCqXiF8DPiLzPwZ4OeAJ4DbgUcy8zDwSFkGeC9wuDyWgOMAEXEZ8GHgF4BrgA9vBRFJ6stRyBOza0CIiDcCvwTcC5CZP87MF4GjwH1ltfuAm8vro8Cns+NrwCUR8RbgPcDDmfl8Zr4APAzcOMK6SKqbrVHIW2MKtkYhGxTGosoVwiFgHfhURDweEfdExBuAyzPzmbLOs8Dl5fV+4Omuz58pZf3KXyUiliJiLSLW1tfXB6uNpHpxFPJEVQkIC8DVwPHMfBvwj5xvHgIgMxPIUWxQZq5m5mJmLu7bt28UXylpXjkKeaKqBIQzwJnMfKwsP0AnQPyoNAVRnp8r758Fruj6/IFS1q9cks7rzhn0s20UsmmG0dg1IGTms8DTEXFlKboeOAU8BGz1FDoGPFhePwS8v/Q2uhZ4qTQtfRl4d0RcWpLJ7y5lktSxPWfQT9coZNMMoxOd1p5dVor4eeAe4CLge8Bv0Akmnwf+OfAD4F9n5vMREcAn6CSM/x/wG5m5Vr7n3wG/V772DzLzUzv97uLiYq6trQ1RLUlzaWFh52DQYxRyv4+0WrCxMYZtnAMRcSIzFwf+XJWAMC0GBKlhdmom6nOsGuIjtTdsQHCksqTZMcTMpU52OjoGBEmzY4iZS53sdHSc3E7S7Bhi5lInOx0dcwiSVDPmECSpBqY5psKAIGlyHEG2o2mPqTAgSJqMaR/txmhUcW7aUzcZECRNxrSPdmMyyjg37ambDAiSxmvr9HnaR7sxGWWcm/aYCgOCpPGpMjfRnI8gG2Wcm/aYCschSBqfKqfJcz6CrNXqP5fSoKY9psIrBEnjs9uVwfLy3I8gG/VZfbvdmZQvs/M8yT+PVwiSxmen0+eaTEU67bP6UfIKQdL4TLtRfEKmeVY/SgYESXvXryN+u91pFtpqUK9JM1FdOZeRpL3Z6km0nQf+qXEuI0nTUdMBZ01kQJA0nJoPOGsiexlJGly/ZqJucz7grIm8QpA0uF2agxJq15OoCQwIkgbXpzkogQ1a3MUyK5hQnjc2GUkaXJ8BZ5u0uJDOgLPWqp2M5o1XCJIG16M5KIG7OV9uTnn+eIUgaXBd8zXk5iabtLibJT7Q1UxkTnn+eIUgaThlvobblpML2XhVMABzyvPIKwRJe1Knyd2azoAgac/abQNAHdhkJEkCDAiSpMKAIEkCDAiSpMKAIEkCDAiSpMKAIEkCBggIEdGKiMcj4otl+VBEPBYRpyPicxFxUSl/XVk+Xd4/2PUdd5TyJyPiPSOvjSRpaINcIXwQeKJr+SPAnZn5VuAF4NZSfivwQim/s6xHRBwBbgGuAm4E2hHhbCeSNCMqBYSIOAD8MnBPWQ7gOuCBssp9wM3l9dGyTHn/+rL+UeD+zPynzPw+cBq4ZgR1kCSNQNUrhI8CvwOcK8tvAl7MzI2yfAbYX17vB54GKO+/VNZ/pbzHZ14REUsRsRYRa+vr69VrIknak10DQkT8CvBcZp6YwPaQmauZuZiZi/v27ZvET0qSqDa53TuBX42Im4CLgX8GfAy4JCIWylXAAeBsWf8scAVwJiIWgDcCf9dVvqX7M5KkKdv1CiEz78jMA5l5kE5S+CuZ+WvAo8D7ymrHgAfL64fKMuX9r2RmlvJbSi+kQ8Bh4Osjq4mkWlhZgYUFiOg8r6xMe4uaYy/TX/8ucH9E/D7wOHBvKb8X+ExEnAaepxNEyMyTEfF54BSwAdyWmd5kT9IrVlbg+PHzy5ub55edXnv8onPyPpsWFxdzbW1t2pshaUIWFnrfi7nVgo2N15art4g4kZmLg37OkcqSZkavYLBTuUbLgCBpZrT6DFXtVz5pdc9vGBAkzYylpcHKJ2krv7F1tbKV36hTUDAgSJoZ7TYsL5+/Imi1OsuzkFBeXR2sfB6ZVJakCiL6vzdrh1GTypI0RrOe3xgFA4IkVTDL+Y1R2cvANElqjK08xupqJ6HcanWCwSzkN0bFgCBJFbXb9QoA29lkJEkCDAiSpMKAIGli6j7Sd94ZEKRZUfOjZRNG+s47A4I0CxpwtGzCSN95Z0CQZkEDjpbOZDr7DAjSLGjA0bIJI33nnQFBmrReuYIGHC2bMNJ33hkQpEnqlyu48sre69foaDnLM5mqw9lOpUna6R6RS0v1nhdBEzPsbKcGBGmS5mkOZc0tp7+W5kEDcgWaXwYEaZKWlth+HZClXJo2A4I0QSu0uYtlNmiRwAYt7mKZFcwVaPrMIUgTtFNOeWNj8tujejKHIM2BBow/0xwzIEgTZE55umo+f+CeGRCkCXK07vQ0YP7APTMgSBPkaN3pacD8gXtmUllSIzRpTKBJZUnagfmb3RkQJM20USWCzd/sbmHaGyBJ/WwlgrdsJYJh8LzL1vrOH9ifOQRJM8uBfMMxhyDtlZ3UBzKJP5cD+SZr14AQEVdExKMRcSoiTkbEB0v5ZRHxcEQ8VZ4vLeURER+PiNMR8a2IuLrru46V9Z+KiGPjq5Y0IDupD2RSfy4TwZNV5QphA/jtzDwCXAvcFhFHgNuBRzLzMPBIWQZ4L3C4PJaA49AJIMCHgV8ArgE+vBVEpKnZOs3tbqjuZif1nibVp99E8GTtGhAy85nM/GZ5/Q/AE8B+4ChwX1ntPuDm8voo8Ons+BpwSUS8BXgP8HBmPp+ZLwAPAzeOsjLSQLaf5vZi20RPk2rKcSDfZA2UQ4iIg8DbgMeAyzPzmfLWs8Dl5fV+4Omuj50pZf3Kt//GUkSsRcTa+vr6IJsnDabK6axtEz1Nsimn3e4kkDM7zwaD8akcECLiJ4E/Bz6UmX/f/V52uiqNpLtSZq5m5mJmLu7bt28UXyn1VuV01raJnmzKqadKASEiLqQTDP40M79Qin9UmoIoz8+V8rPAFV0fP1DK+pVL49erS8xOp7O2TezIppx6qtLLKIB7gScy84+63noI2OopdAx4sKv8/aW30bXAS6Vp6cvAuyPi0pJMfncpk8arX5eYK6/svf7ysm0TFdiUUz9VrhDeCfw6cF1E/HV53AT8F+BfRcRTwA1lGeBLwPeA08B/BVYAMvN54D8D3yiP/1TKpPHqlyt48klPc6UujlRWfays9J6XoEnTXEoMP1LZuYxUDztNetNq9Z//QNIrnLpC863KwDK7xEiVeIWg+bX9qqCXzU2nuZQqMoeg+dVvKsxuToupBnK2UzWPA8ukkTIgaH45sEwaKQOC5le/s38HlklDMSBofjl/wlzwvkPzw6SypLHp1xHMuD1eJpUlzZxJ3UhHo2FA0OTZhtAY3hN5vhgQNFneu7hRvCfyfDEgaLJsQ2gUZw2ZLwYEjU+vpiHbEBrFjmDzxV5GGq3uKagH4RQT0sg4/bWmr8pkcxFkJt13KEggbEOQps4mI41OhTxAZnIXy2zQIoENWtzFMivYhiBNmwFBo1OhmWiTFh+gzYVscAHJhWzwAdrmlLE3rqbPgKDRqdCX8G56Nw01Padsb1zNAgOC+hv0lHWnPEDpXvKhVu+moab3S7c3rmaBAUG9rayQ205Zc7dT1n59DDNfmX3Ufum92RtXs8Bup+pp84IFWvnao9FmtGid21v30O6eqd7NsqPfzd/sjathOLmdRuqCHsFgp/JBtNudg1zXhUPjeeWkWWBAaIoB8wGb9G7U71c+LVWrNes9eBzRq5mQmTP7ePvb354ageXlzM4J+asfy8t9P/IJlvPctvXPQX6C/p+ZtKrVGqL60lwD1nKIY645hCYYooF6ZQWOHF/h37NKi002aXE3S5xabs/MWWvVatk+r6Yxh6D+hujC0m7DqeU2F7c6A8gubm3MVDCA6tWyB49UjQFhXg3SKD7kpPSznvytWi3n5JeqMSDMo0GHtda0C0vVatW0+tLoDZN4mNSjkUnl5eXMVquT9Wy1emc+t97f/mi1+n7to0eW82VaeQ7yZVr56JF6ZFSr/Lmqrlf1u6RZx5BJ5akf9Hd6NC4gVO0O02udrccevnZUVZjWQXUvv21PJNWJAWEe7HbEqnrmP+AVwhAXFENXb1oH1b3+9qT+RtIkDBsQzCGMwFevWmEjFsgIzsUFnIt4bbK3ytxAFbvDfPXKJbZ3Fs5SXuHju5YPa5oTtA3721u5eXsiSSaVO/YwjPWrV63wL08dZ4FNAriArijbddDfvHv1VXcJAwhg8+6uI1bF7jA3PNnueZOZG54cbCbRUfey2emgOu7RwcMc0Lfn5nuxJ5IaZZjLir08gBuBJ4HTwO07rTtsk9FACdTl3iNyq7Y1vEyftoaux0a0XvMbr/qtru3utS3bt3/AFMLEmnL6NbtMovlomCafaW6vNE7MQw4BaAHfBX4auAj438CRfusPExCqHlS3bETvo8JGVGs87neg3/77/QLHy5z/nVYr8495dTD7Y5b3mkLIzMkke/sFnkm0yQ8T9HbbToOB5tW8BIR3AF/uWr4DuKPf+sMEhCoH3m5VztyH+b3tv11lbqCqZ/6z3COmO/AMchUz6t+uckA3kay6GjYgTDqHsB94umv5TCl7RUQsRcRaRKytr68P/AMtejcI9yvf66yef3XktQnebgl8kiVOLvdu9z+5fL7dv2pb/yzPjLk1unkao4MHHVntgDXp1WYuqZyZq5m5mJmL+/btG/jzgx7gP0nvHjuf7HPv3+3edbLNXx45f6A/R3CufEf3Qb/K3ECDHKBmfVqJeTjYznJglaZimMuKYR9MoMlo0BzC8nLvdvthm1/22lZfp9GydaqLNE+Yh+mvI2IB+BvgeuAs8A3g32bmyV7rDzv99VevWuEXT52ftvmvjizxrpP9T/u8paOkOhl2+uuJ3w8hIm4CPkqnx9GfZOYf9FvX+yFI0uCGDQgL49iYnWTml4AvTfp3JUk7m7mksiRpOgwIkiTAgCBJKgwIkiRgCr2MBhER68AP9vAVbwb+dkSbM2+se3M1uf5Nrjucr/+/yMyBR/bOdEDYq4hYG6brVR1Y92bWHZpd/ybXHfZef5uMJEmAAUGSVNQ9IEzg5o0zy7o3V5Pr3+S6wx7rX+scgiSpurpfIUiSKjIgSJKAmgaEiLgxIp6MiNMRcfu0t2ecIuKKiHg0Ik5FxMmI+GApvywiHo6Ip8rzpdPe1nGKiFZEPB4RXyzLhyLisfJv4HMRcdG0t3EcIuKSiHggIr4TEU9ExDuatO8j4rfKv/tvR8RnI+Liuu77iPiTiHguIr7dVdZzX0fHx8vf4FsRcXWV36hdQIiIFnAX8F7gCPBvIuLIdLdqrDaA387MI8C1wG2lvrcDj2TmYeCRslxnHwSe6Fr+CHBnZr4VeAG4dSpbNX4fA/4iM38G+Dk6f4NG7PuI2A/8JrCYmT9LZ0r9W6jvvv9vwI3byvrt6/cCh8tjCThe5QdqFxCAa4DTmfm9zPwxcD9wdMrbNDaZ+UxmfrO8/gc6B4T9dOp8X1ntPuDmqWzgBETEAeCXgXvKcgDXAQ+UVWpZ/4h4I/BLwL0AmfnjzHyRBu17OlP4/0S5+dbrgWeo6b7PzP8FPL+tuN++Pgp8utxA7WvAJRHxlt1+o44BYT/wdNfymVJWexFxEHgb8BhweWY+U956Frh8Wts1AR8Ffgc4V5bfBLyYmRtlua7/Bg4B68CnSnPZPRHxBhqy7zPzLPCHwA/pBIKXgBM0Y99v6bevhzoO1jEgNFJE/CTw58CHMvPvu98r91itZf/iiPgV4LnMPDHtbZmCBeBq4Hhmvg34R7Y1D9V8319K50z4EPBTwBt4bZNKY4xiX9cxIJwFruhaPlDKaisiLqQTDP40M79Qin+0dYlYnp+b1vaN2TuBX42I/0unefA6Ou3ql5RmBKjvv4EzwJnMfKwsP0AnQDRl398AfD8z1zPzZeALdP49NGHfb+m3r4c6DtYxIHwDOFx6GlxEJ8n00JS3aWxKe/m9wBOZ+Uddbz0EHCuvjwEPTnrbJiEz78jMA5l5kM6+/kpm/hrwKPC+slot65+ZzwJPR8SVpeh64BQN2fd0moqujYjXl/8HW/Wv/b7v0m9fPwS8v/Q2uhZ4qatpqb/MrN0DuAn4G+C7wH+Y9vaMua6/SOcy8VvAX5fHTXTa0R8BngL+J3DZtLd1An+LdwFfLK9/Gvg6cBr4M+B1096+MdX554G1sv//O3Bpk/Y98B+B7wDfBj4DvK6u+x74LJ1cyct0rg5v7bevgaDT2/K7wP+h0xNr199w6gpJElDPJiNJ0hAMCJIkwIAgSSoMCJIkwIAgSSoMCJIkwIAgSSr+P5rmBBkmBISnAAAAAElFTkSuQmCC",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Call the function and get y_pred\n",
    "\n",
    "y_pred  = regression_line(b, X_test)\n",
    "\n",
    "plt.plot(X_test, y_test, \"o\", color=\"blue\")\n",
    "plt.plot(X_test, y_pred, \"o\", color=\"red\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lets calculate the mean squared error\n",
    "\n",
    "def MSE(y_pred, y_true):\n",
    "\n",
    "    residuals = 0\n",
    "\n",
    "    for i in range(len(y_pred)):\n",
    "        residuals += (y_pred[i] - y_true[i])**2\n",
    "\n",
    "    return residuals/len(y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(4192827.)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "MSE(y_pred, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x218282caac0>]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(y_test, y_pred, \"o\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
